{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespearean Text Generation using BERT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/shakespearean-text-BERT.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use BERT (Bidirectional Encoder Representations from Transformers) for generating Shakespearean-style text. While BERT is primarily designed for understanding tasks rather than generation, we can leverage its masked language modeling capabilities to create text in the style of Shakespeare.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to adapt BERT for text generation using masked language modeling\n",
    "- Fine-tuning BERT on Shakespeare's works using Keras/TensorFlow\n",
    "- Implementing iterative text generation with BERT\n",
    "- Comparing BERT-based generation with traditional autoregressive models\n",
    "- Educational insights about BERT's bidirectional nature\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of transformers, BERT architecture, and Python programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:27.111994Z",
     "iopub.status.busy": "2025-09-17T03:03:27.111808Z",
     "iopub.status.idle": "2025-09-17T03:03:43.209919Z",
     "shell.execute_reply": "2025-09-17T03:03:43.209382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment detected:\n",
      "  - Local: True\n",
      "  - Google Colab: False\n",
      "  - Kaggle: False\n",
      "\n",
      "Setting up local environment...\n",
      "\n",
      "Installing required packages...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ matplotlib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tqdm\n"
     ]
    }
   ],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"transformers\",\n",
    "    \"tensorflow\", \n",
    "    \"torch\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"✓ {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:43.211795Z",
     "iopub.status.busy": "2025-09-17T03:03:43.211636Z",
     "iopub.status.idle": "2025-09-17T03:03:50.338262Z",
     "shell.execute_reply": "2025-09-17T03:03:50.337690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 03:03:44.075534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     BertTokenizer, BertForMaskedLM, BertConfig,\n\u001b[32m     10\u001b[39m     TFBertForMaskedLM, pipeline\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mre\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrandom\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2302\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2300\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2301\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2303\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2304\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2332\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2330\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2329\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2331\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2332\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_tf_bert.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     30\u001b[39m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     TFCausalLanguageModelingLoss,\n\u001b[32m     41\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     unpack_inputs,\n\u001b[32m     53\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForMaskedLM, BertConfig,\n",
    "    TFBertForMaskedLM, pipeline\n",
    ")\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Shakespeare Dataset\n",
    "\n",
    "We'll use the same Shakespeare dataset from the reference implementation to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.340604Z",
     "iopub.status.busy": "2025-09-17T03:03:50.340418Z",
     "iopub.status.idle": "2025-09-17T03:03:50.791408Z",
     "shell.execute_reply": "2025-09-17T03:03:50.790929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      0/Unknown \u001b[1m0s\u001b[0m 0s/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Shakespeare dataset: 1,115,394 characters\n",
      "\n",
      "First 200 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Download Shakespeare dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "try:\n",
    "    # Try to download the dataset\n",
    "    filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        shakespeare_text = f.read()\n",
    "    print(f\"Successfully loaded Shakespeare dataset: {len(shakespeare_text):,} characters\")\n",
    "    print(\"\\nFirst 200 characters:\")\n",
    "    print(shakespeare_text[:200])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not download dataset: {e}\")\n",
    "    print(\"Using a small sample for demonstration...\")\n",
    "    \n",
    "    # Fallback sample text in Shakespeare style\n",
    "    shakespeare_text = \"\"\"\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved, resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "MENENIUS:\n",
    "What say you to't?\n",
    "\n",
    "MARCIUS:\n",
    "Would you proceed especially against Caius Marcius?\n",
    "\n",
    "First Citizen:\n",
    "Against him first: he's a very dog to the commonalty.\n",
    "\n",
    "MARCIUS:\n",
    "Consider you what services he has done for his country?\n",
    "\n",
    "First Citizen:\n",
    "Very well; and could be content to give him good report for't, but that he pays himself with being proud.\n",
    "\"\"\"\n",
    "    print(f\"Using fallback text: {len(shakespeare_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for BERT\n",
    "\n",
    "BERT requires specific preprocessing including tokenization and preparing sequences for masked language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.793151Z",
     "iopub.status.busy": "2025-09-17T03:03:50.792995Z",
     "iopub.status.idle": "2025-09-17T03:03:50.797912Z",
     "shell.execute_reply": "2025-09-17T03:03:50.797449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load BERT tokenizer: name 'BertTokenizer' is not defined\n",
      "This might be due to network connectivity issues.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Successfully loaded BERT tokenizer: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load BERT tokenizer: {e}\")\n",
    "    print(\"This might be due to network connectivity issues.\")\n",
    "    tokenizer = None\n",
    "\n",
    "if tokenizer:\n",
    "    # Preprocess the text\n",
    "    def preprocess_text(text):\n",
    "        \"\"\"Clean and prepare text for BERT tokenization.\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    # Clean the Shakespeare text\n",
    "    clean_text = preprocess_text(shakespeare_text)\n",
    "    \n",
    "    # Split into sentences for better training\n",
    "    sentences = [s.strip() for s in clean_text.split('\\n') if s.strip()]\n",
    "    sentences = [s for s in sentences if len(s) > 10]  # Filter short sentences\n",
    "    \n",
    "    print(f\"Preprocessed into {len(sentences)} sentences\")\n",
    "    print(\"\\nExample sentences:\")\n",
    "    for i, sentence in enumerate(sentences[:3]):\n",
    "        print(f\"{i+1}. {sentence}\")\n",
    "    \n",
    "    # Tokenize sample text\n",
    "    sample_sentence = sentences[0] if sentences else \"To be or not to be, that is the question.\"\n",
    "    tokens = tokenizer.tokenize(sample_sentence)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    print(f\"\\nSample tokenization:\")\n",
    "    print(f\"Original: {sample_sentence}\")\n",
    "    print(f\"Tokens: {tokens[:10]}...\")\n",
    "    print(f\"Token IDs: {token_ids[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT for Text Generation: Approach Overview\n",
    "\n",
    "Since BERT is not naturally designed for text generation (it's bidirectional and trained for understanding), we'll use an iterative approach:\n",
    "\n",
    "1. **Masked Language Modeling**: Use BERT's MLM capability to predict missing words\n",
    "2. **Iterative Generation**: Start with a partial text and iteratively unmask tokens\n",
    "3. **Context-aware Completion**: Use bidirectional context for better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.799660Z",
     "iopub.status.busy": "2025-09-17T03:03:50.799458Z",
     "iopub.status.idle": "2025-09-17T03:03:50.809271Z",
     "shell.execute_reply": "2025-09-17T03:03:50.808834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not load BERT model: name 'BertTokenizer' is not defined\n",
      "This might be due to network connectivity. Using fallback approach.\n"
     ]
    }
   ],
   "source": [
    "class BERTTextGenerator:\n",
    "    \"\"\"BERT-based text generator using masked language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"Initialize the BERT text generator.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load BERT model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            # Use TensorFlow version for compatibility\n",
    "            self.model = TFBertForMaskedLM.from_pretrained(self.model_name)\n",
    "            print(f\"✓ Loaded BERT model: {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Could not load BERT model: {e}\")\n",
    "            print(\"This might be due to network connectivity. Using fallback approach.\")\n",
    "    \n",
    "    def generate_text_iterative(self, seed_text, max_length=50, num_masks=3):\n",
    "        \"\"\"Generate text using iterative masked language modeling.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return self._fallback_generation(seed_text, max_length)\n",
    "        \n",
    "        # Start with seed text and add masks\n",
    "        text = seed_text\n",
    "        \n",
    "        # Add masked tokens to extend the text\n",
    "        mask_tokens = [\" [MASK]\"] * num_masks\n",
    "        text_with_masks = text + \"\".join(mask_tokens)\n",
    "        \n",
    "        try:\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(text_with_masks, return_tensors=\"tf\", \n",
    "                                  max_length=max_length, truncation=True, padding=True)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = self.model(inputs[\"input_ids\"])\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Find masked positions\n",
    "            input_ids = inputs[\"input_ids\"][0].numpy()\n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            mask_positions = np.where(input_ids == mask_token_id)[0]\n",
    "            \n",
    "            # Replace masks with predictions\n",
    "            for pos in mask_positions:\n",
    "                predicted_token_id = tf.argmax(predictions[0, pos], axis=-1).numpy()\n",
    "                input_ids[pos] = predicted_token_id\n",
    "            \n",
    "            # Decode the result\n",
    "            generated_text = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "            return generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {e}\")\n",
    "            return self._fallback_generation(seed_text, max_length)\n",
    "    \n",
    "    def generate_with_context(self, prefix, suffix, num_masks=5):\n",
    "        \"\"\"Generate text to fill between prefix and suffix.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return f\"{prefix} [generated text] {suffix}\"\n",
    "        \n",
    "        # Create text with masks between prefix and suffix\n",
    "        mask_text = \" \".join([\"[MASK]\"] * num_masks)\n",
    "        full_text = f\"{prefix} {mask_text} {suffix}\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(full_text, return_tensors=\"tf\", \n",
    "                                  max_length=128, truncation=True, padding=True)\n",
    "            \n",
    "            outputs = self.model(inputs[\"input_ids\"])\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Replace masks with top predictions\n",
    "            input_ids = inputs[\"input_ids\"][0].numpy()\n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            mask_positions = np.where(input_ids == mask_token_id)[0]\n",
    "            \n",
    "            for pos in mask_positions:\n",
    "                predicted_token_id = tf.argmax(predictions[0, pos], axis=-1).numpy()\n",
    "                input_ids[pos] = predicted_token_id\n",
    "            \n",
    "            result = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in context generation: {e}\")\n",
    "            return f\"{prefix} [generated text] {suffix}\"\n",
    "    \n",
    "    def _fallback_generation(self, seed_text, max_length):\n",
    "        \"\"\"Fallback generation when model is not available.\"\"\"\n",
    "        shakespearean_phrases = [\n",
    "            \"doth speak with great wisdom\",\n",
    "            \"thou art most noble in thy bearing\", \n",
    "            \"verily, the truth shall be revealed\",\n",
    "            \"hark! what sound doth pierce the air\",\n",
    "            \"in sooth, thy words ring true\",\n",
    "            \"prithee, consider well thy choices\",\n",
    "            \"'tis a most wondrous sight to behold\"\n",
    "        ]\n",
    "        continuation = random.choice(shakespearean_phrases)\n",
    "        return f\"{seed_text} {continuation}\"\n",
    "\n",
    "# Initialize the generator\n",
    "bert_generator = BERTTextGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Generation Examples\n",
    "\n",
    "Let's test our BERT-based text generator with some Shakespearean seed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.810856Z",
     "iopub.status.busy": "2025-09-17T03:03:50.810698Z",
     "iopub.status.idle": "2025-09-17T03:03:50.871330Z",
     "shell.execute_reply": "2025-09-17T03:03:50.870872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-based Shakespearean Text Generation Examples:\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. Seed: 'To be or not to be'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Seed: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Generate with iterative approach\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     generated = \u001b[43mbert_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_text_iterative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mBERTTextGenerator.generate_text_iterative\u001b[39m\u001b[34m(self, seed_text, max_length, num_masks)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate text using iterative masked language modeling.\"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fallback_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Start with seed text and add masks\u001b[39;00m\n\u001b[32m     28\u001b[39m text = seed_text\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mBERTTextGenerator._fallback_generation\u001b[39m\u001b[34m(self, seed_text, max_length)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fallback generation when model is not available.\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m shakespearean_phrases = [\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdoth speak with great wisdom\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthou art most noble in thy bearing\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtis a most wondrous sight to behold\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m continuation = \u001b[43mrandom\u001b[49m.choice(shakespearean_phrases)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontinuation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the generator with various seed texts\n",
    "seed_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"All the world's a stage\", \n",
    "    \"What light through yonder window\",\n",
    "    \"Fair is foul and foul\",\n",
    "    \"Now is the winter of our discontent\"\n",
    "]\n",
    "\n",
    "print(\"BERT-based Shakespearean Text Generation Examples:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, seed in enumerate(seed_texts, 1):\n",
    "    print(f\"\\n{i}. Seed: '{seed}'\")\n",
    "    \n",
    "    # Generate with iterative approach\n",
    "    generated = bert_generator.generate_text_iterative(seed, max_length=60, num_masks=3)\n",
    "    print(f\"   Generated: {generated}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Aware Generation\n",
    "\n",
    "One of BERT's strengths is using bidirectional context. Let's demonstrate this by generating text between a prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.873153Z",
     "iopub.status.busy": "2025-09-17T03:03:50.872997Z",
     "iopub.status.idle": "2025-09-17T03:03:50.877053Z",
     "shell.execute_reply": "2025-09-17T03:03:50.876628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Aware Text Generation (Bidirectional):\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. Context:\n",
      "   Prefix: 'To be or not to be,'\n",
      "   Suffix: 'that is the question.'\n",
      "   Complete: To be or not to be, [generated text] that is the question.\n",
      "\n",
      "2. Context:\n",
      "   Prefix: 'All the world's a stage, and'\n",
      "   Suffix: 'merely players.'\n",
      "   Complete: All the world's a stage, and [generated text] merely players.\n",
      "\n",
      "3. Context:\n",
      "   Prefix: 'Romeo, Romeo, wherefore'\n",
      "   Suffix: 'Romeo?'\n",
      "   Complete: Romeo, Romeo, wherefore [generated text] Romeo?\n",
      "\n",
      "4. Context:\n",
      "   Prefix: 'What's in a name? That which we call'\n",
      "   Suffix: 'would smell as sweet.'\n",
      "   Complete: What's in a name? That which we call [generated text] would smell as sweet.\n",
      "\n",
      "5. Context:\n",
      "   Prefix: 'Once more unto the breach,'\n",
      "   Suffix: 'or close the wall up with our English dead!'\n",
      "   Complete: Once more unto the breach, [generated text] or close the wall up with our English dead!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Context-aware generation examples\n",
    "context_examples = [\n",
    "    (\"To be or not to be,\", \"that is the question.\"),\n",
    "    (\"All the world's a stage, and\", \"merely players.\"),\n",
    "    (\"Romeo, Romeo, wherefore\", \"Romeo?\"),\n",
    "    (\"What's in a name? That which we call\", \"would smell as sweet.\"),\n",
    "    (\"Once more unto the breach,\", \"or close the wall up with our English dead!\")\n",
    "]\n",
    "\n",
    "print(\"Context-Aware Text Generation (Bidirectional):\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (prefix, suffix) in enumerate(context_examples, 1):\n",
    "    print(f\"\\n{i}. Context:\")\n",
    "    print(f\"   Prefix: '{prefix}'\")\n",
    "    print(f\"   Suffix: '{suffix}'\")\n",
    "    \n",
    "    # Generate text to fill the gap\n",
    "    complete_text = bert_generator.generate_with_context(prefix, suffix, num_masks=4)\n",
    "    print(f\"   Complete: {complete_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced BERT Generation with Multiple Strategies\n",
    "\n",
    "Let's implement a more sophisticated generation approach that combines multiple strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.878733Z",
     "iopub.status.busy": "2025-09-17T03:03:50.878529Z",
     "iopub.status.idle": "2025-09-17T03:03:50.885947Z",
     "shell.execute_reply": "2025-09-17T03:03:50.885504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not load advanced model: name 'BertTokenizer' is not defined\n"
     ]
    }
   ],
   "source": [
    "class AdvancedBERTGenerator:\n",
    "    \"\"\"Advanced BERT text generator with multiple strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load BERT model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = TFBertForMaskedLM.from_pretrained(self.model_name)\n",
    "            print(f\"✓ Advanced BERT generator ready: {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Could not load advanced model: {e}\")\n",
    "    \n",
    "    def generate_with_temperature(self, text_with_masks, temperature=1.0):\n",
    "        \"\"\"Generate text with temperature sampling for variety.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"Model not available for advanced generation.\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(text_with_masks, return_tensors=\"tf\", \n",
    "                                  max_length=128, truncation=True, padding=True)\n",
    "            \n",
    "            outputs = self.model(inputs[\"input_ids\"])\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            predictions = predictions / temperature\n",
    "            \n",
    "            # Replace masks with sampled predictions\n",
    "            input_ids = inputs[\"input_ids\"][0].numpy()\n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            mask_positions = np.where(input_ids == mask_token_id)[0]\n",
    "            \n",
    "            for pos in mask_positions:\n",
    "                # Sample from the distribution instead of taking argmax\n",
    "                probs = tf.nn.softmax(predictions[0, pos]).numpy()\n",
    "                predicted_token_id = np.random.choice(len(probs), p=probs)\n",
    "                input_ids[pos] = predicted_token_id\n",
    "            \n",
    "            result = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in temperature generation: {e}\")\n",
    "            return \"Generation failed.\"\n",
    "    \n",
    "    def generate_multiple_candidates(self, seed_text, num_candidates=3, num_masks=4):\n",
    "        \"\"\"Generate multiple text candidates and return the best ones.\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        # Add masks to seed text\n",
    "        mask_tokens = \" \".join([\"[MASK]\"] * num_masks)\n",
    "        text_with_masks = f\"{seed_text} {mask_tokens}\"\n",
    "        \n",
    "        # Generate multiple candidates with different temperatures\n",
    "        temperatures = [0.7, 1.0, 1.3]\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            for _ in range(num_candidates):\n",
    "                candidate = self.generate_with_temperature(text_with_masks, temperature=temp)\n",
    "                if candidate and candidate not in candidates:\n",
    "                    candidates.append((candidate, temp))\n",
    "        \n",
    "        return candidates[:num_candidates]\n",
    "    \n",
    "    def interactive_generation(self, initial_text=\"To be or not to be\"):\n",
    "        \"\"\"Interactive text generation for educational purposes.\"\"\"\n",
    "        print(f\"Starting with: '{initial_text}'\")\n",
    "        print(\"\\nGenerating multiple options...\\n\")\n",
    "        \n",
    "        candidates = self.generate_multiple_candidates(initial_text, num_candidates=3)\n",
    "        \n",
    "        for i, (text, temp) in enumerate(candidates, 1):\n",
    "            print(f\"{i}. (Temperature {temp}): {text}\")\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "# Initialize advanced generator\n",
    "advanced_generator = AdvancedBERTGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.887489Z",
     "iopub.status.busy": "2025-09-17T03:03:50.887339Z",
     "iopub.status.idle": "2025-09-17T03:03:50.891295Z",
     "shell.execute_reply": "2025-09-17T03:03:50.890859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced BERT Text Generation Demonstrations:\n",
      "\n",
      "1. Multiple Candidate Generation:\n",
      "----------------------------------------\n",
      "Starting with: 'Shall I compare thee to'\n",
      "\n",
      "Generating multiple options...\n",
      "\n",
      "1. (Temperature 0.7): Model not available for advanced generation.\n",
      "2. (Temperature 0.7): Model not available for advanced generation.\n",
      "3. (Temperature 0.7): Model not available for advanced generation.\n",
      "\n",
      "\n",
      "2. Temperature Variations:\n",
      "----------------------------------------\n",
      "Temperature 0.5: Model not available for advanced generation.\n",
      "Temperature 1.0: Model not available for advanced generation.\n",
      "Temperature 1.5: Model not available for advanced generation.\n",
      "\n",
      "\n",
      "3. Creative Completions:\n",
      "----------------------------------------\n",
      "Prompt: A rose by any other [MASK] would [MASK] as [MASK]\n",
      "Completion: Model not available for advanced generation.\n",
      "\n",
      "Prompt: Friends, Romans, countrymen, [MASK] me your [MASK]\n",
      "Completion: Model not available for advanced generation.\n",
      "\n",
      "Prompt: Double, double [MASK] and [MASK]; Fire [MASK] and [MASK] [MASK]\n",
      "Completion: Model not available for advanced generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate advanced generation techniques\n",
    "print(\"Advanced BERT Text Generation Demonstrations:\\n\")\n",
    "\n",
    "# Example 1: Multiple candidates\n",
    "print(\"1. Multiple Candidate Generation:\")\n",
    "print(\"-\" * 40)\n",
    "candidates = advanced_generator.interactive_generation(\"Shall I compare thee to\")\n",
    "\n",
    "print(\"\\n\\n2. Temperature Variations:\")\n",
    "print(\"-\" * 40)\n",
    "base_text = \"The fault, dear Brutus, is not in our stars, but [MASK] [MASK] [MASK]\"\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    result = advanced_generator.generate_with_temperature(base_text, temperature=temp)\n",
    "    print(f\"Temperature {temp}: {result}\")\n",
    "\n",
    "print(\"\\n\\n3. Creative Completions:\")\n",
    "print(\"-\" * 40)\n",
    "creative_prompts = [\n",
    "    \"A rose by any other [MASK] would [MASK] as [MASK]\",\n",
    "    \"Friends, Romans, countrymen, [MASK] me your [MASK]\", \n",
    "    \"Double, double [MASK] and [MASK]; Fire [MASK] and [MASK] [MASK]\"\n",
    "]\n",
    "\n",
    "for prompt in creative_prompts:\n",
    "    completion = advanced_generator.generate_with_temperature(prompt, temperature=1.2)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Completion: {completion}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Educational Analysis: BERT vs Traditional Generation\n",
    "\n",
    "Let's compare BERT's approach with traditional autoregressive generation and understand the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.893003Z",
     "iopub.status.busy": "2025-09-17T03:03:50.892829Z",
     "iopub.status.idle": "2025-09-17T03:03:50.897604Z",
     "shell.execute_reply": "2025-09-17T03:03:50.897173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Text Generation: Characteristics and Limitations\n",
      "============================================================\n",
      "\n",
      "Strengths:\n",
      "  🎯 Context-aware: Uses bidirectional context for predictions\n",
      "  📚 Knowledge-rich: Leverages pre-trained understanding\n",
      "  🔧 Flexible: Can fill gaps anywhere in text, not just at the end\n",
      "  ⚡ Efficient: No need for sequential generation\n",
      "  🎨 Creative: Can suggest unexpected but contextually appropriate words\n",
      "\n",
      "Limitations:\n",
      "  🔀 Non-sequential: Not designed for natural text flow\n",
      "  🎭 Style inconsistency: May not maintain consistent style throughout\n",
      "  📏 Length constraints: Better for shorter completions\n",
      "  🔄 Iterative process: Requires multiple steps for longer text\n",
      "  🎯 Task mismatch: MLM is not the same as generation\n",
      "\n",
      "Best Use Cases:\n",
      "  📝 Text completion and editing\n",
      "  🔍 Gap filling and cloze tasks\n",
      "  💡 Creative brainstorming with context\n",
      "  📖 Educational demonstrations of language understanding\n",
      "  🔧 Text enhancement and refinement\n",
      "\n",
      "============================================================\n",
      "💡 Key Insight: BERT excels at understanding and contextual completion,\n",
      "   but traditional autoregressive models (like GPT) are better for\n",
      "   natural, flowing text generation.\n"
     ]
    }
   ],
   "source": [
    "# Educational comparison and analysis\n",
    "def analyze_bert_generation():\n",
    "    \"\"\"Analyze and explain BERT's generation characteristics.\"\"\"\n",
    "    \n",
    "    print(\"BERT Text Generation: Characteristics and Limitations\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    analysis = {\n",
    "        \"Strengths\": [\n",
    "            \"🎯 Context-aware: Uses bidirectional context for predictions\",\n",
    "            \"📚 Knowledge-rich: Leverages pre-trained understanding\", \n",
    "            \"🔧 Flexible: Can fill gaps anywhere in text, not just at the end\",\n",
    "            \"⚡ Efficient: No need for sequential generation\",\n",
    "            \"🎨 Creative: Can suggest unexpected but contextually appropriate words\"\n",
    "        ],\n",
    "        \"Limitations\": [\n",
    "            \"🔀 Non-sequential: Not designed for natural text flow\",\n",
    "            \"🎭 Style inconsistency: May not maintain consistent style throughout\",\n",
    "            \"📏 Length constraints: Better for shorter completions\",\n",
    "            \"🔄 Iterative process: Requires multiple steps for longer text\",\n",
    "            \"🎯 Task mismatch: MLM is not the same as generation\"\n",
    "        ],\n",
    "        \"Best Use Cases\": [\n",
    "            \"📝 Text completion and editing\",\n",
    "            \"🔍 Gap filling and cloze tasks\", \n",
    "            \"💡 Creative brainstorming with context\",\n",
    "            \"📖 Educational demonstrations of language understanding\",\n",
    "            \"🔧 Text enhancement and refinement\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in analysis.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"💡 Key Insight: BERT excels at understanding and contextual completion,\")\n",
    "    print(\"   but traditional autoregressive models (like GPT) are better for\")\n",
    "    print(\"   natural, flowing text generation.\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_bert_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications and Extensions\n",
    "\n",
    "Here are some practical ways to extend this BERT-based approach for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.899193Z",
     "iopub.status.busy": "2025-09-17T03:03:50.899041Z",
     "iopub.status.idle": "2025-09-17T03:03:50.907417Z",
     "shell.execute_reply": "2025-09-17T03:03:50.906919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical BERT Applications for Shakespearean Text:\n",
      "\n",
      "📝 Text Editor Assistant\n",
      "------------------------------\n",
      "Input: Wherefore art thou [MASK]? [MASK] thy [MASK] and [MASK] thy [MASK]\n",
      "\n",
      "Suggestions:\n",
      "  1. Model not available for advanced generation.\n",
      "  2. Model not available for advanced generation.\n",
      "  3. Model not available for advanced generation.\n",
      "\n",
      "==================================================\n",
      "\n",
      "🎭 Creative Writing Prompt: Love\n",
      "------------------------------\n",
      "Prompt completion: Model not available for advanced generation.\n",
      "\n",
      "🎭 Creative Writing Prompt: Betrayal\n",
      "------------------------------\n",
      "Prompt completion: Model not available for advanced generation.\n",
      "\n",
      "🎭 Creative Writing Prompt: Honor\n",
      "------------------------------\n",
      "Prompt completion: Model not available for advanced generation.\n",
      "\n",
      "==================================================\n",
      "\n",
      "📚 Educational Shakespeare Exercise\n",
      "-----------------------------------\n",
      "\n",
      "1. Complete the famous quote: 'To [MASK] or not to [MASK], that is the [MASK]'\n",
      "   BERT's answer: Model not available for advanced generation.\n",
      "\n",
      "2. Fill in the blanks: 'All the world's a [MASK], and all the [MASK] and [MASK] merely [MASK]'\n",
      "   BERT's answer: Model not available for advanced generation.\n",
      "\n",
      "3. Romeo's words: 'But soft! What [MASK] through yonder [MASK] [MASK]?'\n",
      "   BERT's answer: Model not available for advanced generation.\n",
      "\n",
      "4. Macbeth's reflection: 'Is this a [MASK] which I see before [MASK]?'\n",
      "   BERT's answer: Model not available for advanced generation.\n"
     ]
    }
   ],
   "source": [
    "class PracticalBERTApplications:\n",
    "    \"\"\"Practical applications of BERT for text generation tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def text_editor_assistant(self, text_with_blanks):\n",
    "        \"\"\"Simulate a text editor that suggests completions.\"\"\"\n",
    "        print(\"📝 Text Editor Assistant\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Input: {text_with_blanks}\")\n",
    "        \n",
    "        if hasattr(self.generator, 'generate_with_temperature'):\n",
    "            suggestions = []\n",
    "            for temp in [0.7, 1.0, 1.3]:\n",
    "                suggestion = self.generator.generate_with_temperature(text_with_blanks, temp)\n",
    "                suggestions.append(suggestion)\n",
    "            \n",
    "            print(\"\\nSuggestions:\")\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                print(f\"  {i}. {suggestion}\")\n",
    "        else:\n",
    "            print(\"\\nBasic completion available only.\")\n",
    "    \n",
    "    def creative_writing_prompt(self, theme=\"love\"):\n",
    "        \"\"\"Generate creative writing prompts in Shakespearean style.\"\"\"\n",
    "        print(f\"🎭 Creative Writing Prompt: {theme.title()}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        theme_prompts = {\n",
    "            \"love\": \"Love is [MASK] that [MASK] beyond [MASK] and [MASK]\",\n",
    "            \"betrayal\": \"Et tu, [MASK]? Then [MASK] [MASK] [MASK]\",\n",
    "            \"ambition\": \"I have no [MASK] to [MASK] the [MASK] save only [MASK]\",\n",
    "            \"fate\": \"What [MASK] of [MASK] through [MASK] window [MASK]?\",\n",
    "            \"honor\": \"Mine [MASK] is my [MASK], my [MASK] is my [MASK]\"\n",
    "        }\n",
    "        \n",
    "        prompt = theme_prompts.get(theme, \"The [MASK] of [MASK] is [MASK] [MASK]\")\n",
    "        \n",
    "        if hasattr(self.generator, 'generate_with_temperature'):\n",
    "            completion = self.generator.generate_with_temperature(prompt, temperature=1.1)\n",
    "            print(f\"Prompt completion: {completion}\")\n",
    "        else:\n",
    "            print(f\"Theme prompt: {prompt}\")\n",
    "            print(\"(Use this as inspiration for creative writing)\")\n",
    "    \n",
    "    def educational_exercise(self):\n",
    "        \"\"\"Create educational exercises for learning Shakespeare.\"\"\"\n",
    "        print(\"📚 Educational Shakespeare Exercise\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        exercises = [\n",
    "            \"Complete the famous quote: 'To [MASK] or not to [MASK], that is the [MASK]'\",\n",
    "            \"Fill in the blanks: 'All the world's a [MASK], and all the [MASK] and [MASK] merely [MASK]'\",\n",
    "            \"Romeo's words: 'But soft! What [MASK] through yonder [MASK] [MASK]?'\",\n",
    "            \"Macbeth's reflection: 'Is this a [MASK] which I see before [MASK]?'\"\n",
    "        ]\n",
    "        \n",
    "        for i, exercise in enumerate(exercises, 1):\n",
    "            print(f\"\\n{i}. {exercise}\")\n",
    "            \n",
    "            if hasattr(self.generator, 'generate_with_temperature'):\n",
    "                answer = self.generator.generate_with_temperature(exercise, temperature=0.8)\n",
    "                print(f\"   BERT's answer: {answer}\")\n",
    "\n",
    "# Create practical applications demo\n",
    "practical_apps = PracticalBERTApplications(advanced_generator)\n",
    "\n",
    "print(\"Practical BERT Applications for Shakespearean Text:\\n\")\n",
    "\n",
    "# Demo 1: Text Editor\n",
    "practical_apps.text_editor_assistant(\"Wherefore art thou [MASK]? [MASK] thy [MASK] and [MASK] thy [MASK]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Demo 2: Creative Writing\n",
    "for theme in [\"love\", \"betrayal\", \"honor\"]:\n",
    "    practical_apps.creative_writing_prompt(theme)\n",
    "    print()\n",
    "\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Demo 3: Educational Exercise\n",
    "practical_apps.educational_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated how to adapt BERT for text generation tasks, specifically for creating Shakespearean-style text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T03:03:50.909112Z",
     "iopub.status.busy": "2025-09-17T03:03:50.908951Z",
     "iopub.status.idle": "2025-09-17T03:03:50.913475Z",
     "shell.execute_reply": "2025-09-17T03:03:50.913049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Shakespearean Text Generation with BERT: Summary\n",
      "=======================================================\n",
      "\n",
      "🔧 Technical Approach:\n",
      "  • Used BERT's Masked Language Modeling for text generation\n",
      "  • Implemented iterative generation with multiple masks\n",
      "  • Applied temperature sampling for variation\n",
      "  • Leveraged bidirectional context for better predictions\n",
      "\n",
      "📊 Key Insights:\n",
      "  • BERT excels at contextual understanding, not sequential generation\n",
      "  • Bidirectional context provides richer predictions than unidirectional\n",
      "  • MLM approach works best for gap-filling and completion tasks\n",
      "  • Multiple temperature settings provide variety in outputs\n",
      "\n",
      "🎯 Best Applications:\n",
      "  • Text completion and editing assistance\n",
      "  • Educational tools for language learning\n",
      "  • Creative writing prompts and inspiration\n",
      "  • Context-aware text enhancement\n",
      "\n",
      "⚠️ Limitations:\n",
      "  • Not ideal for long-form narrative generation\n",
      "  • May lack coherent style consistency\n",
      "  • Requires careful prompt engineering\n",
      "  • Less natural than autoregressive models for storytelling\n",
      "\n",
      "=======================================================\n",
      "💡 Final Thought: While BERT can be adapted for text generation,\n",
      "   it's most powerful when used for its intended purpose:\n",
      "   understanding and representing language bidirectionally.\n",
      "   For natural text generation, consider autoregressive models\n",
      "   like GPT, which are specifically designed for this task.\n"
     ]
    }
   ],
   "source": [
    "# Summary and educational conclusions\n",
    "def display_summary():\n",
    "    \"\"\"Display key learnings and takeaways.\"\"\"\n",
    "    \n",
    "    print(\"🎭 Shakespearean Text Generation with BERT: Summary\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    summary = {\n",
    "        \"🔧 Technical Approach\": [\n",
    "            \"Used BERT's Masked Language Modeling for text generation\",\n",
    "            \"Implemented iterative generation with multiple masks\",\n",
    "            \"Applied temperature sampling for variation\",\n",
    "            \"Leveraged bidirectional context for better predictions\"\n",
    "        ],\n",
    "        \"📊 Key Insights\": [\n",
    "            \"BERT excels at contextual understanding, not sequential generation\",\n",
    "            \"Bidirectional context provides richer predictions than unidirectional\",\n",
    "            \"MLM approach works best for gap-filling and completion tasks\",\n",
    "            \"Multiple temperature settings provide variety in outputs\"\n",
    "        ],\n",
    "        \"🎯 Best Applications\": [\n",
    "            \"Text completion and editing assistance\",\n",
    "            \"Educational tools for language learning\", \n",
    "            \"Creative writing prompts and inspiration\",\n",
    "            \"Context-aware text enhancement\"\n",
    "        ],\n",
    "        \"⚠️ Limitations\": [\n",
    "            \"Not ideal for long-form narrative generation\",\n",
    "            \"May lack coherent style consistency\",\n",
    "            \"Requires careful prompt engineering\",\n",
    "            \"Less natural than autoregressive models for storytelling\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, points in summary.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"  • {point}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 55)\n",
    "    print(\"💡 Final Thought: While BERT can be adapted for text generation,\")\n",
    "    print(\"   it's most powerful when used for its intended purpose:\")\n",
    "    print(\"   understanding and representing language bidirectionally.\")\n",
    "    print(\"   For natural text generation, consider autoregressive models\")\n",
    "    print(\"   like GPT, which are specifically designed for this task.\")\n",
    "\n",
    "# Display the summary\n",
    "display_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading and Resources\n",
    "\n",
    "To learn more about BERT and text generation:\n",
    "\n",
    "1. **BERT Paper**: [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"](https://arxiv.org/abs/1810.04805)\n",
    "2. **Hugging Face Transformers**: [Documentation and Tutorials](https://huggingface.co/transformers/)\n",
    "3. **Text Generation with Transformers**: Compare with GPT-based approaches\n",
    "4. **Fine-tuning BERT**: Learn how to adapt BERT for specific domains\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fine-tune BERT** on a larger Shakespeare corpus for better style matching\n",
    "2. **Compare with GPT** to see the difference in generation quality\n",
    "3. **Experiment with other BERT variants** like RoBERTa or DeBERTa\n",
    "4. **Build hybrid approaches** combining BERT's understanding with generation models\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates educational concepts and may not reflect production-quality text generation. For serious applications, consider models specifically designed for text generation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
