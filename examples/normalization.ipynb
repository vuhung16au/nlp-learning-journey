{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "normalization_title"
   },
   "source": [
    "# Text Normalization in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/normalization.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text normalization is the process of transforming text into a canonical (standard) form. This preprocessing step helps reduce the complexity of text data and improves the performance of NLP models by ensuring consistent representation of similar words and phrases.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Case normalization\n",
    "- Punctuation handling\n",
    "- Stemming and lemmatization\n",
    "- Handling special characters and unicode\n",
    "- Stop word removal\n",
    "- Text cleaning techniques\n",
    "- Domain-specific normalization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, regular expressions, and NLP concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Let's install the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libraries"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install nltk spacy textblob unidecode contractions\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from textblob import TextBlob\n",
    "from unidecode import unidecode\n",
    "import contractions\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('porter_test')\n",
    "\n",
    "# Import NLTK modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_text"
   },
   "source": [
    "## Sample Text\n",
    "\n",
    "Let's use a sample text with various normalization challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_text"
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Hey There!!! This is a SAMPLE text for NORMALIZATION. It contains UPPERCASE and lowercase words, \n",
    "contractions like don't, won't, can't, and it's. There are also numbers like 123, 4.56, and $100.00.\n",
    "Special characters: @#$%^&*()!!! And some accented characters: café, naïve, résumé.\n",
    "URLs: https://www.example.com and emails: user@domain.com.\n",
    "Multiple    spaces   and\\ttabs\\nand newlines need cleaning too.\n",
    "Words like running, runs, ran should be normalized. Also books, book's, and booking.\n",
    "Stop words: the, is, at, which, on should often be removed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(repr(sample_text))\n",
    "print(\"\\nDisplayed Text:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "case_normalization"
   },
   "source": [
    "## 1. Case Normalization\n",
    "\n",
    "Converting text to a consistent case (usually lowercase) to treat words like \"The\" and \"the\" as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "case_conversion"
   },
   "outputs": [],
   "source": [
    "# Case normalization examples\n",
    "print(\"Original:\", \"This IS a MIXED case TEXT\")\n",
    "print(\"Lowercase:\", \"This IS a MIXED case TEXT\".lower())\n",
    "print(\"Uppercase:\", \"This IS a MIXED case TEXT\".upper())\n",
    "print(\"Title case:\", \"This IS a MIXED case TEXT\".title())\n",
    "print(\"Capitalize:\", \"This IS a MIXED case TEXT\".capitalize())\n",
    "\n",
    "# Apply to sample text\n",
    "text_lower = sample_text.lower()\n",
    "print(\"\\nSample text in lowercase:\")\n",
    "print(text_lower[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whitespace_normalization"
   },
   "source": [
    "## 2. Whitespace Normalization\n",
    "\n",
    "Handling multiple spaces, tabs, and newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whitespace_cleaning"
   },
   "outputs": [],
   "source": [
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Normalize whitespace by replacing multiple spaces, tabs, and newlines with single spaces\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Test whitespace normalization\n",
    "test_whitespace = \"This   has    multiple\\t\\tspaces\\n\\nand\\nnewlines\"\n",
    "print(\"Original:\", repr(test_whitespace))\n",
    "print(\"Normalized:\", repr(normalize_whitespace(test_whitespace)))\n",
    "\n",
    "# Apply to sample text\n",
    "text_whitespace_norm = normalize_whitespace(text_lower)\n",
    "print(\"\\nSample text after whitespace normalization:\")\n",
    "print(text_whitespace_norm[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "punctuation_handling"
   },
   "source": [
    "## 3. Punctuation Handling\n",
    "\n",
    "Different strategies for dealing with punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "punctuation_removal"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from text\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def replace_punctuation_with_space(text):\n",
    "    \"\"\"\n",
    "    Replace punctuation with spaces\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "def keep_sentence_punctuation(text):\n",
    "    \"\"\"\n",
    "    Keep only sentence-ending punctuation\n",
    "    \"\"\"\n",
    "    sentence_punct = '.!?'\n",
    "    other_punct = ''.join(c for c in string.punctuation if c not in sentence_punct)\n",
    "    return text.translate(str.maketrans('', '', other_punct))\n",
    "\n",
    "# Test different punctuation strategies\n",
    "punct_test = \"Hello, world! How are you? I'm fine... Thanks!\"\n",
    "print(\"Original:\", punct_test)\n",
    "print(\"Remove all:\", remove_punctuation(punct_test))\n",
    "print(\"Replace with space:\", normalize_whitespace(replace_punctuation_with_space(punct_test)))\n",
    "print(\"Keep sentence punct:\", keep_sentence_punctuation(punct_test))\n",
    "\n",
    "# Apply to sample text\n",
    "text_no_punct = remove_punctuation(text_whitespace_norm)\n",
    "text_no_punct = normalize_whitespace(text_no_punct)\n",
    "print(\"\\nSample text after punctuation removal:\")\n",
    "print(text_no_punct[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "contraction_expansion"
   },
   "source": [
    "## 4. Contraction Expansion\n",
    "\n",
    "Expanding contractions like \"don't\" → \"do not\", \"won't\" → \"will not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expand_contractions"
   },
   "outputs": [],
   "source": [
    "# Using the contractions library\n",
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand contractions in text\n",
    "    \"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Manual contraction dictionary (alternative approach)\n",
    "contraction_dict = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions_manual(text, contraction_mapping=contraction_dict):\n",
    "    \"\"\"\n",
    "    Expand contractions using manual dictionary\n",
    "    \"\"\"\n",
    "    for contraction, expansion in contraction_mapping.items():\n",
    "        text = re.sub(re.escape(contraction), expansion, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Test contraction expansion\n",
    "contraction_test = \"I don't think it's working. Won't you help me? I can't do it.\"\n",
    "print(\"Original:\", contraction_test)\n",
    "print(\"Expanded (library):\", expand_contractions(contraction_test))\n",
    "print(\"Expanded (manual):\", expand_contractions_manual(contraction_test.lower()))\n",
    "\n",
    "# Apply to our working text\n",
    "text_expanded = expand_contractions(text_no_punct)\n",
    "print(\"\\nSample text after contraction expansion:\")\n",
    "print(text_expanded[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unicode_normalization"
   },
   "source": [
    "## 5. Unicode and Accent Normalization\n",
    "\n",
    "Handling accented characters and special unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unicode_handling"
   },
   "outputs": [],
   "source": [
    "# Different approaches to unicode normalization\n",
    "def remove_accents_unidecode(text):\n",
    "    \"\"\"\n",
    "    Remove accents using unidecode (converts to ASCII)\n",
    "    \"\"\"\n",
    "    return unidecode(text)\n",
    "\n",
    "def remove_accents_unicode(text):\n",
    "    \"\"\"\n",
    "    Remove accents using unicode normalization\n",
    "    \"\"\"\n",
    "    # Normalize to NFD (decomposed form)\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    # Remove combining characters (accents)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode to NFC form (composed)\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# Test unicode normalization\n",
    "unicode_test = \"café naïve résumé piñata Zürich\"\n",
    "print(\"Original:\", unicode_test)\n",
    "print(\"Unidecode:\", remove_accents_unidecode(unicode_test))\n",
    "print(\"Unicode decompose:\", remove_accents_unicode(unicode_test))\n",
    "print(\"Unicode normalize:\", normalize_unicode(unicode_test))\n",
    "\n",
    "# Apply to our text\n",
    "text_unicode_norm = remove_accents_unidecode(text_expanded)\n",
    "print(\"\\nSample text after unicode normalization:\")\n",
    "print(text_unicode_norm[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "number_normalization"
   },
   "source": [
    "## 6. Number Normalization\n",
    "\n",
    "Different strategies for handling numbers in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "number_handling"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\"\n",
    "    Remove all numbers from text\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_numbers_with_token(text, token='<NUM>'):\n",
    "    \"\"\"\n",
    "    Replace numbers with a special token\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\d+', token, text)\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    \"\"\"\n",
    "    More sophisticated number normalization\n",
    "    \"\"\"\n",
    "    # Replace currency\n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d+)?', '<CURRENCY>', text)\n",
    "    # Replace percentages\n",
    "    text = re.sub(r'\\d+(?:\\.\\d+)?%', '<PERCENT>', text)\n",
    "    # Replace decimal numbers\n",
    "    text = re.sub(r'\\d+\\.\\d+', '<DECIMAL>', text)\n",
    "    # Replace integers\n",
    "    text = re.sub(r'\\d+', '<INTEGER>', text)\n",
    "    return text\n",
    "\n",
    "# Test number normalization\n",
    "number_test = \"I have $100.50 and scored 95.5% on the test. There are 42 students.\"\n",
    "print(\"Original:\", number_test)\n",
    "print(\"Remove numbers:\", normalize_whitespace(remove_numbers(number_test)))\n",
    "print(\"Replace with token:\", replace_numbers_with_token(number_test))\n",
    "print(\"Sophisticated norm:\", normalize_numbers(number_test))\n",
    "\n",
    "# Apply simple number removal\n",
    "text_no_numbers = normalize_whitespace(remove_numbers(text_unicode_norm))\n",
    "print(\"\\nSample text after number removal:\")\n",
    "print(text_no_numbers[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stemming"
   },
   "source": [
    "## 7. Stemming\n",
    "\n",
    "Reducing words to their root form by removing suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stemming_algorithms"
   },
   "outputs": [],
   "source": [
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "def compare_stemmers(words):\n",
    "    \"\"\"\n",
    "    Compare different stemming algorithms\n",
    "    \"\"\"\n",
    "    print(f\"{'Word':<15} {'Porter':<15} {'Snowball':<15} {'Lancaster':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for word in words:\n",
    "        porter = porter_stemmer.stem(word)\n",
    "        snowball = snowball_stemmer.stem(word)\n",
    "        lancaster = lancaster_stemmer.stem(word)\n",
    "        print(f\"{word:<15} {porter:<15} {snowball:<15} {lancaster:<15}\")\n",
    "\n",
    "# Test stemming with example words\n",
    "test_words = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly', 'books', 'booking', 'booked']\n",
    "print(\"Stemming Comparison:\")\n",
    "compare_stemmers(test_words)\n",
    "\n",
    "# Apply stemming to sample text\n",
    "def stem_text(text, stemmer):\n",
    "    \"\"\"\n",
    "    Apply stemming to all words in text\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "text_stemmed = stem_text(text_no_numbers, porter_stemmer)\n",
    "print(\"\\nSample text after Porter stemming:\")\n",
    "print(text_stemmed[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lemmatization"
   },
   "source": [
    "## 8. Lemmatization\n",
    "\n",
    "Reducing words to their base or dictionary form (lemma) using linguistic knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lemmatization_comparison"
   },
   "outputs": [],
   "source": [
    "# Initialize lemmatizers\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def compare_stemming_lemmatization(words):\n",
    "    \"\"\"\n",
    "    Compare stemming vs lemmatization\n",
    "    \"\"\"\n",
    "    print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for word in words:\n",
    "        stemmed = porter_stemmer.stem(word)\n",
    "        lemmatized = nltk_lemmatizer.lemmatize(word)\n",
    "        print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n",
    "\n",
    "# Test comparison\n",
    "test_words = ['running', 'runs', 'ran', 'better', 'good', 'feet', 'foot', 'geese', 'goose', 'mice', 'mouse']\n",
    "print(\"Stemming vs Lemmatization:\")\n",
    "compare_stemming_lemmatization(test_words)\n",
    "\n",
    "# NLTK lemmatization\n",
    "def lemmatize_text_nltk(text):\n",
    "    \"\"\"\n",
    "    Apply NLTK lemmatization to text\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [nltk_lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# spaCy lemmatization (more advanced)\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"\n",
    "    Apply spaCy lemmatization to text\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization\n",
    "text_lemmatized_nltk = lemmatize_text_nltk(text_no_numbers)\n",
    "text_lemmatized_spacy = lemmatize_text_spacy(text_no_numbers)\n",
    "\n",
    "print(\"\\nSample text after NLTK lemmatization:\")\n",
    "print(text_lemmatized_nltk[:200] + \"...\")\n",
    "\n",
    "print(\"\\nSample text after spaCy lemmatization:\")\n",
    "print(text_lemmatized_spacy[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stopword_removal"
   },
   "source": [
    "## 9. Stop Word Removal\n",
    "\n",
    "Removing common words that don't carry much semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stopword_handling"
   },
   "outputs": [],
   "source": [
    "# Get English stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = {'also', 'would', 'could', 'should'}\n",
    "\n",
    "print(\"NLTK stop words (first 20):\", list(stop_words_nltk)[:20])\n",
    "print(f\"\\nTotal NLTK stop words: {len(stop_words_nltk)}\")\n",
    "print(f\"Total spaCy stop words: {len(stop_words_spacy)}\")\n",
    "\n",
    "def remove_stopwords(text, stop_words=stop_words_nltk):\n",
    "    \"\"\"\n",
    "    Remove stop words from text\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"\n",
    "    Remove stop words using spaCy\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Test stop word removal\n",
    "stopword_test = \"This is a sample sentence with many common stop words that should be removed.\"\n",
    "print(\"\\nOriginal:\", stopword_test)\n",
    "print(\"Without stop words (NLTK):\", remove_stopwords(stopword_test))\n",
    "print(\"Without stop words (spaCy):\", remove_stopwords_spacy(stopword_test))\n",
    "\n",
    "# Apply to our text\n",
    "text_no_stopwords = remove_stopwords(text_lemmatized_spacy)\n",
    "print(\"\\nSample text after stop word removal:\")\n",
    "print(text_no_stopwords[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comprehensive_normalization"
   },
   "source": [
    "## 10. Comprehensive Normalization Pipeline\n",
    "\n",
    "Combining all normalization techniques into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalization_pipeline"
   },
   "outputs": [],
   "source": [
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    A comprehensive text normalization pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def normalize(self, text, \n",
    "                  lowercase=True,\n",
    "                  remove_punctuation=True,\n",
    "                  remove_numbers=True,\n",
    "                  expand_contractions=True,\n",
    "                  remove_accents=True,\n",
    "                  lemmatize=True,\n",
    "                  stem=False,\n",
    "                  remove_stopwords=True,\n",
    "                  remove_extra_whitespace=True):\n",
    "        \"\"\"\n",
    "        Apply normalization pipeline to text\n",
    "        \"\"\"\n",
    "        result = text\n",
    "        \n",
    "        # Step 1: Expand contractions (before lowercasing)\n",
    "        if expand_contractions:\n",
    "            result = contractions.fix(result)\n",
    "        \n",
    "        # Step 2: Convert to lowercase\n",
    "        if lowercase:\n",
    "            result = result.lower()\n",
    "        \n",
    "        # Step 3: Remove accents\n",
    "        if remove_accents:\n",
    "            result = unidecode(result)\n",
    "        \n",
    "        # Step 4: Remove numbers\n",
    "        if remove_numbers:\n",
    "            result = re.sub(r'\\d+', '', result)\n",
    "        \n",
    "        # Step 5: Remove punctuation\n",
    "        if remove_punctuation:\n",
    "            result = result.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Step 6: Normalize whitespace\n",
    "        if remove_extra_whitespace:\n",
    "            result = re.sub(r'\\s+', ' ', result).strip()\n",
    "        \n",
    "        # Step 7: Lemmatization or stemming\n",
    "        if lemmatize or stem or remove_stopwords:\n",
    "            words = word_tokenize(result)\n",
    "            \n",
    "            if remove_stopwords:\n",
    "                words = [word for word in words if word not in self.stop_words]\n",
    "            \n",
    "            if lemmatize:\n",
    "                words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            elif stem:\n",
    "                words = [self.stemmer.stem(word) for word in words]\n",
    "            \n",
    "            result = ' '.join(words)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test the comprehensive normalizer\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Apply different normalization levels\n",
    "basic_norm = normalizer.normalize(sample_text, \n",
    "                                  lemmatize=False, \n",
    "                                  remove_stopwords=False)\n",
    "print(\"\\nBasic normalization:\")\n",
    "print(basic_norm)\n",
    "\n",
    "full_norm = normalizer.normalize(sample_text)\n",
    "print(\"\\nFull normalization:\")\n",
    "print(full_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain_specific"
   },
   "source": [
    "## 11. Domain-Specific Normalization\n",
    "\n",
    "Different domains may require special normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "social_media_normalization"
   },
   "outputs": [],
   "source": [
    "def normalize_social_media_text(text):\n",
    "    \"\"\"\n",
    "    Normalization specific to social media text\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace user mentions\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    \n",
    "    # Replace hashtags but keep the content\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Replace multiple exclamation marks\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    \n",
    "    # Replace multiple question marks\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "    \n",
    "    # Replace elongated words (e.g., \"sooooo\" -> \"so\")\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_email_text(text):\n",
    "    \"\"\"\n",
    "    Normalization specific to email text\n",
    "    \"\"\"\n",
    "    # Remove email headers patterns\n",
    "    text = re.sub(r'^From:.*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^To:.*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Date:.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email signatures (lines starting with --)\n",
    "    text = re.sub(r'^--.*', '', text, flags=re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    # Remove quoted text (lines starting with >)\n",
    "    text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test domain-specific normalization\n",
    "social_media_text = \"\"\"\n",
    "OMG!!! This is sooooo amazing!!! Check out @username and visit https://example.com \n",
    "#NLP #MachineLearning #AI Loooove this!!!\n",
    "\"\"\"\n",
    "\n",
    "email_text = \"\"\"\n",
    "From: sender@example.com\n",
    "To: recipient@example.com\n",
    "Subject: Meeting Tomorrow\n",
    "\n",
    "Hi there,\n",
    "\n",
    "Can we meet tomorrow at 3 PM?\n",
    "\n",
    "> Previous message:\n",
    "> Yes, that works for me.\n",
    "\n",
    "Thanks!\n",
    "\n",
    "--\n",
    "Best regards,\n",
    "John Doe\n",
    "john@company.com\n",
    "\"\"\"\n",
    "\n",
    "print(\"Social Media Text (Original):\")\n",
    "print(social_media_text)\n",
    "print(\"\\nSocial Media Text (Normalized):\")\n",
    "print(normalize_social_media_text(social_media_text))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nEmail Text (Original):\")\n",
    "print(email_text)\n",
    "print(\"\\nEmail Text (Normalized):\")\n",
    "print(normalize_email_text(email_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_metrics"
   },
   "source": [
    "## 12. Evaluation and Comparison\n",
    "\n",
    "How to evaluate the effectiveness of different normalization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalization_evaluation"
   },
   "outputs": [],
   "source": [
    "def analyze_normalization_impact(original_text, normalized_text):\n",
    "    \"\"\"\n",
    "    Analyze the impact of normalization on text\n",
    "    \"\"\"\n",
    "    original_tokens = word_tokenize(original_text.lower())\n",
    "    normalized_tokens = word_tokenize(normalized_text.lower())\n",
    "    \n",
    "    original_vocab = set(original_tokens)\n",
    "    normalized_vocab = set(normalized_tokens)\n",
    "    \n",
    "    print(\"Normalization Impact Analysis:\")\n",
    "    print(f\"Original tokens: {len(original_tokens)}\")\n",
    "    print(f\"Normalized tokens: {len(normalized_tokens)}\")\n",
    "    print(f\"Token reduction: {len(original_tokens) - len(normalized_tokens)} ({((len(original_tokens) - len(normalized_tokens)) / len(original_tokens) * 100):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOriginal vocabulary size: {len(original_vocab)}\")\n",
    "    print(f\"Normalized vocabulary size: {len(normalized_vocab)}\")\n",
    "    print(f\"Vocabulary reduction: {len(original_vocab) - len(normalized_vocab)} ({((len(original_vocab) - len(normalized_vocab)) / len(original_vocab) * 100):.1f}%)\")\n",
    "    \n",
    "    # Show examples of removed tokens\n",
    "    removed_tokens = original_vocab - normalized_vocab\n",
    "    if removed_tokens:\n",
    "        print(f\"\\nExamples of removed tokens: {list(removed_tokens)[:10]}\")\n",
    "\n",
    "# Compare different normalization strategies\n",
    "print(\"Comparing normalization strategies on sample text:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Strategy 1: Minimal normalization\n",
    "minimal_norm = normalizer.normalize(sample_text, \n",
    "                                    remove_punctuation=False,\n",
    "                                    remove_numbers=False,\n",
    "                                    lemmatize=False,\n",
    "                                    remove_stopwords=False)\n",
    "print(\"\\n1. Minimal Normalization (lowercase + whitespace only):\")\n",
    "analyze_normalization_impact(sample_text, minimal_norm)\n",
    "\n",
    "# Strategy 2: Medium normalization\n",
    "medium_norm = normalizer.normalize(sample_text,\n",
    "                                   lemmatize=False,\n",
    "                                   remove_stopwords=False)\n",
    "print(\"\\n2. Medium Normalization (+ punctuation + numbers):\")\n",
    "analyze_normalization_impact(sample_text, medium_norm)\n",
    "\n",
    "# Strategy 3: Full normalization\n",
    "full_norm = normalizer.normalize(sample_text)\n",
    "print(\"\\n3. Full Normalization (+ lemmatization + stop words):\")\n",
    "analyze_normalization_impact(sample_text, full_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 13. Exercises\n",
    "\n",
    "Practice text normalization with these exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_1"
   },
   "source": [
    "### Exercise 1: Custom Normalization Function\n",
    "\n",
    "Create a normalization function for a specific domain (e.g., product reviews, scientific papers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_normalization_exercise"
   },
   "outputs": [],
   "source": [
    "def normalize_product_reviews(text):\n",
    "    \"\"\"\n",
    "    TODO: Implement normalization specific to product reviews\n",
    "    Consider:\n",
    "    - Handling star ratings (e.g., \"5/5 stars\", \"★★★★★\")\n",
    "    - Price mentions (e.g., \"$19.99\", \"twenty dollars\")\n",
    "    - Product model numbers\n",
    "    - Common review phrases\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with sample product review\n",
    "product_review = \"\"\"\n",
    "This product is AMAZING!!! I bought it for $29.99 and it's worth every penny.\n",
    "Model XYZ-123 works perfectly. I'd give it 5/5 stars ★★★★★.\n",
    "Definitely recommend to anyone. Fast shipping too!!!\n",
    "\"\"\"\n",
    "\n",
    "# Your test code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_2"
   },
   "source": [
    "### Exercise 2: Normalization Comparison\n",
    "\n",
    "Compare the effect of different normalization strategies on a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalization_comparison_exercise"
   },
   "outputs": [],
   "source": [
    "# Sample dataset of movie reviews (simplified)\n",
    "movie_reviews = [\n",
    "    (\"This movie is FANTASTIC!!! I loved it so much. Best film I've seen this year!\", \"positive\"),\n",
    "    (\"Terrible movie. Don't waste your time. Acting was awful.\", \"negative\"),\n",
    "    (\"It's an okay film. Not great, not terrible. Just average.\", \"neutral\"),\n",
    "    (\"AMAZING storyline and great acting! Highly recommended!!!\", \"positive\"),\n",
    "    (\"Boring and predictable. Couldn't wait for it to end.\", \"negative\")\n",
    "]\n",
    "\n",
    "# TODO: Apply different normalization strategies and analyze:\n",
    "# 1. Vocabulary size for each strategy\n",
    "# 2. Most common words for each strategy\n",
    "# 3. Which strategy might work better for sentiment classification?\n",
    "\n",
    "# Your analysis code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "key_takeaways"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Normalization is context-dependent**:\n",
    "   - Social media text needs different handling than formal documents\n",
    "   - Consider your downstream task when choosing normalization steps\n",
    "\n",
    "2. **Order matters**:\n",
    "   - Expand contractions before lowercasing\n",
    "   - Handle special characters before removing punctuation\n",
    "   - Apply lemmatization/stemming after other preprocessing\n",
    "\n",
    "3. **Trade-offs to consider**:\n",
    "   - **Information loss**: Aggressive normalization removes information\n",
    "   - **Computational cost**: Some methods are more expensive\n",
    "   - **Domain specificity**: Generic approaches may not work for specialized text\n",
    "\n",
    "4. **Stemming vs Lemmatization**:\n",
    "   - Stemming is faster but can be inaccurate\n",
    "   - Lemmatization is more accurate but slower\n",
    "   - spaCy lemmatization is generally better than NLTK\n",
    "\n",
    "5. **Stop word removal**:\n",
    "   - Useful for some tasks (topic modeling, search)\n",
    "   - May hurt performance for others (sentiment analysis, machine translation)\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Start simple**: Begin with basic normalization and add complexity as needed\n",
    "2. **Evaluate impact**: Measure how normalization affects your specific task\n",
    "3. **Keep originals**: Always preserve original text for reference\n",
    "4. **Document choices**: Record what normalization steps you applied and why\n",
    "5. **Consider alternatives**: Modern transformer models often work well with minimal preprocessing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn about advanced preprocessing techniques\n",
    "- Explore language-specific normalization challenges\n",
    "- Study the impact of normalization on different NLP tasks\n",
    "- Practice with real-world datasets from your domain of interest\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [NLTK Book - Text Preprocessing](https://www.nltk.org/book/ch03.html)\n",
    "- [spaCy Linguistic Features](https://spacy.io/usage/linguistic-features)\n",
    "- [Text Preprocessing in Python](https://realpython.com/python-string-formatting/)\n",
    "- [Unicode in Python](https://docs.python.org/3/howto/unicode.html)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}