{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "classification_title"
   },
   "source": [
    "# Text Classification in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/text-classification.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text classification is the task of assigning predefined categories or labels to text documents. It's one of the most common NLP applications, used for spam detection, sentiment analysis, topic categorization, and more.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Text preprocessing for classification\n",
    "- Feature extraction techniques\n",
    "- Traditional ML algorithms\n",
    "- Deep learning approaches\n",
    "- Transformer-based classification\n",
    "- Evaluation metrics\n",
    "- Real-world applications\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, machine learning, and NLP preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"scikit-learn\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"nltk\",\n",
    "    \"wordcloud\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"‚úì {package}\")\n",
    "\n",
    "print(\"\\nüéâ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# NLP and ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data with error handling\n",
    "nltk_datasets = ['punkt', 'punkt_tab', 'stopwords']\n",
    "print(\"Downloading NLTK datasets...\")\n",
    "for dataset in nltk_datasets:\n",
    "    try:\n",
    "        nltk.download(dataset, quiet=True)\n",
    "        print(f\"‚úì {dataset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to download {dataset}: {e}\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Note: Transformers models will be loaded when needed due to potential network requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_dataset"
   },
   "source": [
    "## Sample Dataset Creation\n",
    "\n",
    "Let's create a sample dataset for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset"
   },
   "outputs": [],
   "source": [
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample text classification dataset\"\"\"\n",
    "    \n",
    "    # Technology articles\n",
    "    tech_texts = [\n",
    "        \"Artificial intelligence is revolutionizing the tech industry with machine learning algorithms.\",\n",
    "        \"New smartphone features include advanced camera technology and faster processors.\",\n",
    "        \"Cloud computing services are becoming more popular among businesses worldwide.\",\n",
    "        \"Cybersecurity threats are increasing as more companies move to digital platforms.\",\n",
    "        \"Software development practices are evolving with new programming languages and frameworks.\",\n",
    "        \"Data science and analytics are driving business intelligence and decision making.\",\n",
    "        \"Virtual reality and augmented reality technologies are creating new user experiences.\",\n",
    "        \"Internet of Things devices are connecting everyday objects to the digital world.\"\n",
    "    ]\n",
    "    \n",
    "    # Sports articles\n",
    "    sports_texts = [\n",
    "        \"The football team won the championship after a thrilling final match.\",\n",
    "        \"Basketball players are training hard for the upcoming tournament season.\",\n",
    "        \"Olympic athletes are preparing for the games with intensive workout routines.\",\n",
    "        \"Soccer fans are excited about the world cup matches this summer.\",\n",
    "        \"Tennis players compete in grand slam tournaments around the world.\",\n",
    "        \"Swimming records were broken during the international competition last week.\",\n",
    "        \"Baseball season starts with teams showing strong performance in training.\",\n",
    "        \"Golf championship attracts professional players from different countries.\"\n",
    "    ]\n",
    "    \n",
    "    # Health articles\n",
    "    health_texts = [\n",
    "        \"Regular exercise and healthy diet are essential for maintaining good health.\",\n",
    "        \"Medical research shows the benefits of preventive care and early diagnosis.\",\n",
    "        \"Mental health awareness is increasing with better access to therapy and counseling.\",\n",
    "        \"Vaccination programs are crucial for preventing infectious disease outbreaks.\",\n",
    "        \"Nutrition experts recommend balanced meals with fruits and vegetables.\",\n",
    "        \"Healthcare systems are adapting to serve aging populations better.\",\n",
    "        \"Medical technology advances are improving patient care and treatment outcomes.\",\n",
    "        \"Public health initiatives focus on community wellness and disease prevention.\"\n",
    "    ]\n",
    "    \n",
    "    # Business articles\n",
    "    business_texts = [\n",
    "        \"Stock market analysis shows positive trends in technology sector investments.\",\n",
    "        \"Corporate earnings reports indicate strong financial performance this quarter.\",\n",
    "        \"Business strategies are adapting to changing market conditions and consumer behavior.\",\n",
    "        \"Startup companies are securing funding through venture capital and investor networks.\",\n",
    "        \"International trade agreements are affecting global supply chain operations.\",\n",
    "        \"Marketing campaigns are leveraging social media platforms for brand awareness.\",\n",
    "        \"Economic indicators suggest steady growth in manufacturing and services sectors.\",\n",
    "        \"Business leadership focuses on innovation and sustainable development practices.\"\n",
    "    ]\n",
    "    \n",
    "    # Create dataset\n",
    "    data = []\n",
    "    \n",
    "    for text in tech_texts:\n",
    "        data.append({'text': text, 'category': 'Technology'})\n",
    "    for text in sports_texts:\n",
    "        data.append({'text': text, 'category': 'Sports'})\n",
    "    for text in health_texts:\n",
    "        data.append({'text': text, 'category': 'Health'})\n",
    "    for text in business_texts:\n",
    "        data.append({'text': text, 'category': 'Business'})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create and explore dataset\n",
    "df = create_sample_dataset()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Display sample texts\n",
    "print(\"\\nSample texts:\")\n",
    "for category in df['category'].unique():\n",
    "    sample_text = df[df['category'] == category]['text'].iloc[0]\n",
    "    print(f\"{category}: {sample_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## Text Preprocessing for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text_preprocessing"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=True, lowercase=True):\n",
    "    \"\"\"Preprocess text for classification\"\"\"\n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing comparison:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['text_processed'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_extraction"
   },
   "source": [
    "## Feature Extraction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_methods"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['text_processed']\n",
    "y = df['category']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 1. Bag of Words (Count Vectorizer)\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# 2. TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Feature extraction results:\")\n",
    "print(f\"Count Vectorizer - Train shape: {X_train_counts.shape}, Test shape: {X_test_counts.shape}\")\n",
    "print(f\"TF-IDF Vectorizer - Train shape: {X_train_tfidf.shape}, Test shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nSample features: {list(feature_names[:10])}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "counts_sum = np.array(X_train_counts.sum(axis=0)).flatten()\n",
    "plt.hist(counts_sum, bins=30, alpha=0.7)\n",
    "plt.title('Count Vectorizer Feature Distribution')\n",
    "plt.xlabel('Feature Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "tfidf_sum = np.array(X_train_tfidf.sum(axis=0)).flatten()\n",
    "plt.hist(tfidf_sum, bins=30, alpha=0.7)\n",
    "plt.title('TF-IDF Feature Distribution')\n",
    "plt.xlabel('TF-IDF Score Sum')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "traditional_ml"
   },
   "source": [
    "## Traditional Machine Learning Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml_classifiers"
   },
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "\n",
    "print(\"Traditional ML Classification Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Train on TF-IDF features\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(classifier, X_train_tfidf, y_train, cv=3)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = list(results.keys())\n",
    "accuracies = [results[method]['accuracy'] for method in methods]\n",
    "cv_means = [results[method]['cv_mean'] for method in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.7)\n",
    "axes[0].bar(x + width/2, cv_means, width, label='CV Mean', alpha=0.7)\n",
    "axes[0].set_xlabel('Classifier')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Classifier Performance Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(methods, rotation=45)\n",
    "axes[0].legend()\n",
    "\n",
    "# Confusion matrix for best performer\n",
    "best_classifier = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "cm = confusion_matrix(y_test, results[best_classifier]['predictions'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classifiers[best_classifier].classes_,\n",
    "            yticklabels=classifiers[best_classifier].classes_,\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix - {best_classifier}')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report for best classifier\n",
    "print(f\"\\nDetailed Classification Report - {best_classifier}:\")\n",
    "print(classification_report(y_test, results[best_classifier]['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transformer_classification"
   },
   "source": [
    "## Transformer-Based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer_approach"
   },
   "outputs": [],
   "source": [
    "# Initialize zero-shot classification pipeline\n",
    "try:\n",
    "    zero_shot_classifier = pipeline(\"zero-shot-classification\", \n",
    "                                   model=\"facebook/bart-large-mnli\")\n",
    "    print(\"Loaded BART model for zero-shot classification\")\nexcept:\n",
    "    try:\n",
    "        zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
    "        print(\"Loaded default zero-shot classification model\")\n",
    "    except:\n",
    "        zero_shot_classifier = None\n",
    "        print(\"Could not load transformer classification model\")\n",
    "\n",
    "def transformer_classification(texts, candidate_labels):\n",
    "    \"\"\"Classify texts using transformer model\"\"\"\n",
    "    if zero_shot_classifier is None:\n",
    "        return [{'labels': ['N/A'], 'scores': [0.0]} for _ in texts]\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            result = zero_shot_classifier(text, candidate_labels)\n",
    "            results.append(result)\n",
    "        except:\n",
    "            results.append({'labels': ['Error'], 'scores': [0.0]})\n",
    "    \n",
    "    return results\n",
    "\n",
    "if zero_shot_classifier:\n",
    "    # Test transformer classification\n",
    "    candidate_labels = ['Technology', 'Sports', 'Health', 'Business']\n",
    "    test_texts = X_test.tolist()\n",
    "    \n",
    "    print(\"\\nTransformer-Based Classification:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Classify a few test examples\n",
    "    sample_results = transformer_classification(test_texts[:5], candidate_labels)\n",
    "    \n",
    "    transformer_predictions = []\n",
    "    transformer_confidences = []\n",
    "    \n",
    "    for i, (text, result) in enumerate(zip(test_texts[:5], sample_results)):\n",
    "        predicted_label = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        actual_label = y_test.iloc[i]\n",
    "        \n",
    "        transformer_predictions.append(predicted_label)\n",
    "        transformer_confidences.append(confidence)\n",
    "        \n",
    "        print(f\"\\nText: {text[:100]}...\")\n",
    "        print(f\"Actual: {actual_label}\")\n",
    "        print(f\"Predicted: {predicted_label} (confidence: {confidence:.3f})\")\n",
    "        print(f\"All scores: {dict(zip(result['labels'], result['scores']))}\")\n",
    "    \n",
    "    # Calculate accuracy for the sample\n",
    "    sample_accuracy = sum(1 for pred, actual in zip(transformer_predictions, y_test.iloc[:5]) \n",
    "                         if pred == actual) / len(transformer_predictions)\n",
    "    \n",
    "    print(f\"\\nTransformer Sample Accuracy: {sample_accuracy:.3f}\")\n",
    "    print(f\"Average Confidence: {np.mean(transformer_confidences):.3f}\")\nelse:\n",
    "    print(\"Transformer classification not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_importance"
   },
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_features"
   },
   "outputs": [],
   "source": [
    "# Analyze feature importance using the best traditional classifier\n",
    "best_model = classifiers[best_classifier]\n",
    "\n",
    "def get_top_features(classifier, vectorizer, class_labels, top_n=5):\n",
    "    \"\"\"Get top features for each class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    if hasattr(classifier, 'coef_'):\n",
    "        # For linear models\n",
    "        coef = classifier.coef_\n",
    "    elif hasattr(classifier, 'feature_importances_'):\n",
    "        # For tree-based models\n",
    "        coef = classifier.feature_importances_.reshape(1, -1)\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "    top_features = {}\n",
    "    \n",
    "    if len(coef.shape) > 1 and coef.shape[0] > 1:\n",
    "        # Multi-class\n",
    "        for i, class_label in enumerate(class_labels):\n",
    "            if i < coef.shape[0]:\n",
    "                top_indices = coef[i].argsort()[-top_n:][::-1]\n",
    "                top_features[class_label] = [\n",
    "                    (feature_names[idx], coef[i][idx]) \n",
    "                    for idx in top_indices\n",
    "                ]\n",
    "    else:\n",
    "        # Binary or single feature importance\n",
    "        feature_scores = coef.flatten()\n",
    "        top_indices = feature_scores.argsort()[-top_n:][::-1]\n",
    "        top_features['overall'] = [\n",
    "            (feature_names[idx], feature_scores[idx]) \n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# Get top features\n",
    "class_labels = best_model.classes_\n",
    "top_features = get_top_features(best_model, tfidf_vectorizer, class_labels)\n",
    "\n",
    "print(f\"Top Features Analysis - {best_classifier}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for class_label, features in top_features.items():\n",
    "    print(f\"\\n{class_label}:\")\n",
    "    for feature, score in features:\n",
    "        print(f\"  {feature}: {score:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "if len(top_features) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (class_label, features) in enumerate(top_features.items()):\n",
    "        if i < len(axes):\n",
    "            words = [f[0] for f in features]\n",
    "            scores = [f[1] for f in features]\n",
    "            \n",
    "            axes[i].barh(words, scores)\n",
    "            axes[i].set_title(f'Top Features - {class_label}')\n",
    "            axes[i].set_xlabel('Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "error_analysis"
   },
   "source": [
    "## Error Analysis and Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_errors"
   },
   "outputs": [],
   "source": [
    "# Analyze misclassified examples\n",
    "best_predictions = results[best_classifier]['predictions']\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, best_predictions)) \n",
    "                        if true != pred]\n",
    "\n",
    "print(f\"Error Analysis - {best_classifier}:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total misclassified: {len(misclassified_indices)} out of {len(y_test)}\")\n",
    "print(f\"Error rate: {len(misclassified_indices)/len(y_test):.3f}\")\n",
    "\n",
    "if misclassified_indices:\n",
    "    print(\"\\nMisclassified Examples:\")\n",
    "    for i in misclassified_indices[:3]:  # Show first 3 errors\n",
    "        text_idx = y_test.index[i]\n",
    "        original_text = df.loc[text_idx, 'text']\n",
    "        true_label = y_test.iloc[i]\n",
    "        pred_label = best_predictions[i]\n",
    "        \n",
    "        print(f\"\\nText: {original_text}\")\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        print(f\"Predicted Label: {pred_label}\")\n",
    "\n",
    "# Prediction confidence analysis\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    prediction_probabilities = best_model.predict_proba(X_test_tfidf)\n",
    "    max_probabilities = np.max(prediction_probabilities, axis=1)\n",
    "    \n",
    "    # Analyze confidence distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(max_probabilities, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Maximum Prediction Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    \n",
    "    # Confidence vs accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    correct_predictions = (best_predictions == y_test).astype(int)\n",
    "    \n",
    "    # Bin by confidence\n",
    "    confidence_bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "    bin_accuracies = []\n",
    "    \n",
    "    for i in range(len(confidence_bins)-1):\n",
    "        mask = (max_probabilities >= confidence_bins[i]) & (max_probabilities < confidence_bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_accuracy = np.mean(correct_predictions[mask])\n",
    "        else:\n",
    "            bin_accuracy = 0\n",
    "        bin_accuracies.append(bin_accuracy)\n",
    "    \n",
    "    plt.plot(bin_centers, bin_accuracies, 'o-')\n",
    "    plt.xlabel('Prediction Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Confidence vs Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAverage prediction confidence: {np.mean(max_probabilities):.3f}\")\n",
    "    print(f\"Minimum confidence: {np.min(max_probabilities):.3f}\")\n",
    "    print(f\"Maximum confidence: {np.max(max_probabilities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "applications"
   },
   "source": [
    "## Real-World Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "applications_demo"
   },
   "outputs": [],
   "source": [
    "# Application 1: Email Classification\n",
    "def email_classifier_demo():\n",
    "    \"\"\"Demo email classification system\"\"\"\n",
    "    email_texts = [\n",
    "        \"Congratulations! You've won $1 million. Click here to claim your prize now!\",\n",
    "        \"Meeting scheduled for tomorrow at 2 PM in conference room A. Please confirm attendance.\",\n",
    "        \"Your account has been compromised. Verify your identity immediately by clicking this link.\",\n",
    "        \"Weekly team report attached. Please review and provide feedback by Friday.\",\n",
    "        \"Special offer: 50% off all products! Limited time only. Shop now and save big!\"\n",
    "    ]\n",
    "    \n",
    "    email_categories = ['spam', 'work', 'phishing', 'work', 'promotional']\n",
    "    \n",
    "    return email_texts, email_categories\n",
    "\n",
    "# Application 2: Customer Feedback Classification\n",
    "def feedback_classifier_demo():\n",
    "    \"\"\"Demo customer feedback classification\"\"\"\n",
    "    feedback_texts = [\n",
    "        \"The product quality is excellent and delivery was fast. Highly recommend!\",\n",
    "        \"Terrible customer service. My issue was not resolved after multiple calls.\",\n",
    "        \"Good value for money. Product works as expected but could be improved.\",\n",
    "        \"Outstanding experience! Will definitely purchase again. Five stars!\",\n",
    "        \"Product broke after one week. Very disappointed with the quality.\"\n",
    "    ]\n",
    "    \n",
    "    sentiment_categories = ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
    "    \n",
    "    return feedback_texts, sentiment_categories\n",
    "\n",
    "# Application 3: News Category Classification\n",
    "def classify_new_documents(texts, classifier, vectorizer):\n",
    "    \"\"\"Classify new documents using trained model\"\"\"\n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vectors = vectorizer.transform(processed_texts)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = classifier.predict(text_vectors)\n",
    "    \n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        probabilities = classifier.predict_proba(text_vectors)\n",
    "        confidences = np.max(probabilities, axis=1)\n",
    "    else:\n",
    "        confidences = [1.0] * len(predictions)\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "# Test applications\n",
    "print(\"Real-World Application Demos:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demo 1: News classification with our trained model\n",
    "new_articles = [\n",
    "    \"Scientists develop new vaccine that shows promising results in clinical trials.\",\n",
    "    \"Cryptocurrency prices surge as institutional investors increase their holdings.\",\n",
    "    \"Olympic games feature exciting competitions with athletes breaking world records.\",\n",
    "    \"New smartphone app uses artificial intelligence to improve user productivity.\"\n",
    "]\n",
    "\n",
    "predictions, confidences = classify_new_documents(new_articles, best_model, tfidf_vectorizer)\n",
    "\n",
    "print(\"1. News Article Classification:\")\n",
    "for text, pred, conf in zip(new_articles, predictions, confidences):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Category: {pred} (confidence: {conf:.3f})\")\n",
    "\n",
    "# Demo 2: Email classification (zero-shot with transformer)\n",
    "if zero_shot_classifier:\n",
    "    email_texts, _ = email_classifier_demo()\n",
    "    email_labels = ['spam', 'work', 'personal', 'promotional']\n",
    "    \n",
    "    print(\"\\n2. Email Classification (Transformer):\")\n",
    "    email_results = transformer_classification(email_texts[:3], email_labels)\n",
    "    \n",
    "    for text, result in zip(email_texts[:3], email_results):\n",
    "        print(f\"\\nEmail: {text[:60]}...\")\n",
    "        print(f\"Classification: {result['labels'][0]} (score: {result['scores'][0]:.3f})\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n3. Model Performance Summary:\")\n",
    "print(f\"Best Traditional Model: {best_classifier}\")\n",
    "print(f\"Test Accuracy: {results[best_classifier]['accuracy']:.3f}\")\n",
    "print(f\"Number of Features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Training Examples: {X_train_tfidf.shape[0]}\")\n",
    "print(f\"Test Examples: {X_test_tfidf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Multi-label Classification**: Modify the approach for documents that can belong to multiple categories\n",
    "2. **Imbalanced Dataset Handling**: Implement techniques for dealing with uneven class distributions\n",
    "3. **Feature Engineering**: Create custom features like text length, readability scores, etc.\n",
    "4. **Model Ensemble**: Combine multiple classifiers for improved performance\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Preprocessing matters**: Proper text cleaning and normalization significantly impact performance\n",
    "- **Feature extraction choices**: TF-IDF often outperforms simple bag-of-words for classification\n",
    "- **Model selection**: Different algorithms work better for different types of text data\n",
    "- **Evaluation beyond accuracy**: Consider precision, recall, and F1-score for each class\n",
    "- **Transformer models**: Provide excellent performance but require more computational resources\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Start simple**: Begin with basic features and traditional algorithms\n",
    "2. **Cross-validation**: Always use cross-validation to get reliable performance estimates\n",
    "3. **Error analysis**: Examine misclassified examples to understand model limitations\n",
    "4. **Feature engineering**: Domain-specific features can significantly improve performance\n",
    "5. **Class imbalance**: Handle uneven class distributions with appropriate techniques\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Spam detection**: Classify emails as spam or legitimate\n",
    "- **Sentiment analysis**: Determine emotional tone of text\n",
    "- **Topic categorization**: Automatically organize documents by subject\n",
    "- **Intent classification**: Understand user intentions in chatbots\n",
    "- **Content moderation**: Identify inappropriate or harmful content\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn about deep learning approaches (LSTM, CNN for text)\n",
    "- Explore advanced transformer architectures (BERT, RoBERTa)\n",
    "- Study multi-label and hierarchical classification\n",
    "- Practice with real-world datasets from your domain\n",
    "- Learn about active learning and few-shot classification\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Scikit-learn Text Classification](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
    "- [Text Classification Datasets](https://www.tensorflow.org/datasets/catalog/overview#text)\n",
    "- [GLUE Benchmark](https://gluebenchmark.com/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}