{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos_tagging_title"
   },
   "source": [
    "# Part-of-Speech (POS) Tagging in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/pos-tagging.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Part-of-Speech (POS) tagging is the process of assigning grammatical categories (such as noun, verb, adjective, etc.) to each word in a text. This fundamental NLP task provides crucial linguistic information that can improve the performance of many downstream applications.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Understanding POS tags and their importance\n",
    "- Different POS tagging approaches\n",
    "- Using NLTK for POS tagging\n",
    "- Using spaCy for advanced POS tagging\n",
    "- Rule-based vs statistical approaches\n",
    "- Evaluation metrics for POS tagging\n",
    "- Applications of POS tagging\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, linguistics concepts, and NLP preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Let's install the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libraries"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install nltk spacy pandas matplotlib seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import NLTK modules\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import brown, treebank\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "understanding_pos"
   },
   "source": [
    "## 1. Understanding POS Tags\n",
    "\n",
    "Let's start by understanding what POS tags are and their common categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pos_tag_explanation"
   },
   "outputs": [],
   "source": [
    "# Common POS tag categories and examples\n",
    "pos_categories = {\n",
    "    'Noun': {\n",
    "        'description': 'Words that represent people, places, things, or ideas',\n",
    "        'tags': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'examples': ['cat', 'cats', 'London', 'Americans']\n",
    "    },\n",
    "    'Verb': {\n",
    "        'description': 'Words that express actions, states, or occurrences',\n",
    "        'tags': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'examples': ['run', 'ran', 'running', 'eaten', 'eat', 'eats']\n",
    "    },\n",
    "    'Adjective': {\n",
    "        'description': 'Words that describe or modify nouns',\n",
    "        'tags': ['JJ', 'JJR', 'JJS'],\n",
    "        'examples': ['big', 'bigger', 'biggest']\n",
    "    },\n",
    "    'Adverb': {\n",
    "        'description': 'Words that modify verbs, adjectives, or other adverbs',\n",
    "        'tags': ['RB', 'RBR', 'RBS'],\n",
    "        'examples': ['quickly', 'more quickly', 'most quickly']\n",
    "    },\n",
    "    'Pronoun': {\n",
    "        'description': 'Words that replace nouns',\n",
    "        'tags': ['PRP', 'PRP$'],\n",
    "        'examples': ['he', 'she', 'his', 'her']\n",
    "    },\n",
    "    'Preposition': {\n",
    "        'description': 'Words that show relationships between words',\n",
    "        'tags': ['IN'],\n",
    "        'examples': ['in', 'on', 'at', 'over']\n",
    "    },\n",
    "    'Determiner': {\n",
    "        'description': 'Words that introduce nouns',\n",
    "        'tags': ['DT'],\n",
    "        'examples': ['the', 'a', 'an', 'this', 'that']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Common Part-of-Speech Categories:\")\n",
    "print(\"=\" * 50)\n",
    "for category, info in pos_categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(f\"  Penn Treebank Tags: {', '.join(info['tags'])}\")\n",
    "    print(f\"  Examples: {', '.join(info['examples'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_text"
   },
   "source": [
    "## 2. Sample Text for POS Tagging\n",
    "\n",
    "Let's use various sentences to demonstrate different POS tagging scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_sample_texts"
   },
   "outputs": [],
   "source": [
    "# Different types of sentences for testing POS tagging\n",
    "sample_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Natural Language Processing is fascinating and challenging.\",\n",
    "    \"I am learning about machine learning algorithms.\",\n",
    "    \"The conference will be held in San Francisco next year.\",\n",
    "    \"She quickly ran to the store yesterday.\",\n",
    "    \"The beautiful, old building was demolished last week.\",\n",
    "    \"Can you help me with this difficult problem?\",\n",
    "    \"Time flies like an arrow; fruit flies like a banana.\",  # Ambiguous sentence\n",
    "    \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\"  # Very ambiguous\n",
    "]\n",
    "\n",
    "print(\"Sample sentences for POS tagging analysis:\")\n",
    "for i, sentence in enumerate(sample_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nltk_pos_tagging"
   },
   "source": [
    "## 3. NLTK POS Tagging\n",
    "\n",
    "NLTK provides several POS taggers with different tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic_nltk_tagging"
   },
   "outputs": [],
   "source": [
    "def analyze_sentence_nltk(sentence):\n",
    "    \"\"\"\n",
    "    Analyze a sentence using NLTK POS tagging\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # POS tagging with Penn Treebank tagset\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # POS tagging with Universal tagset\n",
    "    universal_tags = pos_tag(tokens, tagset='universal')\n",
    "    \n",
    "    return tokens, pos_tags, universal_tags\n",
    "\n",
    "# Analyze the first few sentences\n",
    "print(\"NLTK POS Tagging Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, sentence in enumerate(sample_sentences[:3], 1):\n",
    "    print(f\"\\nSentence {i}: {sentence}\")\n",
    "    tokens, pos_tags, universal_tags = analyze_sentence_nltk(sentence)\n",
    "    \n",
    "    print(\"\\nPenn Treebank Tags:\")\n",
    "    for token, tag in pos_tags:\n",
    "        print(f\"  {token:12} -> {tag}\")\n",
    "    \n",
    "    print(\"\\nUniversal Tags:\")\n",
    "    for token, tag in universal_tags:\n",
    "        print(f\"  {token:12} -> {tag}\")\n",
    "    \n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spacy_pos_tagging"
   },
   "source": [
    "## 4. spaCy POS Tagging\n",
    "\n",
    "spaCy provides more sophisticated POS tagging with additional linguistic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spacy_tagging_analysis"
   },
   "outputs": [],
   "source": [
    "def analyze_sentence_spacy(sentence):\n",
    "    \"\"\"\n",
    "    Analyze a sentence using spaCy POS tagging\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    analysis = []\n",
    "    for token in doc:\n",
    "        analysis.append({\n",
    "            'text': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'pos': token.pos_,\n",
    "            'tag': token.tag_,\n",
    "            'dep': token.dep_,\n",
    "            'shape': token.shape_,\n",
    "            'is_alpha': token.is_alpha,\n",
    "            'is_stop': token.is_stop\n",
    "        })\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze sentences with spaCy\n",
    "print(\"spaCy POS Tagging Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(sample_sentences[:2], 1):\n",
    "    print(f\"\\nSentence {i}: {sentence}\")\n",
    "    analysis = analyze_sentence_spacy(sentence)\n",
    "    \n",
    "    # Create a formatted table\n",
    "    print(f\"\\n{'Token':<12} {'Lemma':<12} {'POS':<6} {'Tag':<6} {'Dep':<8} {'Shape':<8} {'Stop?':<5}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for token_info in analysis:\n",
    "        print(f\"{token_info['text']:<12} {token_info['lemma']:<12} {token_info['pos']:<6} \"\n",
    "              f\"{token_info['tag']:<6} {token_info['dep']:<8} {token_info['shape']:<8} \"\n",
    "              f\"{str(token_info['is_stop']):<5}\")\n",
    "    \n",
    "    print(\"-\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos_tag_meanings"
   },
   "source": [
    "## 5. Understanding POS Tag Meanings\n",
    "\n",
    "Let's create a comprehensive guide to Penn Treebank POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pos_tag_reference"
   },
   "outputs": [],
   "source": [
    "# Penn Treebank POS tag reference\n",
    "penn_treebank_tags = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ': 'Adjective',\n",
    "    'JJR': 'Adjective, comparative',\n",
    "    'JJS': 'Adjective, superlative',\n",
    "    'LS': 'List item marker',\n",
    "    'MD': 'Modal',\n",
    "    'NN': 'Noun, singular',\n",
    "    'NNS': 'Noun, plural',\n",
    "    'NNP': 'Proper noun, singular',\n",
    "    'NNPS': 'Proper noun, plural',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP': 'Personal pronoun',\n",
    "    'PRP$': 'Possessive pronoun',\n",
    "    'RB': 'Adverb',\n",
    "    'RBR': 'Adverb, comparative',\n",
    "    'RBS': 'Adverb, superlative',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'to',\n",
    "    'UH': 'Interjection',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBD': 'Verb, past tense',\n",
    "    'VBG': 'Verb, gerund or present participle',\n",
    "    'VBN': 'Verb, past participle',\n",
    "    'VBP': 'Verb, non-3rd person singular present',\n",
    "    'VBZ': 'Verb, 3rd person singular present',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP': 'Wh-pronoun',\n",
    "    'WP$': 'Possessive wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}\n",
    "\n",
    "def explain_pos_tags(sentence):\n",
    "    \"\"\"\n",
    "    Explain POS tags for a sentence\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(\"\\nPOS Tag Explanations:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for token, tag in pos_tags:\n",
    "        explanation = penn_treebank_tags.get(tag, \"Unknown tag\")\n",
    "        print(f\"{token:15} {tag:6} -> {explanation}\")\n",
    "\n",
    "# Explain tags for a complex sentence\n",
    "complex_sentence = \"The researchers carefully analyzed the surprisingly complex data.\"\n",
    "explain_pos_tags(complex_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ambiguous_cases"
   },
   "source": [
    "## 6. Handling Ambiguous Cases\n",
    "\n",
    "Some words can have multiple POS tags depending on context. Let's explore these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ambiguity_examples"
   },
   "outputs": [],
   "source": [
    "# Examples of words with multiple possible POS tags\n",
    "ambiguous_examples = [\n",
    "    (\"I can open the can.\", \"'can' as modal vs noun\"),\n",
    "    (\"The book is on the table. I book a flight.\", \"'book' as noun vs verb\"),\n",
    "    (\"The light is bright. Light the candle.\", \"'light' as noun vs verb\"),\n",
    "    (\"They saw the movie. He used a saw.\", \"'saw' as verb vs noun\"),\n",
    "    (\"She will present the gift. The present situation.\", \"'present' as verb vs adjective\"),\n",
    "    (\"The fast car. He runs fast.\", \"'fast' as adjective vs adverb\"),\n",
    "    (\"Time flies like an arrow.\", \"Ambiguous parsing\")\n",
    "]\n",
    "\n",
    "print(\"Analyzing Ambiguous POS Cases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence, description in ambiguous_examples:\n",
    "    print(f\"\\nExample: {description}\")\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    \n",
    "    # NLTK analysis\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    print(\"NLTK Tags:\", [(token, tag) for token, tag in pos_tags])\n",
    "    \n",
    "    # spaCy analysis\n",
    "    doc = nlp(sentence)\n",
    "    spacy_tags = [(token.text, token.pos_, token.tag_) for token in doc]\n",
    "    \n",
    "    print(\"spaCy Tags:\", spacy_tags)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos_tag_distribution"
   },
   "source": [
    "## 7. POS Tag Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of POS tags in different types of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pos_distribution_analysis"
   },
   "outputs": [],
   "source": [
    "def analyze_pos_distribution(text, title=\"Text\"):\n",
    "    \"\"\"\n",
    "    Analyze POS tag distribution in text\n",
    "    \"\"\"\n",
    "    # Tokenize and tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Count tag frequencies\n",
    "    tag_counts = Counter([tag for _, tag in pos_tags])\n",
    "    \n",
    "    # Convert to universal tags for simpler analysis\n",
    "    universal_tags = pos_tag(tokens, tagset='universal')\n",
    "    universal_counts = Counter([tag for _, tag in universal_tags])\n",
    "    \n",
    "    return tag_counts, universal_counts\n",
    "\n",
    "# Analyze different text types\n",
    "text_samples = {\n",
    "    \"News\": \"\"\"The government announced new policies yesterday. The prime minister held a press conference to discuss economic reforms. Markets responded positively to the announcement.\"\"\",\n",
    "    \n",
    "    \"Scientific\": \"\"\"The researchers conducted experiments to test the hypothesis. Results showed significant correlations between variables. Further analysis is required to validate these findings.\"\"\",\n",
    "    \n",
    "    \"Narrative\": \"\"\"She walked slowly through the dark forest. The wind whispered through the trees as she carefully stepped over fallen branches. Suddenly, she heard a strange noise behind her.\"\"\",\n",
    "    \n",
    "    \"Social Media\": \"\"\"Just saw the most amazing movie! Can't believe how good it was. Definitely recommend watching it. Best film this year!!!\"\"\"\n",
    "}\n",
    "\n",
    "# Analyze each text type\n",
    "all_distributions = {}\n",
    "\n",
    "for text_type, text in text_samples.items():\n",
    "    tag_counts, universal_counts = analyze_pos_distribution(text, text_type)\n",
    "    all_distributions[text_type] = universal_counts\n",
    "    \n",
    "    print(f\"\\n{text_type} Text POS Distribution:\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(\"Universal POS tags:\")\n",
    "    for tag, count in universal_counts.most_common():\n",
    "        percentage = (count / sum(universal_counts.values())) * 100\n",
    "        print(f\"  {tag:8}: {count:3} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for plotting\n",
    "all_tags = set()\n",
    "for dist in all_distributions.values():\n",
    "    all_tags.update(dist.keys())\n",
    "\n",
    "text_types = list(all_distributions.keys())\n",
    "tag_data = {}\n",
    "\n",
    "for tag in all_tags:\n",
    "    tag_data[tag] = [all_distributions[text_type].get(tag, 0) for text_type in text_types]\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = range(len(text_types))\n",
    "width = 0.15\n",
    "colors = plt.cm.Set3(range(len(all_tags)))\n",
    "\n",
    "for i, (tag, counts) in enumerate(tag_data.items()):\n",
    "    offset = width * (i - len(all_tags) / 2)\n",
    "    plt.bar([xi + offset for xi in x], counts, width, label=tag, color=colors[i])\n",
    "\n",
    "plt.xlabel('Text Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('POS Tag Distribution Across Text Types')\n",
    "plt.xticks(x, text_types)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_pos_tagging"
   },
   "source": [
    "## 8. Custom POS Tagging Rules\n",
    "\n",
    "Sometimes we need custom rules for domain-specific text or to handle special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_rules"
   },
   "outputs": [],
   "source": [
    "def custom_pos_tagger(sentence):\n",
    "    \"\"\"\n",
    "    Custom POS tagger with domain-specific rules\n",
    "    \"\"\"\n",
    "    # Start with standard POS tagging\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Apply custom rules\n",
    "    custom_tags = []\n",
    "    \n",
    "    for i, (token, tag) in enumerate(pos_tags):\n",
    "        # Custom rule 1: Technical terms ending in -ing that are nouns\n",
    "        if token.endswith('ing') and token.lower() in ['programming', 'computing', 'processing', 'learning']:\n",
    "            custom_tags.append((token, 'NN-TECH'))\n",
    "        \n",
    "        # Custom rule 2: URLs and email addresses\n",
    "        elif re.match(r'https?://\\S+', token) or re.match(r'\\S+@\\S+\\.\\S+', token):\n",
    "            custom_tags.append((token, 'URL-EMAIL'))\n",
    "        \n",
    "        # Custom rule 3: Hashtags and mentions\n",
    "        elif token.startswith('#'):\n",
    "            custom_tags.append((token, 'HASHTAG'))\n",
    "        elif token.startswith('@'):\n",
    "            custom_tags.append((token, 'MENTION'))\n",
    "        \n",
    "        # Custom rule 4: Numbers with units\n",
    "        elif re.match(r'\\d+(?:\\.\\d+)?(?:kg|km|mb|gb|%)', token.lower()):\n",
    "            custom_tags.append((token, 'NUM-UNIT'))\n",
    "        \n",
    "        # Default: use standard tag\n",
    "        else:\n",
    "            custom_tags.append((token, tag))\n",
    "    \n",
    "    return custom_tags\n",
    "\n",
    "# Test custom POS tagger\n",
    "custom_sentences = [\n",
    "    \"Machine learning and natural language processing are fascinating fields.\",\n",
    "    \"Contact me at john@example.com or visit https://example.com for more info.\",\n",
    "    \"Check out #NLP and #MachineLearning. Follow @nlp_expert for updates.\",\n",
    "    \"The file size is 150MB and the distance is 2.5km.\"\n",
    "]\n",
    "\n",
    "print(\"Custom POS Tagging Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence in custom_sentences:\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    \n",
    "    # Standard tagging\n",
    "    standard_tags = pos_tag(word_tokenize(sentence))\n",
    "    print(\"Standard:\", standard_tags)\n",
    "    \n",
    "    # Custom tagging\n",
    "    custom_tags = custom_pos_tagger(sentence)\n",
    "    print(\"Custom:  \", custom_tags)\n",
    "    \n",
    "    # Highlight differences\n",
    "    differences = [(token, std_tag, cust_tag) for (token, std_tag), (_, cust_tag) \n",
    "                   in zip(standard_tags, custom_tags) if std_tag != cust_tag]\n",
    "    \n",
    "    if differences:\n",
    "        print(\"Changes: \", [(token, f\"{std}->{cust}\") for token, std, cust in differences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos_tagging_accuracy"
   },
   "source": [
    "## 9. Evaluating POS Tagging Accuracy\n",
    "\n",
    "Let's evaluate POS tagging accuracy using a gold standard dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "accuracy_evaluation"
   },
   "outputs": [],
   "source": [
    "# Use a small sample from the Brown corpus for evaluation\n",
    "def evaluate_pos_tagger(test_sentences, tagger_func):\n",
    "    \"\"\"\n",
    "    Evaluate POS tagger accuracy\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        # Extract words and true tags\n",
    "        words = [word for word, tag in sentence]\n",
    "        true_tags = [tag for word, tag in sentence]\n",
    "        \n",
    "        # Get predicted tags\n",
    "        predicted_tags = [tag for word, tag in tagger_func(words)]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        for true_tag, pred_tag in zip(true_tags, predicted_tags):\n",
    "            total += 1\n",
    "            if true_tag == pred_tag:\n",
    "                correct += 1\n",
    "            confusion_matrix[true_tag][pred_tag] += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, confusion_matrix\n",
    "\n",
    "# Get sample sentences from Brown corpus\n",
    "brown_sentences = brown.tagged_sents()[:100]  # First 100 sentences\n",
    "\n",
    "# Define different taggers to compare\n",
    "def nltk_tagger(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "def spacy_tagger(words):\n",
    "    text = ' '.join(words)\n",
    "    doc = nlp(text)\n",
    "    # Convert spaCy tags to Penn Treebank format for comparison\n",
    "    return [(token.text, token.tag_) for token in doc]\n",
    "\n",
    "# Evaluate taggers\n",
    "print(\"POS Tagging Accuracy Evaluation:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "taggers = {\n",
    "    'NLTK': nltk_tagger,\n",
    "    'spaCy': spacy_tagger\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, tagger in taggers.items():\n",
    "    accuracy, confusion = evaluate_pos_tagger(brown_sentences, tagger)\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name:8}: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "# Analyze common errors for NLTK tagger\n",
    "_, nltk_confusion = evaluate_pos_tagger(brown_sentences, nltk_tagger)\n",
    "\n",
    "print(\"\\nMost Common POS Tagging Errors (NLTK):\")\n",
    "errors = []\n",
    "for true_tag, predictions in nltk_confusion.items():\n",
    "    for pred_tag, count in predictions.items():\n",
    "        if true_tag != pred_tag and count > 2:  # Only show errors that occur multiple times\n",
    "            errors.append((count, true_tag, pred_tag))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, true_tag, pred_tag in errors[:10]:\n",
    "    print(f\"  {true_tag} -> {pred_tag}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "applications"
   },
   "source": [
    "## 10. Applications of POS Tagging\n",
    "\n",
    "Let's explore practical applications of POS tagging in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pos_applications"
   },
   "outputs": [],
   "source": [
    "# Application 1: Extract specific word types\n",
    "def extract_word_types(text, target_pos=['NN', 'NNS', 'NNP', 'NNPS']):\n",
    "    \"\"\"\n",
    "    Extract words of specific POS types (default: nouns)\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    return [word for word, tag in pos_tags if tag in target_pos]\n",
    "\n",
    "# Application 2: Generate reading difficulty metrics\n",
    "def calculate_text_complexity(text):\n",
    "    \"\"\"\n",
    "    Calculate text complexity based on POS distribution\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Count different POS types\n",
    "    pos_counts = Counter([tag for _, tag in pos_tags])\n",
    "    total_words = len(tokens)\n",
    "    \n",
    "    # Calculate complexity metrics\n",
    "    noun_ratio = (pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + \n",
    "                  pos_counts.get('NNP', 0) + pos_counts.get('NNPS', 0)) / total_words\n",
    "    \n",
    "    verb_ratio = (pos_counts.get('VB', 0) + pos_counts.get('VBD', 0) + \n",
    "                  pos_counts.get('VBG', 0) + pos_counts.get('VBN', 0) + \n",
    "                  pos_counts.get('VBP', 0) + pos_counts.get('VBZ', 0)) / total_words\n",
    "    \n",
    "    adj_ratio = (pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + \n",
    "                 pos_counts.get('JJS', 0)) / total_words\n",
    "    \n",
    "    adv_ratio = (pos_counts.get('RB', 0) + pos_counts.get('RBR', 0) + \n",
    "                 pos_counts.get('RBS', 0)) / total_words\n",
    "    \n",
    "    # Simple complexity score (higher = more complex)\n",
    "    complexity_score = noun_ratio * 1.5 + adj_ratio * 1.2 + adv_ratio * 1.1 + verb_ratio * 0.8\n",
    "    \n",
    "    return {\n",
    "        'noun_ratio': noun_ratio,\n",
    "        'verb_ratio': verb_ratio,\n",
    "        'adjective_ratio': adj_ratio,\n",
    "        'adverb_ratio': adv_ratio,\n",
    "        'complexity_score': complexity_score\n",
    "    }\n",
    "\n",
    "# Application 3: Extract noun phrases\n",
    "def extract_noun_phrases(text):\n",
    "    \"\"\"\n",
    "    Extract simple noun phrases using POS patterns\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = []\n",
    "    \n",
    "    # Pattern: (Determiner)? (Adjective)* Noun+\n",
    "    current_phrase = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['DET']:  # Determiner\n",
    "            current_phrase = [token.text]\n",
    "        elif token.pos_ in ['ADJ'] and current_phrase:  # Adjective\n",
    "            current_phrase.append(token.text)\n",
    "        elif token.pos_ in ['NOUN', 'PROPN']:  # Noun\n",
    "            current_phrase.append(token.text)\n",
    "            if len(current_phrase) > 1:  # Only phrases with multiple words\n",
    "                noun_phrases.append(' '.join(current_phrase))\n",
    "        else:\n",
    "            current_phrase = []\n",
    "    \n",
    "    return noun_phrases\n",
    "\n",
    "# Test applications\n",
    "test_text = \"\"\"\n",
    "The advanced machine learning algorithms can process natural language efficiently. \n",
    "These sophisticated computational methods require extensive training data and \n",
    "powerful computing resources. Modern artificial intelligence systems demonstrate \n",
    "remarkable performance in complex linguistic tasks.\n",
    "\"\"\"\n",
    "\n",
    "print(\"POS Tagging Applications:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nTest Text: {test_text.strip()}\")\n",
    "\n",
    "# Application 1: Extract nouns\n",
    "nouns = extract_word_types(test_text)\n",
    "print(f\"\\nExtracted Nouns: {nouns}\")\n",
    "\n",
    "# Extract adjectives\n",
    "adjectives = extract_word_types(test_text, ['JJ', 'JJR', 'JJS'])\n",
    "print(f\"Extracted Adjectives: {adjectives}\")\n",
    "\n",
    "# Application 2: Text complexity\n",
    "complexity = calculate_text_complexity(test_text)\n",
    "print(f\"\\nText Complexity Analysis:\")\n",
    "for metric, value in complexity.items():\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {value:.3f}\")\n",
    "\n",
    "# Application 3: Noun phrases\n",
    "noun_phrases = extract_noun_phrases(test_text)\n",
    "print(f\"\\nExtracted Noun Phrases: {noun_phrases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_features"
   },
   "source": [
    "## 11. Advanced POS Tagging Features\n",
    "\n",
    "Exploring more advanced aspects of POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "advanced_pos_features"
   },
   "outputs": [],
   "source": [
    "# Morphological analysis using spaCy\n",
    "def detailed_morphological_analysis(text):\n",
    "    \"\"\"\n",
    "    Perform detailed morphological analysis\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    analysis = []\n",
    "    for token in doc:\n",
    "        morph_info = {\n",
    "            'text': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'pos': token.pos_,\n",
    "            'tag': token.tag_,\n",
    "            'morph': str(token.morph),\n",
    "            'is_oov': token.is_oov,  # Out of vocabulary\n",
    "            'is_alpha': token.is_alpha,\n",
    "            'is_digit': token.is_digit,\n",
    "            'is_punct': token.is_punct\n",
    "        }\n",
    "        analysis.append(morph_info)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# POS-based text preprocessing\n",
    "def pos_based_filtering(text, keep_pos=['NOUN', 'VERB', 'ADJ', 'ADV']):\n",
    "    \"\"\"\n",
    "    Filter text to keep only specific POS types\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.lemma_.lower() for token in doc \n",
    "                      if token.pos_ in keep_pos and not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Sentence complexity scoring\n",
    "def score_sentence_complexity(sentence):\n",
    "    \"\"\"\n",
    "    Score sentence complexity based on POS patterns\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Count different elements\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    \n",
    "    # Complexity factors\n",
    "    factors = {\n",
    "        'length': len(doc),\n",
    "        'unique_pos': len(set(token.pos_ for token in doc)),\n",
    "        'subordinate_clauses': sum(1 for token in doc if token.dep_ in ['ccomp', 'advcl']),\n",
    "        'passive_voice': sum(1 for token in doc if token.dep_ == 'auxpass'),\n",
    "        'adjective_density': pos_counts.get('ADJ', 0) / len(doc),\n",
    "        'adverb_density': pos_counts.get('ADV', 0) / len(doc)\n",
    "    }\n",
    "    \n",
    "    # Simple complexity score\n",
    "    complexity = (factors['length'] * 0.1 + \n",
    "                 factors['unique_pos'] * 0.5 +\n",
    "                 factors['subordinate_clauses'] * 2 +\n",
    "                 factors['passive_voice'] * 1.5 +\n",
    "                 factors['adjective_density'] * 10 +\n",
    "                 factors['adverb_density'] * 8)\n",
    "    \n",
    "    return complexity, factors\n",
    "\n",
    "# Test advanced features\n",
    "test_sentences = [\n",
    "    \"The cat sat on the mat.\",  # Simple\n",
    "    \"The incredibly beautiful, ancient cathedral was being carefully restored by skilled craftsmen.\",  # Complex\n",
    "    \"Although machine learning algorithms are powerful, they require extensive computational resources.\",  # Complex with subordinate clause\n",
    "]\n",
    "\n",
    "print(\"Advanced POS Tagging Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    \n",
    "    # Detailed morphological analysis\n",
    "    morph_analysis = detailed_morphological_analysis(sentence)\n",
    "    print(\"\\nMorphological features (first 5 tokens):\")\n",
    "    for token_info in morph_analysis[:5]:\n",
    "        print(f\"  {token_info['text']}: POS={token_info['pos']}, Morph={token_info['morph']}\")\n",
    "    \n",
    "    # POS-based filtering\n",
    "    filtered = pos_based_filtering(sentence)\n",
    "    print(f\"\\nFiltered (content words only): {filtered}\")\n",
    "    \n",
    "    # Complexity scoring\n",
    "    complexity, factors = score_sentence_complexity(sentence)\n",
    "    print(f\"\\nComplexity Score: {complexity:.2f}\")\n",
    "    print(f\"  Length: {factors['length']} words\")\n",
    "    print(f\"  Unique POS: {factors['unique_pos']}\")\n",
    "    print(f\"  Subordinate clauses: {factors['subordinate_clauses']}\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 12. Exercises\n",
    "\n",
    "Practice POS tagging with these exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_1"
   },
   "source": [
    "### Exercise 1: Domain-Specific POS Analysis\n",
    "\n",
    "Analyze POS distributions in different domains and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_pos_exercise"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement analysis for different text domains\n",
    "# Domains to analyze: Academic papers, News articles, Social media posts, Legal documents\n",
    "\n",
    "def analyze_domain_pos_patterns(texts_by_domain):\n",
    "    \"\"\"\n",
    "    Analyze POS patterns across different domains\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Sample texts for different domains\n",
    "domain_texts = {\n",
    "    'academic': \"The study investigates the correlation between variables using statistical analysis.\",\n",
    "    'news': \"The government announced new policies yesterday during a press conference.\",\n",
    "    'social_media': \"OMG this is so cool! Can't believe it happened! #amazing\",\n",
    "    'legal': \"The party shall comply with all applicable laws and regulations.\"\n",
    "}\n",
    "\n",
    "# Your analysis code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_2"
   },
   "source": [
    "### Exercise 2: Custom POS Tagger for Technical Text\n",
    "\n",
    "Create a custom POS tagger that handles technical terminology better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "technical_pos_exercise"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement a custom POS tagger for technical text\n",
    "# Consider: Programming terms, technical jargon, version numbers, file extensions\n",
    "\n",
    "def technical_pos_tagger(text):\n",
    "    \"\"\"\n",
    "    POS tagger optimized for technical text\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with technical text\n",
    "technical_text = \"\"\"\n",
    "The algorithm uses TensorFlow 2.0 and Python 3.8. The model.py file contains \n",
    "the neural network implementation. GPU acceleration with CUDA 11.0 improves \n",
    "training speed by 10x.\n",
    "\"\"\"\n",
    "\n",
    "# Your test code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "key_takeaways"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **POS tagging is fundamental**: It provides crucial linguistic information for many NLP tasks\n",
    "\n",
    "2. **Multiple approaches available**:\n",
    "   - Rule-based: Fast but limited\n",
    "   - Statistical: Better accuracy, handles unseen words\n",
    "   - Neural: State-of-the-art performance\n",
    "\n",
    "3. **Context matters**: Same word can have different POS tags in different contexts\n",
    "\n",
    "4. **Library comparison**:\n",
    "   - **NLTK**: Good for learning and basic tasks\n",
    "   - **spaCy**: Production-ready, more accurate, additional linguistic features\n",
    "\n",
    "5. **Applications are diverse**:\n",
    "   - Information extraction\n",
    "   - Text preprocessing\n",
    "   - Syntactic analysis\n",
    "   - Text complexity assessment\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Choose the right tool**: spaCy for production, NLTK for experimentation\n",
    "2. **Consider domain adaptation**: Customize taggers for specific domains\n",
    "3. **Evaluate on your data**: Generic accuracy metrics may not reflect performance on your specific text\n",
    "4. **Handle ambiguity**: Be aware that some tagging decisions are inherently ambiguous\n",
    "5. **Combine with other features**: POS tags work best when combined with other linguistic features\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "- **Information Extraction**: Identify entities and relationships\n",
    "- **Text Summarization**: Focus on content words (nouns, verbs, adjectives)\n",
    "- **Sentiment Analysis**: Adjectives and adverbs carry sentiment information\n",
    "- **Question Answering**: POS helps identify question types and answer candidates\n",
    "- **Machine Translation**: Grammatical information improves translation quality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn about dependency parsing and syntactic analysis\n",
    "- Explore named entity recognition (NER)\n",
    "- Study chunking and phrase extraction\n",
    "- Practice with domain-specific text from your area of interest\n",
    "- Experiment with neural POS taggers and their architectures\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Penn Treebank POS Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "- [Universal POS Tags](https://universaldependencies.org/u/pos/)\n",
    "- [NLTK POS Tagging Documentation](https://www.nltk.org/book/ch05.html)\n",
    "- [spaCy Linguistic Features](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
    "- [Natural Language Processing with Python](https://www.nltk.org/book/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}