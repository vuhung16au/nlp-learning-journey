{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summarization_title"
   },
   "source": [
    "# Text Summarization in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/text-summarization.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text summarization is the process of creating a concise and coherent summary of a longer text while preserving key information and main ideas. It can be extractive (selecting important sentences) or abstractive (generating new text).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Extractive vs abstractive summarization\n",
    "- Frequency-based approaches\n",
    "- TF-IDF based summarization\n",
    "- TextRank algorithm\n",
    "- Transformer-based summarization\n",
    "- Evaluation metrics\n",
    "- Real-world applications\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, NLP preprocessing, and vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"nltk\",\n",
    "    \"scikit-learn\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"networkx\",\n",
    "    \"rouge-score\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"âœ“ {package}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "# NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data with error handling\n",
    "nltk_datasets = ['punkt', 'punkt_tab', 'stopwords', 'wordnet']\n",
    "print(\"Downloading NLTK datasets...\")\n",
    "for dataset in nltk_datasets:\n",
    "    try:\n",
    "        nltk.download(dataset, quiet=True)\n",
    "        print(f\"âœ“ {dataset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to download {dataset}: {e}\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize components with error handling\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"âœ“ NLTK components initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  NLTK components initialization failed: {e}\")\n",
    "    stop_words = set()\n",
    "    lemmatizer = None\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Note: Transformers models will be loaded when needed due to potential network requirements\n",
    "print(\"\\nâš ï¸  Note: Transformer models require internet access and will be loaded when needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_text"
   },
   "source": [
    "## Sample Text for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample_articles"
   },
   "outputs": [],
   "source": [
    "# Sample articles for summarization\n",
    "sample_articles = {\n",
    "    \"Technology\": \"\"\"\n",
    "    Artificial Intelligence has revolutionized numerous industries in recent years. Machine learning algorithms \n",
    "    are now capable of performing tasks that were once thought to be exclusively human. Deep learning models \n",
    "    have shown remarkable success in image recognition, natural language processing, and game playing.\n",
    "    \n",
    "    The healthcare industry has particularly benefited from AI advancements. Medical imaging systems powered \n",
    "    by AI can detect diseases earlier and more accurately than traditional methods. Drug discovery processes \n",
    "    have been accelerated through machine learning techniques that can predict molecular behavior.\n",
    "    \n",
    "    However, the rapid advancement of AI also raises concerns about job displacement and ethical implications. \n",
    "    Many experts argue for the need to develop responsible AI systems that are transparent and fair. \n",
    "    Governments worldwide are working on regulations to ensure AI development serves humanity's best interests.\n",
    "    \n",
    "    The future of AI looks promising with continued research in areas like quantum computing and neural networks. \n",
    "    As AI systems become more sophisticated, they will likely play an even greater role in solving complex \n",
    "    global challenges such as climate change and resource management.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Climate\": \"\"\"\n",
    "    Climate change represents one of the most pressing challenges of our time. Rising global temperatures \n",
    "    are causing widespread environmental disruptions including melting ice caps, rising sea levels, and \n",
    "    extreme weather events. The primary driver of this change is the emission of greenhouse gases from human activities.\n",
    "    \n",
    "    Renewable energy sources such as solar and wind power have emerged as crucial solutions. These technologies \n",
    "    have become more efficient and cost-effective, making them viable alternatives to fossil fuels. Many countries \n",
    "    have committed to achieving net-zero emissions by 2050 through massive investments in clean energy infrastructure.\n",
    "    \n",
    "    Individual actions also play a significant role in combating climate change. Reducing energy consumption, \n",
    "    choosing sustainable transportation options, and supporting environmentally conscious businesses can \n",
    "    collectively make a substantial impact. Education and awareness campaigns are essential for motivating \n",
    "    widespread behavioral changes.\n",
    "    \n",
    "    International cooperation is vital for addressing this global challenge. The Paris Agreement has brought \n",
    "    nations together to set emission reduction targets and share climate technologies. However, achieving \n",
    "    these goals requires sustained political will and continued innovation in clean technologies.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"Sample Articles for Summarization:\")\n",
    "for title, article in sample_articles.items():\n",
    "    word_count = len(article.split())\n",
    "    sentence_count = len(sent_tokenize(article))\n",
    "    print(f\"\\n{title} Article:\")\n",
    "    print(f\"  Words: {word_count}, Sentences: {sentence_count}\")\n",
    "    print(f\"  Preview: {article.strip()[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frequency_based"
   },
   "source": [
    "## Frequency-Based Summarization\n",
    "\n",
    "Simple approach based on word frequency and sentence scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frequency_summarization"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    # Remove extra whitespace and split into sentences\n",
    "    sentences = sent_tokenize(text.strip())\n",
    "    return sentences\n",
    "\n",
    "def frequency_based_summarization(text, num_sentences=3):\n",
    "    \"\"\"Summarize text based on word frequency\"\"\"\n",
    "    # Preprocess\n",
    "    sentences = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize and count word frequencies\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Score sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_words = word_tokenize(sentence.lower())\n",
    "        sentence_words = [word for word in sentence_words if word.isalnum() and word not in stop_words]\n",
    "        \n",
    "        score = 0\n",
    "        for word in sentence_words:\n",
    "            score += word_freq.get(word, 0)\n",
    "        \n",
    "        # Normalize by sentence length\n",
    "        if len(sentence_words) > 0:\n",
    "            sentence_scores[i] = score / len(sentence_words)\n",
    "        else:\n",
    "            sentence_scores[i] = 0\n",
    "    \n",
    "    # Select top sentences\n",
    "    top_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    top_sentences.sort()  # Maintain original order\n",
    "    \n",
    "    summary = ' '.join([sentences[i] for i in top_sentences])\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'selected_sentences': top_sentences,\n",
    "        'sentence_scores': sentence_scores,\n",
    "        'word_frequencies': word_freq.most_common(10)\n",
    "    }\n",
    "\n",
    "# Test frequency-based summarization\n",
    "print(\"Frequency-Based Summarization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for title, article in sample_articles.items():\n",
    "    result = frequency_based_summarization(article, num_sentences=2)\n",
    "    \n",
    "    print(f\"\\n{title} Article Summary:\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Top words: {[word for word, freq in result['word_frequencies'][:5]]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfidf_summarization"
   },
   "source": [
    "## TF-IDF Based Summarization\n",
    "\n",
    "Using TF-IDF scores to identify important sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfidf_approach"
   },
   "outputs": [],
   "source": [
    "def tfidf_based_summarization(text, num_sentences=3):\n",
    "    \"\"\"Summarize text using TF-IDF scores\"\"\"\n",
    "    sentences = preprocess_text(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return {\n",
    "            'summary': text.strip(),\n",
    "            'selected_sentences': list(range(len(sentences))),\n",
    "            'tfidf_scores': []\n",
    "        }\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        max_features=1000\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Calculate sentence scores (sum of TF-IDF values)\n",
    "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Select top sentences\n",
    "    top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "    top_indices.sort()  # Maintain original order\n",
    "    \n",
    "    summary = ' '.join([sentences[i] for i in top_indices])\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'selected_sentences': top_indices.tolist(),\n",
    "        'tfidf_scores': sentence_scores.tolist(),\n",
    "        'feature_names': vectorizer.get_feature_names_out()[:10].tolist()\n",
    "    }\n",
    "\n",
    "# Test TF-IDF summarization\n",
    "print(\"TF-IDF Based Summarization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for title, article in sample_articles.items():\n",
    "    result = tfidf_based_summarization(article, num_sentences=2)\n",
    "    \n",
    "    print(f\"\\n{title} Article Summary:\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Selected sentences: {result['selected_sentences']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "textrank"
   },
   "source": [
    "## TextRank Algorithm\n",
    "\n",
    "Graph-based ranking algorithm similar to PageRank for sentence selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "textrank_implementation"
   },
   "outputs": [],
   "source": [
    "def textrank_summarization(text, num_sentences=3, similarity_threshold=0.1):\n",
    "    \"\"\"Summarize text using TextRank algorithm\"\"\"\n",
    "    sentences = preprocess_text(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return {\n",
    "            'summary': text.strip(),\n",
    "            'selected_sentences': list(range(len(sentences))),\n",
    "            'textrank_scores': []\n",
    "        }\n",
    "    \n",
    "    # Create TF-IDF vectors for sentences\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Create graph\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    \n",
    "    # Apply threshold to create edges only for similar sentences\n",
    "    edges_to_remove = []\n",
    "    for i, j in nx_graph.edges():\n",
    "        if similarity_matrix[i][j] < similarity_threshold:\n",
    "            edges_to_remove.append((i, j))\n",
    "    \n",
    "    nx_graph.remove_edges_from(edges_to_remove)\n",
    "    \n",
    "    # Calculate PageRank scores\n",
    "    try:\n",
    "        scores = nx.pagerank(nx_graph, max_iter=100)\n",
    "    except:\n",
    "        # Fallback to simple scoring if PageRank fails\n",
    "        scores = {i: np.mean(similarity_matrix[i]) for i in range(len(sentences))}\n",
    "    \n",
    "    # Select top sentences\n",
    "    ranked_sentences = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [idx for idx, score in ranked_sentences[:num_sentences]]\n",
    "    top_indices.sort()  # Maintain original order\n",
    "    \n",
    "    summary = ' '.join([sentences[i] for i in top_indices])\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'selected_sentences': top_indices,\n",
    "        'textrank_scores': [scores[i] for i in range(len(sentences))],\n",
    "        'similarity_matrix_shape': similarity_matrix.shape\n",
    "    }\n",
    "\n",
    "# Test TextRank summarization\n",
    "print(\"TextRank Summarization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for title, article in sample_articles.items():\n",
    "    result = textrank_summarization(article, num_sentences=2)\n",
    "    \n",
    "    print(f\"\\n{title} Article Summary:\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Selected sentences: {result['selected_sentences']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transformer_summarization"
   },
   "source": [
    "## Transformer-Based Summarization\n",
    "\n",
    "Using pre-trained transformer models for abstractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer_summary"
   },
   "outputs": [],
   "source": [
    "# Initialize summarization pipeline\n",
    "try:\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    print(\"Loaded BART model for summarization\")\nexcept:\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "        print(\"Loaded T5-small model for summarization\")\n",
    "    except:\n",
    "        summarizer = None\n",
    "        print(\"Could not load transformer summarization model\")\n",
    "\n",
    "def transformer_summarization(text, max_length=130, min_length=30):\n",
    "    \"\"\"Summarize text using transformer model\"\"\"\n",
    "    if summarizer is None:\n",
    "        return {\n",
    "            'summary': \"Transformer model not available\",\n",
    "            'model_used': \"None\"\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Handle long texts by truncating if necessary\n",
    "        if len(text.split()) > 1000:\n",
    "            words = text.split()[:1000]\n",
    "            text = ' '.join(words)\n",
    "        \n",
    "        result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        \n",
    "        return {\n",
    "            'summary': result[0]['summary_text'],\n",
    "            'model_used': 'BART/T5',\n",
    "            'original_length': len(text.split()),\n",
    "            'summary_length': len(result[0]['summary_text'].split())\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'summary': f\"Error in summarization: {str(e)}\",\n",
    "            'model_used': 'Error'\n",
    "        }\n",
    "\n",
    "# Test transformer summarization\n",
    "if summarizer:\n",
    "    print(\"\\nTransformer-Based Summarization:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for title, article in sample_articles.items():\n",
    "        result = transformer_summarization(article)\n",
    "        \n",
    "        print(f\"\\n{title} Article Summary:\")\n",
    "        print(f\"Summary: {result['summary']}\")\n",
    "        if 'original_length' in result:\n",
    "            compression_ratio = result['summary_length'] / result['original_length']\n",
    "            print(f\"Compression ratio: {compression_ratio:.2f}\")\n",
    "        print(\"-\" * 30)\nelse:\n",
    "    print(\"Transformer summarization not available in this environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## Comparing Summarization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "method_comparison"
   },
   "outputs": [],
   "source": [
    "def compare_summarization_methods(text, num_sentences=2):\n",
    "    \"\"\"Compare different summarization methods\"\"\"\n",
    "    methods = {\n",
    "        'Frequency-based': frequency_based_summarization(text, num_sentences),\n",
    "        'TF-IDF': tfidf_based_summarization(text, num_sentences),\n",
    "        'TextRank': textrank_summarization(text, num_sentences)\n",
    "    }\n",
    "    \n",
    "    if summarizer:\n",
    "        methods['Transformer'] = transformer_summarization(text)\n",
    "    \n",
    "    return methods\n",
    "\n",
    "# Compare methods on one article\n",
    "test_article = sample_articles['Technology']\n",
    "comparison_results = compare_summarization_methods(test_article)\n",
    "\n",
    "print(\"Summarization Methods Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original text length: {len(test_article.split())} words\")\n",
    "print(f\"Original sentences: {len(sent_tokenize(test_article))}\")\n",
    "print()\n",
    "\n",
    "for method, result in comparison_results.items():\n",
    "    print(f\"{method}:\")\n",
    "    summary = result['summary']\n",
    "    summary_length = len(summary.split())\n",
    "    compression_ratio = summary_length / len(test_article.split())\n",
    "    \n",
    "    print(f\"  Summary ({summary_length} words, {compression_ratio:.2f} compression):\")\n",
    "    print(f\"  {summary}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Methods to evaluate summarization quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_metrics"
   },
   "outputs": [],
   "source": [
    "def calculate_rouge_simple(reference, candidate):\n",
    "    \"\"\"Simple ROUGE-1 calculation (word overlap)\"\"\"\n",
    "    ref_words = set(word_tokenize(reference.lower()))\n",
    "    cand_words = set(word_tokenize(candidate.lower()))\n",
    "    \n",
    "    if len(cand_words) == 0:\n",
    "        return {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "    \n",
    "    overlap = ref_words.intersection(cand_words)\n",
    "    \n",
    "    precision = len(overlap) / len(cand_words)\n",
    "    recall = len(overlap) / len(ref_words) if len(ref_words) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_summary_quality(original_text, summary):\n",
    "    \"\"\"Evaluate summary quality with multiple metrics\"\"\"\n",
    "    # Basic metrics\n",
    "    original_words = len(word_tokenize(original_text))\n",
    "    summary_words = len(word_tokenize(summary))\n",
    "    compression_ratio = summary_words / original_words\n",
    "    \n",
    "    # Content preservation (overlap with original)\n",
    "    rouge_scores = calculate_rouge_simple(original_text, summary)\n",
    "    \n",
    "    # Readability (sentence structure)\n",
    "    summary_sentences = sent_tokenize(summary)\n",
    "    avg_sentence_length = np.mean([len(word_tokenize(s)) for s in summary_sentences])\n",
    "    \n",
    "    return {\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'rouge_precision': rouge_scores['precision'],\n",
    "        'rouge_recall': rouge_scores['recall'],\n",
    "        'rouge_f1': rouge_scores['f1'],\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'num_sentences': len(summary_sentences)\n",
    "    }\n",
    "\n",
    "# Evaluate all summarization methods\n",
    "print(\"Summarization Quality Evaluation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<15} {'Compression':<12} {'ROUGE-F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "evaluation_results = {}\n",
    "for method, result in comparison_results.items():\n",
    "    evaluation = evaluate_summary_quality(test_article, result['summary'])\n",
    "    evaluation_results[method] = evaluation\n",
    "    \n",
    "    print(f\"{method:<15} {evaluation['compression_ratio']:<12.2f} \"\n",
    "          f\"{evaluation['rouge_f1']:<10.3f} {evaluation['rouge_precision']:<10.3f} \"\n",
    "          f\"{evaluation['rouge_recall']:<10.3f}\")\n",
    "\n",
    "# Visualization\n",
    "methods = list(evaluation_results.keys())\n",
    "metrics = ['compression_ratio', 'rouge_f1', 'rouge_precision', 'rouge_recall']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [evaluation_results[method][metric] for method in methods]\n",
    "    axes[i].bar(methods, values)\n",
    "    axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "applications"
   },
   "source": [
    "## Real-World Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "applications_demo"
   },
   "outputs": [],
   "source": [
    "# Application 1: News Article Summarization\n",
    "def news_summarization_pipeline(articles, method='textrank'):\n",
    "    \"\"\"Process multiple news articles for summarization\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        if method == 'frequency':\n",
    "            result = frequency_based_summarization(article, num_sentences=2)\n",
    "        elif method == 'tfidf':\n",
    "            result = tfidf_based_summarization(article, num_sentences=2)\n",
    "        elif method == 'textrank':\n",
    "            result = textrank_summarization(article, num_sentences=2)\n",
    "        else:\n",
    "            result = {'summary': 'Unknown method'}\n",
    "        \n",
    "        summaries.append({\n",
    "            'article_id': i,\n",
    "            'original_length': len(article.split()),\n",
    "            'summary': result['summary'],\n",
    "            'summary_length': len(result['summary'].split())\n",
    "        })\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Application 2: Document Clustering with Summaries\n",
    "def summarize_document_clusters(documents):\n",
    "    \"\"\"Summarize documents and group by similarity\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        summary_result = textrank_summarization(doc, num_sentences=1)\n",
    "        summaries.append(summary_result['summary'])\n",
    "    \n",
    "    # Calculate similarity between summaries\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(summaries)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    return summaries, similarity_matrix\n",
    "\n",
    "# Application 3: Meeting Notes Summarization\n",
    "def meeting_notes_summarization(meeting_text):\n",
    "    \"\"\"Specialized summarization for meeting notes\"\"\"\n",
    "    # Extract action items and key decisions\n",
    "    sentences = sent_tokenize(meeting_text)\n",
    "    \n",
    "    action_keywords = ['action', 'todo', 'follow up', 'next steps', 'assign', 'responsible']\n",
    "    decision_keywords = ['decided', 'agreed', 'concluded', 'resolution', 'vote']\n",
    "    \n",
    "    action_items = []\n",
    "    decisions = []\n",
    "    other_important = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        if any(keyword in sentence_lower for keyword in action_keywords):\n",
    "            action_items.append(sentence)\n",
    "        elif any(keyword in sentence_lower for keyword in decision_keywords):\n",
    "            decisions.append(sentence)\n",
    "        else:\n",
    "            # Use TF-IDF to score remaining sentences\n",
    "            other_important.append(sentence)\n",
    "    \n",
    "    # Get general summary from remaining sentences\n",
    "    if other_important:\n",
    "        general_summary = tfidf_based_summarization(' '.join(other_important), num_sentences=2)\n",
    "    else:\n",
    "        general_summary = {'summary': 'No additional summary available'}\n",
    "    \n",
    "    return {\n",
    "        'action_items': action_items[:3],  # Top 3 action items\n",
    "        'decisions': decisions[:3],        # Top 3 decisions\n",
    "        'general_summary': general_summary['summary']\n",
    "    }\n",
    "\n",
    "# Test applications\n",
    "print(\"Real-World Applications:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# News summarization\n",
    "news_articles = list(sample_articles.values())\n",
    "news_summaries = news_summarization_pipeline(news_articles, method='textrank')\n",
    "\n",
    "print(\"1. News Article Summaries:\")\n",
    "for summary in news_summaries:\n",
    "    compression = summary['summary_length'] / summary['original_length']\n",
    "    print(f\"   Article {summary['article_id']}: {compression:.2f} compression\")\n",
    "    print(f\"   Summary: {summary['summary'][:100]}...\")\n",
    "\n",
    "# Meeting notes example\n",
    "meeting_text = \"\"\"\n",
    "The team discussed the quarterly goals and performance metrics. We decided to increase \n",
    "the marketing budget by 20% for the next quarter. John will follow up with the design \n",
    "team about the new product mockups. The action item for Sarah is to coordinate with \n",
    "the sales team on lead generation strategies. We agreed that the current project \n",
    "timeline needs to be extended by two weeks. The team concluded that remote work \n",
    "policies should remain flexible. Next steps include scheduling client meetings \n",
    "and preparing quarterly reports.\n",
    "\"\"\"\n",
    "\n",
    "meeting_summary = meeting_notes_summarization(meeting_text)\n",
    "\n",
    "print(\"\\n2. Meeting Notes Summary:\")\n",
    "print(f\"   Action Items: {len(meeting_summary['action_items'])}\")\n",
    "for action in meeting_summary['action_items']:\n",
    "    print(f\"     - {action}\")\n",
    "print(f\"   Decisions: {len(meeting_summary['decisions'])}\")\n",
    "for decision in meeting_summary['decisions']:\n",
    "    print(f\"     - {decision}\")\n",
    "print(f\"   General Summary: {meeting_summary['general_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Multi-Document Summarization**: Combine multiple related documents into one summary\n",
    "2. **Query-Focused Summarization**: Create summaries focused on specific questions or topics\n",
    "3. **Timeline Summarization**: Summarize events in chronological order\n",
    "4. **Sentiment-Aware Summarization**: Include sentiment information in summaries\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Extractive vs Abstractive**: Extractive selects existing sentences, abstractive generates new text\n",
    "- **Multiple approaches**: Frequency-based, TF-IDF, TextRank, and transformer models each have strengths\n",
    "- **Quality vs Speed**: Transformer models provide better quality but are slower\n",
    "- **Domain matters**: Different types of text may need different summarization approaches\n",
    "- **Evaluation is challenging**: Automatic metrics don't always capture human perception of quality\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Choose appropriate method**: Consider your quality requirements and computational constraints\n",
    "2. **Preprocess carefully**: Clean text and handle special formatting\n",
    "3. **Set reasonable length targets**: Too short loses information, too long defeats the purpose\n",
    "4. **Evaluate on your domain**: Generic models may not work well for specialized text\n",
    "5. **Consider user needs**: Different users may need different types of summaries\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **News aggregation**: Create brief summaries of news articles\n",
    "- **Document management**: Summarize long reports and documents\n",
    "- **Email processing**: Generate brief summaries of email threads\n",
    "- **Research assistance**: Summarize academic papers and research\n",
    "- **Content curation**: Create digests of content for different audiences\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn about multi-document summarization techniques\n",
    "- Explore query-focused and topic-specific summarization\n",
    "- Study evaluation metrics like ROUGE, BLEU, and METEOR\n",
    "- Practice with domain-specific documents\n",
    "- Experiment with fine-tuning transformer models\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [TextRank Paper](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "- [BART Model](https://huggingface.co/facebook/bart-large-cnn)\n",
    "- [ROUGE Evaluation](https://aclanthology.org/W04-1013/)\n",
    "- [Hugging Face Summarization](https://huggingface.co/models?pipeline_tag=summarization)\n",
    "- [DUC/TAC Evaluation Campaigns](https://duc.nist.gov/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}