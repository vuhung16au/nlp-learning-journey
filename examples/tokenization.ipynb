{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenization_title"
   },
   "source": [
    "# Tokenization in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/tokenization.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements. It's one of the fundamental preprocessing steps in NLP.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Different types of tokenization\n",
    "- Word-level tokenization\n",
    "- Subword tokenization\n",
    "- Sentence tokenization\n",
    "- Using popular NLP libraries (NLTK, spaCy, Transformers)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python and text processing concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Let's install the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libraries"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install nltk spacy transformers tokenizers\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_text"
   },
   "source": [
    "## Sample Text\n",
    "\n",
    "Let's use a sample text to demonstrate different tokenization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_text"
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field! It combines linguistics, \n",
    "computer science, and machine learning. Today's NLP models can understand context, \n",
    "generate text, and even translate languages. Isn't that amazing? Let's explore \n",
    "tokenization techniques. Visit www.example.com for more info!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic_tokenization"
   },
   "source": [
    "## 1. Basic Tokenization\n",
    "\n",
    "### Simple Split-based Tokenization\n",
    "\n",
    "The simplest form of tokenization is splitting text by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic_split"
   },
   "outputs": [],
   "source": [
    "# Basic whitespace tokenization\n",
    "basic_tokens = sample_text.split()\n",
    "\n",
    "print(\"Basic Split Tokenization:\")\n",
    "print(f\"Number of tokens: {len(basic_tokens)}\")\n",
    "print(\"First 10 tokens:\", basic_tokens[:10])\n",
    "print(\"\\nLimitations: Notice punctuation is attached to words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regex_tokenization"
   },
   "source": [
    "### Regular Expression Tokenization\n",
    "\n",
    "We can use regular expressions for more sophisticated tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "regex_split"
   },
   "outputs": [],
   "source": [
    "# Regular expression tokenization\n",
    "import re\n",
    "\n",
    "# Pattern to match words (alphanumeric characters)\n",
    "pattern = r'\\b\\w+\\b'\n",
    "regex_tokens = re.findall(pattern, sample_text)\n",
    "\n",
    "print(\"Regular Expression Tokenization:\")\n",
    "print(f\"Number of tokens: {len(regex_tokens)}\")\n",
    "print(\"First 10 tokens:\", regex_tokens[:10])\n",
    "print(\"\\nNote: Punctuation is now separated from words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nltk_tokenization"
   },
   "source": [
    "## 2. NLTK Tokenization\n",
    "\n",
    "NLTK provides robust tokenization methods that handle various edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nltk_word_tokenize"
   },
   "outputs": [],
   "source": [
    "# NLTK Word Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk_word_tokens = word_tokenize(sample_text)\n",
    "\n",
    "print(\"NLTK Word Tokenization:\")\n",
    "print(f\"Number of tokens: {len(nltk_word_tokens)}\")\n",
    "print(\"All tokens:\", nltk_word_tokens)\n",
    "print(\"\\nAdvantages: Handles punctuation, contractions, and special cases better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nltk_sent_tokenize"
   },
   "outputs": [],
   "source": [
    "# NLTK Sentence Tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "\n",
    "print(\"NLTK Sentence Tokenization:\")\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sent.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spacy_tokenization"
   },
   "source": [
    "## 3. spaCy Tokenization\n",
    "\n",
    "spaCy provides industrial-strength tokenization with linguistic awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spacy_tokenize"
   },
   "outputs": [],
   "source": [
    "# spaCy tokenization\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(\"spaCy Tokenization:\")\n",
    "print(f\"Number of tokens: {len(doc)}\")\n",
    "\n",
    "# Display tokens with additional information\n",
    "print(\"\\nTokens with linguistic information:\")\n",
    "for token in doc[:15]:  # First 15 tokens\n",
    "    print(f\"Token: '{token.text}' | Lemma: '{token.lemma_}' | POS: {token.pos_} | Is_alpha: {token.is_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spacy_sentences"
   },
   "outputs": [],
   "source": [
    "# spaCy sentence segmentation\n",
    "print(\"spaCy Sentence Segmentation:\")\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}: {sent.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "subword_tokenization"
   },
   "source": [
    "## 4. Subword Tokenization\n",
    "\n",
    "Subword tokenization is crucial for handling out-of-vocabulary words and is widely used in modern NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bert_tokenization"
   },
   "source": [
    "### BERT Tokenization (WordPiece)\n",
    "\n",
    "BERT uses WordPiece tokenization, which breaks words into subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bert_tokenizer"
   },
   "outputs": [],
   "source": [
    "# BERT tokenizer (WordPiece)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text\n",
    "bert_tokens = bert_tokenizer.tokenize(sample_text)\n",
    "bert_token_ids = bert_tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"BERT WordPiece Tokenization:\")\n",
    "print(f\"Number of tokens: {len(bert_tokens)}\")\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "for token in bert_tokens[:20]:\n",
    "    print(f\"'{token}'\")\n",
    "\n",
    "print(\"\\nSpecial tokens:\")\n",
    "print(f\"[CLS] token ID: {bert_tokenizer.cls_token_id}\")\n",
    "print(f\"[SEP] token ID: {bert_tokenizer.sep_token_id}\")\n",
    "print(f\"[PAD] token ID: {bert_tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpt_tokenization"
   },
   "source": [
    "### GPT Tokenization (Byte Pair Encoding)\n",
    "\n",
    "GPT models use Byte Pair Encoding (BPE) for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpt_tokenizer"
   },
   "outputs": [],
   "source": [
    "# GPT tokenizer (BPE)\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Tokenize the text\n",
    "gpt_tokens = gpt_tokenizer.tokenize(sample_text)\n",
    "gpt_token_ids = gpt_tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"GPT BPE Tokenization:\")\n",
    "print(f\"Number of tokens: {len(gpt_tokens)}\")\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "for token in gpt_tokens[:20]:\n",
    "    print(f\"'{token}'\")\n",
    "\n",
    "print(\"\\nNote: ƒ† symbol represents beginning of a word in GPT tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## 5. Tokenization Comparison\n",
    "\n",
    "Let's compare the different tokenization methods on a complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "complex_example"
   },
   "outputs": [],
   "source": [
    "complex_text = \"The pre-trained transformer-based models can handle out-of-vocabulary words like 'supercalifragilisticexpialidocious' effectively!\"\n",
    "\n",
    "print(\"Complex Text:\", complex_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Basic split\n",
    "basic = complex_text.split()\n",
    "print(f\"\\nBasic Split ({len(basic)} tokens):\")\n",
    "print(basic)\n",
    "\n",
    "# NLTK\n",
    "nltk_tokens = word_tokenize(complex_text)\n",
    "print(f\"\\nNLTK ({len(nltk_tokens)} tokens):\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "# spaCy\n",
    "spacy_doc = nlp(complex_text)\n",
    "spacy_tokens = [token.text for token in spacy_doc]\n",
    "print(f\"\\nspaCy ({len(spacy_tokens)} tokens):\")\n",
    "print(spacy_tokens)\n",
    "\n",
    "# BERT\n",
    "bert_tokens_complex = bert_tokenizer.tokenize(complex_text)\n",
    "print(f\"\\nBERT WordPiece ({len(bert_tokens_complex)} tokens):\")\n",
    "print(bert_tokens_complex)\n",
    "\n",
    "# GPT\n",
    "gpt_tokens_complex = gpt_tokenizer.tokenize(complex_text)\n",
    "print(f\"\\nGPT BPE ({len(gpt_tokens_complex)} tokens):\")\n",
    "print(gpt_tokens_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "practical_considerations"
   },
   "source": [
    "## 6. Practical Considerations\n",
    "\n",
    "### Handling Special Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "special_cases"
   },
   "outputs": [],
   "source": [
    "special_text = \"\"\"\n",
    "Email: user@example.com\n",
    "URL: https://www.example.com/path?param=value\n",
    "Phone: +1-555-123-4567\n",
    "Hashtag: #NLP #MachineLearning\n",
    "Mention: @username\n",
    "Contraction: don't, won't, can't\n",
    "Numbers: 3.14, $100.50, 1,000,000\n",
    "Unicode: üòÄ üåü √±√°√©√≠√≥√∫\n",
    "\"\"\"\n",
    "\n",
    "print(\"Special Cases Text:\")\n",
    "print(special_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nNLTK Tokenization:\")\n",
    "nltk_special = word_tokenize(special_text)\n",
    "print(nltk_special)\n",
    "\n",
    "print(\"\\nspaCy Tokenization:\")\n",
    "spacy_special = [token.text for token in nlp(special_text)]\n",
    "print(spacy_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_tokenization"
   },
   "source": [
    "### Custom Tokenization Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_rules"
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Custom tokenizer that preserves certain patterns\n",
    "    \"\"\"\n",
    "    # Preserve emails, URLs, hashtags, mentions\n",
    "    import re\n",
    "    \n",
    "    # Define patterns\n",
    "    patterns = {\n",
    "        'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'url': r'https?://[^\\s]+',\n",
    "        'hashtag': r'#\\w+',\n",
    "        'mention': r'@\\w+',\n",
    "        'phone': r'\\+?1?-?\\(?\\d{3}\\)?-?\\d{3}-?\\d{4}',\n",
    "    }\n",
    "    \n",
    "    tokens = []\n",
    "    remaining_text = text\n",
    "    \n",
    "    # Find and extract special patterns\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matches = re.finditer(pattern, remaining_text)\n",
    "        for match in matches:\n",
    "            tokens.append((match.group(), pattern_name))\n",
    "    \n",
    "    # Use NLTK for the rest\n",
    "    for pattern in patterns.values():\n",
    "        remaining_text = re.sub(pattern, ' ', remaining_text)\n",
    "    \n",
    "    regular_tokens = [(token, 'word') for token in word_tokenize(remaining_text) if token.strip()]\n",
    "    \n",
    "    return tokens + regular_tokens\n",
    "\n",
    "# Test custom tokenizer\n",
    "custom_tokens = custom_tokenizer(special_text)\n",
    "print(\"Custom Tokenization with Type Labels:\")\n",
    "for token, token_type in custom_tokens[:20]:  # First 20 tokens\n",
    "    print(f\"'{token}' ({token_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Try these exercises to practice tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_1"
   },
   "source": [
    "### Exercise 1: Multilingual Tokenization\n",
    "\n",
    "Test different tokenizers on multilingual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multilingual_text"
   },
   "outputs": [],
   "source": [
    "multilingual_text = \"\"\"\n",
    "English: Hello, how are you?\n",
    "Spanish: Hola, ¬øc√≥mo est√°s?\n",
    "French: Bonjour, comment allez-vous?\n",
    "German: Hallo, wie geht es dir?\n",
    "Chinese: ‰Ω†Â•ΩÔºå‰Ω†Â•ΩÂêóÔºü\n",
    "Japanese: „Åì„Çì„Å´„Å°„ÅØ„ÄÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü\n",
    "\"\"\"\n",
    "\n",
    "print(\"Multilingual Text Tokenization:\")\n",
    "print(\"Original text:\")\n",
    "print(multilingual_text)\n",
    "\n",
    "# TODO: Try different tokenizers on this multilingual text\n",
    "# Compare NLTK, spaCy, and transformer tokenizers\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_2"
   },
   "source": [
    "### Exercise 2: Token Statistics\n",
    "\n",
    "Analyze tokenization statistics for a longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "token_stats"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement a function that takes a text and returns:\n",
    "# - Total number of tokens\n",
    "# - Average token length\n",
    "# - Most common tokens\n",
    "# - Vocabulary size (unique tokens)\n",
    "\n",
    "def analyze_tokenization(text, tokenizer_func):\n",
    "    \"\"\"\n",
    "    Analyze tokenization statistics\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with a longer text (you can use any text you like)\n",
    "long_text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "and artificial intelligence concerned with the interactions between computers and \n",
    "human language, in particular how to program computers to process and analyze \n",
    "large amounts of natural language data. The goal is a computer capable of \n",
    "understanding the contents of documents, including the contextual nuances of \n",
    "the language within them.\n",
    "\"\"\"\n",
    "\n",
    "# Your analysis code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "key_takeaways"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Different tokenizers for different purposes**:\n",
    "   - Basic split: Fast but limited\n",
    "   - NLTK: Good for traditional NLP tasks\n",
    "   - spaCy: Industrial-strength with linguistic features\n",
    "   - Transformer tokenizers: Essential for modern NLP models\n",
    "\n",
    "2. **Subword tokenization advantages**:\n",
    "   - Handles out-of-vocabulary words\n",
    "   - More efficient vocabulary usage\n",
    "   - Better for morphologically rich languages\n",
    "\n",
    "3. **Consider your use case**:\n",
    "   - Text preprocessing: NLTK or spaCy\n",
    "   - Modern NLP models: Use matching tokenizer\n",
    "   - Custom applications: May need custom rules\n",
    "\n",
    "4. **Important considerations**:\n",
    "   - Language support\n",
    "   - Special character handling\n",
    "   - Performance requirements\n",
    "   - Downstream task compatibility\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore text normalization techniques\n",
    "- Learn about stemming and lemmatization\n",
    "- Study different subword algorithms (BPE, WordPiece, SentencePiece)\n",
    "- Practice with domain-specific texts (social media, biomedical, legal)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [spaCy Documentation](https://spacy.io/)\n",
    "- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}