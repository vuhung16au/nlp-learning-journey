{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activation_functions_title"
   },
   "source": [
    "# Activation Functions in Natural Language Processing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/activation-functions.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Activation functions are crucial components of neural networks, including those used in NLP. They introduce non-linearity, allowing the model to learn complex patterns. While many activation functions can be used, some are particularly common in NLP models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Mathematical definitions of key activation functions used in NLP\n",
    "- Visual representations and graphs of activation functions\n",
    "- When and which functions suit NLP tasks best\n",
    "- Practical implementations with examples\n",
    "- Comparison of different activation functions\n",
    "\n",
    "## Key Activation Functions for NLP\n",
    "\n",
    "We'll focus on the most widely used activation functions in NLP:\n",
    "- **ReLU (Rectified Linear Unit)**: Default for hidden layers\n",
    "- **Sigmoid**: Used in gates and binary classification\n",
    "- **Tanh (Hyperbolic Tangent)**: Zero-centered alternative to sigmoid\n",
    "- **Softmax**: Essential for multi-class classification and probability distributions\n",
    "- **GELU (Gaussian Error Linear Unit)**: Modern alternative used in transformers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, neural networks, and mathematical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_setup"
   },
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"pandas\",\n",
    "    \"plotly\",\n",
    "    \"scipy\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"✓ {package}\")\n",
    "\n",
    "print(\"\\n🎉 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.special import erf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for better display\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"📊 All libraries imported successfully!\")\n",
    "print(\"🎨 Plotting style configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mathematical_definitions"
   },
   "source": [
    "## Mathematical Definitions of Activation Functions\n",
    "\n",
    "Let's start by defining the mathematical formulations of the key activation functions used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "relu_definition"
   },
   "source": [
    "### 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$ \\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases} $$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$ \\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases} $$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $[0, +\\infty)$\n",
    "- **Advantages:** Computationally efficient, addresses vanishing gradients\n",
    "- **Disadvantages:** Can cause \"dying ReLU\" problem (neurons that never activate)\n",
    "- **NLP Usage:** Default choice for hidden layers in most neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sigmoid_definition"
   },
   "source": [
    "### 2. Sigmoid Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) $$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $(0, 1)$\n",
    "- **Advantages:** Smooth, differentiable, outputs probabilities\n",
    "- **Disadvantages:** Suffers from vanishing gradients, not zero-centered\n",
    "- **NLP Usage:** Binary classification, gating mechanisms in LSTMs and GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tanh_definition"
   },
   "source": [
    "### 3. Hyperbolic Tangent (tanh)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1} $$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$ \\tanh'(x) = 1 - \\tanh^2(x) $$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $(-1, 1)$\n",
    "- **Advantages:** Zero-centered, smooth and differentiable\n",
    "- **Disadvantages:** Still suffers from vanishing gradients\n",
    "- **NLP Usage:** RNN hidden states, when zero-centered output is desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "softmax_definition"
   },
   "source": [
    "### 4. Softmax Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "\n",
    "**Numerically Stable Version:**\n",
    "\n",
    "$$ \\text{softmax}(x_i) = \\frac{e^{x_i - \\max(\\mathbf{x})}}{\\sum_{j=1}^{n} e^{x_j - \\max(\\mathbf{x})}} $$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $(0, 1)$ for each element, with $\\sum_{i=1}^{n} \\text{softmax}(x_i) = 1$\n",
    "- **Advantages:** Converts logits to probability distribution\n",
    "- **NLP Usage:** Multi-class classification, attention mechanisms, language modeling output layer\n",
    "\n",
    "**Example in NLP Context:**\n",
    "- English: \"My name is\" → Vietnamese: \"Tên tôi là\"\n",
    "- Softmax can output probabilities for each Vietnamese word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gelu_definition"
   },
   "source": [
    "### 5. GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "\n",
    "where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "**Approximation (commonly used):**\n",
    "\n",
    "$$ \\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right) $$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** $(-\\infty, +\\infty)$\n",
    "- **Advantages:** Smooth, non-monotonic, good performance in transformers\n",
    "- **NLP Usage:** Used in BERT, GPT, and other modern transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "implementation"
   },
   "source": [
    "## Implementation of Activation Functions\n",
    "\n",
    "Let's implement these activation functions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "activation_implementations"
   },
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions commonly used in NLP.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        \"\"\"\n",
    "        ReLU (Rectified Linear Unit) activation function.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array or scalar\n",
    "            \n",
    "        Returns:\n",
    "            ReLU activated values\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array or scalar\n",
    "            \n",
    "        Returns:\n",
    "            Sigmoid activated values\n",
    "        \"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x_clipped = np.clip(x, -250, 250)\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent activation function.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array or scalar\n",
    "            \n",
    "        Returns:\n",
    "            Tanh activated values\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        \"\"\"Derivative of tanh function.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x, axis=-1):\n",
    "        \"\"\"\n",
    "        Softmax activation function (numerically stable).\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "            axis: Axis along which to apply softmax\n",
    "            \n",
    "        Returns:\n",
    "            Softmax probability distribution\n",
    "        \"\"\"\n",
    "        # Numerical stability: subtract max\n",
    "        x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu(x):\n",
    "        \"\"\"\n",
    "        GELU (Gaussian Error Linear Unit) activation function.\n",
    "        Uses the approximation commonly implemented in practice.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array or scalar\n",
    "            \n",
    "        Returns:\n",
    "            GELU activated values\n",
    "        \"\"\"\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu_exact(x):\n",
    "        \"\"\"\n",
    "        Exact GELU using the error function (erf).\n",
    "        \n",
    "        Args:\n",
    "            x: Input array or scalar\n",
    "            \n",
    "        Returns:\n",
    "            Exact GELU activated values\n",
    "        \"\"\"\n",
    "        return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
    "\n",
    "# Test the implementations\n",
    "print(\"✅ Activation functions implemented successfully!\")\n",
    "\n",
    "# Quick test with sample values\n",
    "test_input = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"\\nTest input: {test_input}\")\n",
    "print(f\"ReLU: {ActivationFunctions.relu(test_input)}\")\n",
    "print(f\"Sigmoid: {ActivationFunctions.sigmoid(test_input):.4f}\")\n",
    "print(f\"Tanh: {ActivationFunctions.tanh(test_input):.4f}\")\n",
    "print(f\"GELU: {ActivationFunctions.gelu(test_input):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## Visualization of Activation Functions\n",
    "\n",
    "Let's create graphs to visualize how these activation functions behave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_functions"
   },
   "outputs": [],
   "source": [
    "def plot_activation_functions():\n",
    "    \"\"\"Plot all activation functions for comparison.\"\"\"\n",
    "    \n",
    "    # Generate input range\n",
    "    x = np.linspace(-6, 6, 1000)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Activation Functions Used in NLP', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Plot ReLU\n",
    "    axes[0, 0].plot(x, ActivationFunctions.relu(x), 'b-', linewidth=3, label='ReLU')\n",
    "    axes[0, 0].plot(x, ActivationFunctions.relu_derivative(x), 'r--', linewidth=2, label=\"ReLU'\")\n",
    "    axes[0, 0].set_title('ReLU (Rectified Linear Unit)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Input (x)')\n",
    "    axes[0, 0].set_ylabel('Output')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Plot Sigmoid\n",
    "    axes[0, 1].plot(x, ActivationFunctions.sigmoid(x), 'g-', linewidth=3, label='Sigmoid')\n",
    "    axes[0, 1].plot(x, ActivationFunctions.sigmoid_derivative(x), 'r--', linewidth=2, label=\"Sigmoid'\")\n",
    "    axes[0, 1].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Input (x)')\n",
    "    axes[0, 1].set_ylabel('Output')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Plot Tanh\n",
    "    axes[0, 2].plot(x, ActivationFunctions.tanh(x), 'orange', linewidth=3, label='Tanh')\n",
    "    axes[0, 2].plot(x, ActivationFunctions.tanh_derivative(x), 'r--', linewidth=2, label=\"Tanh'\")\n",
    "    axes[0, 2].set_title('Hyperbolic Tangent (tanh)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Input (x)')\n",
    "    axes[0, 2].set_ylabel('Output')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, 2].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Plot GELU\n",
    "    axes[1, 0].plot(x, ActivationFunctions.gelu(x), 'purple', linewidth=3, label='GELU (approx)')\n",
    "    axes[1, 0].plot(x, ActivationFunctions.gelu_exact(x), 'm--', linewidth=2, label='GELU (exact)')\n",
    "    axes[1, 0].set_title('GELU (Gaussian Error Linear Unit)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Input (x)')\n",
    "    axes[1, 0].set_ylabel('Output')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Plot Softmax example (3 inputs)\n",
    "    x_soft = np.linspace(-3, 3, 100)\n",
    "    # Create 3 different input patterns\n",
    "    input1 = x_soft\n",
    "    input2 = 0.5 * x_soft + 1\n",
    "    input3 = -0.5 * x_soft + 0.5\n",
    "    \n",
    "    # Stack inputs and apply softmax\n",
    "    inputs = np.stack([input1, input2, input3], axis=0)\n",
    "    softmax_outputs = ActivationFunctions.softmax(inputs, axis=0)\n",
    "    \n",
    "    axes[1, 1].plot(x_soft, softmax_outputs[0], 'b-', linewidth=3, label='Class 1')\n",
    "    axes[1, 1].plot(x_soft, softmax_outputs[1], 'g-', linewidth=3, label='Class 2')\n",
    "    axes[1, 1].plot(x_soft, softmax_outputs[2], 'r-', linewidth=3, label='Class 3')\n",
    "    axes[1, 1].set_title('Softmax Function (3 Classes)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Input (x)')\n",
    "    axes[1, 1].set_ylabel('Probability')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Comparison plot\n",
    "    axes[1, 2].plot(x, ActivationFunctions.relu(x), 'b-', linewidth=2, label='ReLU')\n",
    "    axes[1, 2].plot(x, ActivationFunctions.sigmoid(x), 'g-', linewidth=2, label='Sigmoid')\n",
    "    axes[1, 2].plot(x, ActivationFunctions.tanh(x), 'orange', linewidth=2, label='Tanh')\n",
    "    axes[1, 2].plot(x, ActivationFunctions.gelu(x), 'purple', linewidth=2, label='GELU')\n",
    "    axes[1, 2].set_title('Comparison of Activation Functions', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Input (x)')\n",
    "    axes[1, 2].set_ylabel('Output')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, 2].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[1, 2].set_ylim(-2, 3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "plot_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp_applications"
   },
   "source": [
    "## NLP Applications and When to Use Each Function\n",
    "\n",
    "Let's explore when and where each activation function is most suitable in NLP tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlp_usage_examples"
   },
   "outputs": [],
   "source": [
    "def demonstrate_nlp_applications():\n",
    "    \"\"\"Demonstrate practical NLP applications of activation functions.\"\"\"\n",
    "    \n",
    "    print(\"🔥 ACTIVATION FUNCTIONS IN NLP APPLICATIONS\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Example 1: Text Classification with Softmax\n",
    "    print(\"\\n📝 1. TEXT CLASSIFICATION WITH SOFTMAX\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate logits for sentiment classification\n",
    "    # English: \"I love programming\" → Vietnamese: \"Tôi yêu lập trình\"\n",
    "    logits = np.array([\n",
    "        [2.1, -1.2, 0.3],   # Sentence 1: \"I love programming\" - likely positive\n",
    "        [-0.8, 2.5, -0.1],  # Sentence 2: \"This is terrible\" - likely negative  \n",
    "        [0.1, 0.2, 0.1]     # Sentence 3: \"It's okay\" - neutral\n",
    "    ])\n",
    "    \n",
    "    class_names = ['Positive', 'Negative', 'Neutral']\n",
    "    sentences = [\n",
    "        'English: \"I love programming\" / Vietnamese: \"Tôi yêu lập trình\"',\n",
    "        'English: \"This is terrible\" / Vietnamese: \"Điều này thật tệ\"',\n",
    "        'English: \"It\\'s okay\" / Vietnamese: \"Không sao\"'\n",
    "    ]\n",
    "    \n",
    "    probabilities = ActivationFunctions.softmax(logits)\n",
    "    \n",
    "    for i, (sentence, probs) in enumerate(zip(sentences, probabilities)):\n",
    "        print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "        print(f\"Logits: {logits[i]}\")\n",
    "        for j, (class_name, prob) in enumerate(zip(class_names, probs)):\n",
    "            print(f\"  {class_name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        predicted_class = class_names[np.argmax(probs)]\n",
    "        print(f\"  → Predicted: {predicted_class}\")\n",
    "    \n",
    "    # Example 2: ReLU in Hidden Layers\n",
    "    print(\"\\n\\n⚡ 2. ReLU IN HIDDEN LAYERS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate word embeddings being processed through ReLU\n",
    "    word_embeddings = np.array([\n",
    "        [-0.5, 0.8, -0.2, 1.2],  # \"Hello\" / \"Xin chào\"\n",
    "        [0.3, -0.6, 0.9, -0.1],  # \"world\" / \"thế giới\"\n",
    "        [-0.8, -0.3, 0.4, 0.7]   # \"!\" / \"!\"\n",
    "    ])\n",
    "    \n",
    "    words = ['\"Hello\"/\"Xin chào\"', '\"world\"/\"thế giới\"', '\"!\"/\"!\"']\n",
    "    \n",
    "    print(\"Word embeddings processed through ReLU:\")\n",
    "    for i, (word, embedding) in enumerate(zip(words, word_embeddings)):\n",
    "        relu_output = ActivationFunctions.relu(embedding)\n",
    "        print(f\"\\n{word}:\")\n",
    "        print(f\"  Input:  {embedding}\")\n",
    "        print(f\"  ReLU:   {relu_output}\")\n",
    "        print(f\"  Sparsity: {np.sum(relu_output == 0)}/{len(relu_output)} zeros\")\n",
    "    \n",
    "    # Example 3: Sigmoid in LSTM Gates\n",
    "    print(\"\\n\\n🚪 3. SIGMOID IN LSTM GATES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate LSTM gate computations\n",
    "    gate_inputs = np.array([-2.0, -0.5, 0.0, 0.5, 2.0])\n",
    "    gate_outputs = ActivationFunctions.sigmoid(gate_inputs)\n",
    "    \n",
    "    print(\"LSTM Gate behavior (Sigmoid):\")\n",
    "    print(\"Input\\t→\\tSigmoid\\t→\\tInterpretation\")\n",
    "    interpretations = [\n",
    "        \"Strongly close gate\",\n",
    "        \"Mostly close gate\", \n",
    "        \"Half open/closed\",\n",
    "        \"Mostly open gate\",\n",
    "        \"Strongly open gate\"\n",
    "    ]\n",
    "    \n",
    "    for inp, out, interp in zip(gate_inputs, gate_outputs, interpretations):\n",
    "        print(f\"{inp:5.1f}\\t→\\t{out:.4f}\\t→\\t{interp}\")\n",
    "    \n",
    "    # Example 4: GELU in Transformers\n",
    "    print(\"\\n\\n🤖 4. GELU IN TRANSFORMER MODELS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate transformer feed-forward layer\n",
    "    transformer_inputs = np.linspace(-3, 3, 7)\n",
    "    gelu_outputs = ActivationFunctions.gelu(transformer_inputs)\n",
    "    relu_outputs = ActivationFunctions.relu(transformer_inputs)\n",
    "    \n",
    "    print(\"GELU vs ReLU in Transformer Feed-Forward:\")\n",
    "    print(\"Input\\t→\\tGELU\\t→\\tReLU\\t→\\tDifference\")\n",
    "    \n",
    "    for inp, gelu_out, relu_out in zip(transformer_inputs, gelu_outputs, relu_outputs):\n",
    "        diff = gelu_out - relu_out\n",
    "        print(f\"{inp:5.1f}\\t→\\t{gelu_out:6.3f}\\t→\\t{relu_out:6.3f}\\t→\\t{diff:7.3f}\")\n",
    "    \n",
    "    print(\"\\n💡 Notice: GELU provides smoother gradients and non-zero outputs for negative inputs!\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_nlp_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "best_practices"
   },
   "source": [
    "## Best Practices and Guidelines\n",
    "\n",
    "Here's a comprehensive guide for choosing activation functions in NLP tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_table"
   },
   "outputs": [],
   "source": [
    "def create_comparison_table():\n",
    "    \"\"\"Create a comprehensive comparison table of activation functions.\"\"\"\n",
    "    \n",
    "    # Create comparison data\n",
    "    comparison_data = {\n",
    "        'Function': ['ReLU', 'Sigmoid', 'Tanh', 'Softmax', 'GELU'],\n",
    "        'Range': ['[0, ∞)', '(0, 1)', '(-1, 1)', '(0, 1), Σ=1', '(-∞, ∞)'],\n",
    "        'Zero-Centered': ['No', 'No', 'Yes', 'No', 'Yes'],\n",
    "        'Vanishing Gradient': ['No', 'Yes', 'Yes', 'Depends', 'Less prone'],\n",
    "        'Computational Cost': ['Very Low', 'Medium', 'Medium', 'High', 'High'],\n",
    "        'Main NLP Use Case': [\n",
    "            'Hidden layers',\n",
    "            'Gates, Binary classification', \n",
    "            'RNN hidden states',\n",
    "            'Multi-class output, Attention',\n",
    "            'Transformer feed-forward'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"📊 ACTIVATION FUNCTIONS COMPARISON TABLE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def provide_selection_guidelines():\n",
    "    \"\"\"Provide detailed guidelines for selecting activation functions.\"\"\"\n",
    "    \n",
    "    guidelines = {\n",
    "        \"🏗️ HIDDEN LAYERS\": {\n",
    "            \"First Choice\": \"ReLU - Fast, prevents vanishing gradients\",\n",
    "            \"Alternative\": \"GELU - For transformer-based models (BERT, GPT)\",\n",
    "            \"Avoid\": \"Sigmoid/Tanh - Cause vanishing gradients in deep networks\"\n",
    "        },\n",
    "        \n",
    "        \"🎯 OUTPUT LAYERS\": {\n",
    "            \"Binary Classification\": \"Sigmoid - Outputs probability for single class\",\n",
    "            \"Multi-class Classification\": \"Softmax - Probability distribution over classes\", \n",
    "            \"Regression\": \"Linear/None - Direct output without bounds\"\n",
    "        },\n",
    "        \n",
    "        \"🔄 RECURRENT NETWORKS\": {\n",
    "            \"LSTM/GRU Gates\": \"Sigmoid - Controls information flow (0=close, 1=open)\",\n",
    "            \"RNN Hidden States\": \"Tanh - Zero-centered, bounded output\",\n",
    "            \"Modern Alternative\": \"ReLU variants - Better gradient flow\"\n",
    "        },\n",
    "        \n",
    "        \"🤖 TRANSFORMER MODELS\": {\n",
    "            \"Feed-Forward Layers\": \"GELU - Smooth, used in BERT/GPT\",\n",
    "            \"Attention Mechanism\": \"Softmax - For attention weights\",\n",
    "            \"Alternative\": \"Swish/SiLU - Similar performance to GELU\"\n",
    "        },\n",
    "        \n",
    "        \"🌍 MULTILINGUAL NLP\": {\n",
    "            \"Language Detection\": \"Softmax - Probability over languages\",\n",
    "            \"Translation Models\": \"Softmax (output), ReLU/GELU (hidden)\",\n",
    "            \"Cross-lingual Embeddings\": \"ReLU/GELU - Preserve semantic relationships\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\\n🎯 ACTIVATION FUNCTION SELECTION GUIDELINES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for category, items in guidelines.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * (len(category) - 2))  # Subtract emoji characters\n",
    "        for subcategory, guideline in items.items():\n",
    "            print(f\"  • {subcategory}: {guideline}\")\n",
    "\n",
    "# Generate the comparison and guidelines\n",
    "comparison_df = create_comparison_table()\n",
    "provide_selection_guidelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **ReLU** is the default choice for hidden layers due to its computational efficiency and ability to mitigate vanishing gradients.\n",
    "\n",
    "2. **Sigmoid** is essential for binary classification and gating mechanisms in RNNs (LSTMs, GRUs).\n",
    "\n",
    "3. **Tanh** provides zero-centered output, making it suitable for RNN hidden states.\n",
    "\n",
    "4. **Softmax** is indispensable for multi-class classification, attention mechanisms, and any scenario requiring probability distributions.\n",
    "\n",
    "5. **GELU** represents the modern approach used in transformer models like BERT and GPT, offering smooth gradients and better performance.\n",
    "\n",
    "### Quick Selection Guide for NLP\n",
    "\n",
    "- **Hidden Layers**: ReLU (general) or GELU (transformers)\n",
    "- **Binary Classification**: Sigmoid\n",
    "- **Multi-class Classification**: Softmax\n",
    "- **RNN Gates**: Sigmoid\n",
    "- **RNN Hidden States**: Tanh\n",
    "- **Attention Weights**: Softmax\n",
    "- **Transformer Feed-Forward**: GELU\n",
    "\n",
    "### Vietnamese/English NLP Considerations\n",
    "\n",
    "When working with Vietnamese-English translation or multilingual NLP:\n",
    "- Use **Softmax** for language detection and word prediction\n",
    "- **ReLU/GELU** in hidden layers preserve cross-lingual semantic relationships\n",
    "- **Attention mechanisms** with Softmax help align words across languages\n",
    "\n",
    "Example: English \"My name is\" → Vietnamese \"Tên tôi là\" requires attention mechanisms using Softmax to properly align word correspondences.\n",
    "\n",
    "Understanding these activation functions and their appropriate usage is crucial for building effective NLP models that can handle complex language understanding tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}