{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "date_converter_title"
      },
      "source": [
        "# Date String Converter using Encoder-Decoder Architecture with PyTorch\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/date-string-converter.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an Encoder-Decoder model that converts date strings from natural language format (e.g., \"April 22, 2019\") to ISO format (e.g., \"2019-04-22\"). We'll use a character-level sequence-to-sequence approach with LSTM networks in PyTorch.\n",
        "\n",
        "**Note**: This repository prioritizes PyTorch over TensorFlow. This notebook has been updated to use PyTorch implementations.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Sequence-to-sequence modeling for string transformation using PyTorch\n",
        "- Character-level tokenization for date processing\n",
        "- LSTM Encoder-Decoder architecture in PyTorch\n",
        "- Data generation for training date conversion models\n",
        "- Model training and evaluation techniques with PyTorch\n",
        "- Inference and prediction on new date strings\n",
        "- Vietnamese/English date format examples\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Basic understanding of Python, neural networks, and sequence-to-sequence models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Environment Detection and Setup (Required for all notebooks in this repository)\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq libpq-dev\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")\n",
        "\n",
        "# PyTorch logging setup\n",
        "def setup_pytorch_logging():\n",
        "    \"\"\"Setup platform-specific PyTorch logging directories.\"\"\"\n",
        "    if IS_COLAB:\n",
        "        root_logdir = \"/content/pytorch_logs\"\n",
        "    elif IS_KAGGLE:\n",
        "        root_logdir = \"./pytorch_logs\"\n",
        "    else:\n",
        "        root_logdir = os.path.join(os.getcwd(), \"pytorch_logs\")\n",
        "    \n",
        "    os.makedirs(root_logdir, exist_ok=True)\n",
        "    return root_logdir\n",
        "\n",
        "def get_run_logdir(experiment_name=\"run\"):\n",
        "    \"\"\"Generate unique run directory for training logs.\"\"\"\n",
        "    root_logdir = setup_pytorch_logging()\n",
        "    run_id = time.strftime(f\"{experiment_name}_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "print(\"\\nInstalling required packages...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                      capture_output=True)\n",
        "    print(f\"‚úì {package}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import datetime\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_generation"
      },
      "source": [
        "## 1. Data Generation\n",
        "\n",
        "We'll generate synthetic training data with various date formats commonly found in natural language text, including Vietnamese/English examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "date_data_generator"
      },
      "outputs": [],
      "source": [
        "class DateDataGenerator:\n",
        "    \"\"\"Generate date conversion training data.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # English month names\n",
        "        self.months_full = [\n",
        "            'January', 'February', 'March', 'April', 'May', 'June',\n",
        "            'July', 'August', 'September', 'October', 'November', 'December'\n",
        "        ]\n",
        "        \n",
        "        self.months_short = [\n",
        "            'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
        "        ]\n",
        "        \n",
        "        # Vietnamese month names (for demonstration)\n",
        "        self.months_vietnamese = [\n",
        "            'Th√°ng m·ªôt', 'Th√°ng hai', 'Th√°ng ba', 'Th√°ng t∆∞', 'Th√°ng nƒÉm', 'Th√°ng s√°u',\n",
        "            'Th√°ng b·∫£y', 'Th√°ng t√°m', 'Th√°ng ch√≠n', 'Th√°ng m∆∞·ªùi', 'Th√°ng m∆∞·ªùi m·ªôt', 'Th√°ng m∆∞·ªùi hai'\n",
        "        ]\n",
        "        \n",
        "        # Number representations\n",
        "        self.ordinals = {\n",
        "            1: ['1st', 'first'], 2: ['2nd', 'second'], 3: ['3rd', 'third'],\n",
        "            4: ['4th', 'fourth'], 5: ['5th', 'fifth'], 6: ['6th', 'sixth'],\n",
        "            7: ['7th', 'seventh'], 8: ['8th', 'eighth'], 9: ['9th', 'ninth'],\n",
        "            10: ['10th', 'tenth'], 11: ['11th', 'eleventh'], 12: ['12th', 'twelfth'],\n",
        "            13: ['13th', 'thirteenth'], 14: ['14th', 'fourteenth'], 15: ['15th', 'fifteenth'],\n",
        "            16: ['16th', 'sixteenth'], 17: ['17th', 'seventeenth'], 18: ['18th', 'eighteenth'],\n",
        "            19: ['19th', 'nineteenth'], 20: ['20th', 'twentieth'], 21: ['21st', 'twenty-first'],\n",
        "            22: ['22nd', 'twenty-second'], 23: ['23rd', 'twenty-third'], 24: ['24th', 'twenty-fourth'],\n",
        "            25: ['25th', 'twenty-fifth'], 26: ['26th', 'twenty-sixth'], 27: ['27th', 'twenty-seventh'],\n",
        "            28: ['28th', 'twenty-eighth'], 29: ['29th', 'twenty-ninth'], 30: ['30th', 'thirtieth'],\n",
        "            31: ['31st', 'thirty-first']\n",
        "        }\n",
        "    \n",
        "    def generate_random_date(self):\n",
        "        \"\"\"Generate a random date.\"\"\"\n",
        "        year = random.randint(1990, 2030)\n",
        "        month = random.randint(1, 12)\n",
        "        \n",
        "        # Handle different month lengths\n",
        "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
        "            day = random.randint(1, 31)\n",
        "        elif month in [4, 6, 9, 11]:\n",
        "            day = random.randint(1, 30)\n",
        "        else:  # February\n",
        "            # Simple leap year check\n",
        "            if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):\n",
        "                day = random.randint(1, 29)\n",
        "            else:\n",
        "                day = random.randint(1, 28)\n",
        "        \n",
        "        return year, month, day\n",
        "    \n",
        "    def format_date_natural(self, year, month, day):\n",
        "        \"\"\"Generate various natural language date formats.\"\"\"\n",
        "        formats = []\n",
        "        \n",
        "        # Format 1: Month DD, YYYY\n",
        "        formats.append(f\"{self.months_full[month-1]} {day}, {year}\")\n",
        "        \n",
        "        # Format 2: DD Month YYYY\n",
        "        formats.append(f\"{day} {self.months_full[month-1]} {year}\")\n",
        "        \n",
        "        # Format 3: Mon DD, YYYY\n",
        "        formats.append(f\"{self.months_short[month-1]} {day}, {year}\")\n",
        "        \n",
        "        # Format 4: DD Mon YYYY\n",
        "        formats.append(f\"{day} {self.months_short[month-1]} {year}\")\n",
        "        \n",
        "        # Format 5: Ordinal format\n",
        "        if day in self.ordinals:\n",
        "            ordinal = random.choice(self.ordinals[day])\n",
        "            formats.append(f\"{self.months_full[month-1]} {ordinal}, {year}\")\n",
        "            formats.append(f\"the {ordinal} of {self.months_full[month-1]} {year}\")\n",
        "        \n",
        "        # Format 6: MM/DD/YYYY\n",
        "        formats.append(f\"{month:02d}/{day:02d}/{year}\")\n",
        "        \n",
        "        # Format 7: DD/MM/YYYY (European style)\n",
        "        formats.append(f\"{day:02d}/{month:02d}/{year}\")\n",
        "        \n",
        "        # Format 8: Vietnamese style (demonstration)\n",
        "        formats.append(f\"ng√†y {day} {self.months_vietnamese[month-1]} nƒÉm {year}\")\n",
        "        \n",
        "        return random.choice(formats)\n",
        "    \n",
        "    def format_date_iso(self, year, month, day):\n",
        "        \"\"\"Format date in ISO format (target).\"\"\"\n",
        "        return f\"{year:04d}-{month:02d}-{day:02d}\"\n",
        "    \n",
        "    def generate_dataset(self, num_samples=10000):\n",
        "        \"\"\"Generate a dataset of date conversion pairs.\"\"\"\n",
        "        data = []\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            year, month, day = self.generate_random_date()\n",
        "            natural_format = self.format_date_natural(year, month, day)\n",
        "            iso_format = self.format_date_iso(year, month, day)\n",
        "            \n",
        "            data.append({\n",
        "                'input': natural_format,\n",
        "                'output': iso_format,\n",
        "                'year': year,\n",
        "                'month': month,\n",
        "                'day': day\n",
        "            })\n",
        "        \n",
        "        return data\n",
        "\n",
        "# Generate training data\n",
        "generator = DateDataGenerator()\n",
        "dataset = generator.generate_dataset(10000)\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(f\"Generated {len(df)} date conversion pairs\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Display some Vietnamese/English examples\n",
        "print(\"\\nüáªüá≥üá∫üá∏ Vietnamese/English Date Examples:\")\n",
        "vietnamese_examples = df[df['input'].str.contains('ng√†y|th√°ng|nƒÉm', case=False, na=False)]\n",
        "if not vietnamese_examples.empty:\n",
        "    print(vietnamese_examples[['input', 'output']].head(3))\n",
        "else:\n",
        "    print(\"No Vietnamese examples in current sample, generating some:\")\n",
        "    for i in range(3):\n",
        "        year, month, day = generator.generate_random_date()\n",
        "        vn_format = f\"ng√†y {day} {generator.months_vietnamese[month-1]} nƒÉm {year}\"\n",
        "        iso_format = generator.format_date_iso(year, month, day)\n",
        "        print(f\"Vietnamese: '{vn_format}' ‚Üí ISO: '{iso_format}'\")\n",
        "\n",
        "print(\"\\nEnglish examples:\")\n",
        "english_examples = df[~df['input'].str.contains('ng√†y|th√°ng|nƒÉm', case=False, na=False)]\n",
        "print(english_examples[['input', 'output']].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preprocessing"
      },
      "source": [
        "## 2. Data Preprocessing and Character-Level Tokenization\n",
        "\n",
        "We'll implement character-level tokenization for our sequence-to-sequence model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenization"
      },
      "outputs": [],
      "source": [
        "class CharacterTokenizer:\n",
        "    \"\"\"Character-level tokenizer for date strings.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "        self.vocab_size = 0\n",
        "        \n",
        "        # Special tokens\n",
        "        self.SOS_token = '<SOS>'\n",
        "        self.EOS_token = '<EOS>'\n",
        "        self.PAD_token = '<PAD>'\n",
        "        self.UNK_token = '<UNK>'\n",
        "    \n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build character vocabulary from texts.\"\"\"\n",
        "        all_chars = set()\n",
        "        \n",
        "        # Collect all unique characters\n",
        "        for text in texts:\n",
        "            all_chars.update(list(text))\n",
        "        \n",
        "        # Add special tokens\n",
        "        special_tokens = [self.PAD_token, self.SOS_token, self.EOS_token, self.UNK_token]\n",
        "        \n",
        "        # Build mappings\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "        \n",
        "        # Add special tokens first\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            self.char_to_idx[token] = i\n",
        "            self.idx_to_char[i] = token\n",
        "        \n",
        "        # Add regular characters\n",
        "        for i, char in enumerate(sorted(all_chars), len(special_tokens)):\n",
        "            self.char_to_idx[char] = i\n",
        "            self.idx_to_char[i] = char\n",
        "        \n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "        \n",
        "        print(f\"Built vocabulary with {self.vocab_size} characters\")\n",
        "        print(f\"Special tokens: {special_tokens}\")\n",
        "        print(f\"Sample characters: {list(sorted(all_chars))[:20]}\")\n",
        "    \n",
        "    def encode(self, text, max_length=None, add_eos=False):\n",
        "        \"\"\"Encode text to indices.\"\"\"\n",
        "        indices = []\n",
        "        \n",
        "        for char in text:\n",
        "            if char in self.char_to_idx:\n",
        "                indices.append(self.char_to_idx[char])\n",
        "            else:\n",
        "                indices.append(self.char_to_idx[self.UNK_token])\n",
        "        \n",
        "        if add_eos:\n",
        "            indices.append(self.char_to_idx[self.EOS_token])\n",
        "        \n",
        "        # Pad or truncate to max_length\n",
        "        if max_length is not None:\n",
        "            if len(indices) < max_length:\n",
        "                indices.extend([self.char_to_idx[self.PAD_token]] * (max_length - len(indices)))\n",
        "            else:\n",
        "                indices = indices[:max_length]\n",
        "        \n",
        "        return indices\n",
        "    \n",
        "    def decode(self, indices, stop_at_eos=True):\n",
        "        \"\"\"Decode indices to text.\"\"\"\n",
        "        chars = []\n",
        "        \n",
        "        for idx in indices:\n",
        "            if idx in self.idx_to_char:\n",
        "                char = self.idx_to_char[idx]\n",
        "                if char == self.EOS_token and stop_at_eos:\n",
        "                    break\n",
        "                elif char not in [self.PAD_token, self.SOS_token]:\n",
        "                    chars.append(char)\n",
        "        \n",
        "        return ''.join(chars)\n",
        "\n",
        "# Build tokenizer\n",
        "tokenizer = CharacterTokenizer()\n",
        "\n",
        "# Collect all text for vocabulary building\n",
        "all_texts = df['input'].tolist() + df['output'].tolist()\n",
        "tokenizer.build_vocab(all_texts)\n",
        "\n",
        "# Test tokenization\n",
        "sample_input = df['input'].iloc[0]\n",
        "sample_output = df['output'].iloc[0]\n",
        "\n",
        "print(f\"\\nTokenization test:\")\n",
        "print(f\"Input: '{sample_input}'\")\n",
        "encoded_input = tokenizer.encode(sample_input, max_length=50)\n",
        "print(f\"Encoded: {encoded_input[:20]}...\")\n",
        "decoded_input = tokenizer.decode(encoded_input)\n",
        "print(f\"Decoded: '{decoded_input}'\")\n",
        "\n",
        "print(f\"\\nOutput: '{sample_output}'\")\n",
        "encoded_output = tokenizer.encode(sample_output, add_eos=True)\n",
        "print(f\"Encoded: {encoded_output}\")\n",
        "decoded_output = tokenizer.decode(encoded_output)\n",
        "print(f\"Decoded: '{decoded_output}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_class"
      },
      "source": [
        "## 3. PyTorch Dataset and DataLoader\n",
        "\n",
        "Create a custom PyTorch dataset for our date conversion task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pytorch_dataset"
      },
      "outputs": [],
      "source": [
        "class DateConversionDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for date conversion.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataframe, tokenizer, max_input_length=50, max_output_length=15):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_output_length = max_output_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        \n",
        "        # Encode input (source)\n",
        "        input_text = row['input']\n",
        "        input_encoded = self.tokenizer.encode(input_text, max_length=self.max_input_length)\n",
        "        \n",
        "        # Encode output (target)\n",
        "        output_text = row['output']\n",
        "        output_encoded = self.tokenizer.encode(output_text, max_length=self.max_output_length, add_eos=True)\n",
        "        \n",
        "        # Create decoder input (shifted by one position)\n",
        "        decoder_input = [self.tokenizer.char_to_idx[self.tokenizer.SOS_token]] + output_encoded[:-1]\n",
        "        if len(decoder_input) < self.max_output_length:\n",
        "            decoder_input.extend([self.tokenizer.char_to_idx[self.tokenizer.PAD_token]] * (self.max_output_length - len(decoder_input)))\n",
        "        else:\n",
        "            decoder_input = decoder_input[:self.max_output_length]\n",
        "        \n",
        "        return {\n",
        "            'encoder_input': torch.tensor(input_encoded, dtype=torch.long),\n",
        "            'decoder_input': torch.tensor(decoder_input, dtype=torch.long),\n",
        "            'decoder_target': torch.tensor(output_encoded, dtype=torch.long),\n",
        "            'input_text': input_text,\n",
        "            'output_text': output_text\n",
        "        }\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(df))\n",
        "val_size = int(0.1 * len(df))\n",
        "test_size = len(df) - train_size - val_size\n",
        "\n",
        "train_df = df[:train_size].reset_index(drop=True)\n",
        "val_df = df[train_size:train_size+val_size].reset_index(drop=True)\n",
        "test_df = df[train_size+val_size:].reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset split:\")\n",
        "print(f\"  Train: {len(train_df)} samples\")\n",
        "print(f\"  Validation: {len(val_df)} samples\")\n",
        "print(f\"  Test: {len(test_df)} samples\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DateConversionDataset(train_df, tokenizer)\n",
        "val_dataset = DateConversionDataset(val_df, tokenizer)\n",
        "test_dataset = DateConversionDataset(test_df, tokenizer)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nCreated data loaders with batch size {batch_size}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch shapes:\")\n",
        "print(f\"  Encoder input: {sample_batch['encoder_input'].shape}\")\n",
        "print(f\"  Decoder input: {sample_batch['decoder_input'].shape}\")\n",
        "print(f\"  Decoder target: {sample_batch['decoder_target'].shape}\")\n",
        "\n",
        "# Show first sample\n",
        "print(f\"\\nFirst sample:\")\n",
        "print(f\"  Input text: '{sample_batch['input_text'][0]}'\")\n",
        "print(f\"  Output text: '{sample_batch['output_text'][0]}'\")\n",
        "print(f\"  Encoder input: {sample_batch['encoder_input'][0][:20].tolist()}...\")\n",
        "print(f\"  Decoder input: {sample_batch['decoder_input'][0].tolist()}\")\n",
        "print(f\"  Decoder target: {sample_batch['decoder_target'][0].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_architecture"
      },
      "source": [
        "## 4. Encoder-Decoder Model Architecture\n",
        "\n",
        "Implement the LSTM-based encoder-decoder model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_definition"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    \"\"\"LSTM Encoder for sequence-to-sequence model.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    \"\"\"LSTM Decoder for sequence-to-sequence model.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        \n",
        "        # Project to vocabulary size\n",
        "        predictions = self.output_projection(outputs)  # (batch_size, seq_length, vocab_size)\n",
        "        \n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    \"\"\"Complete sequence-to-sequence model.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        \n",
        "        self.encoder = EncoderLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "        self.decoder = DecoderLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        # Encode\n",
        "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
        "        \n",
        "        # Decode\n",
        "        decoder_outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
        "        \n",
        "        return decoder_outputs\n",
        "    \n",
        "    def predict(self, encoder_input, tokenizer, max_length=15, device='cpu'):\n",
        "        \"\"\"Generate prediction for a given input.\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode\n",
        "            encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
        "            \n",
        "            # Initialize decoder input with SOS token\n",
        "            decoder_input = torch.tensor([[tokenizer.char_to_idx[tokenizer.SOS_token]]], \n",
        "                                       dtype=torch.long, device=device)\n",
        "            \n",
        "            predictions = []\n",
        "            \n",
        "            for _ in range(max_length):\n",
        "                # Decode one step\n",
        "                decoder_output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "                \n",
        "                # Get the predicted token\n",
        "                predicted_token = decoder_output.argmax(dim=-1)\n",
        "                predictions.append(predicted_token.item())\n",
        "                \n",
        "                # Check for EOS token\n",
        "                if predicted_token.item() == tokenizer.char_to_idx[tokenizer.EOS_token]:\n",
        "                    break\n",
        "                \n",
        "                # Use predicted token as next input\n",
        "                decoder_input = predicted_token\n",
        "            \n",
        "            return predictions\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Seq2SeqModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Model summary\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"  Embedding dimension: 128\")\n",
        "print(f\"  Hidden dimension: 256\")\n",
        "print(f\"  Number of layers: 2\")\n",
        "\n",
        "# Test forward pass\n",
        "sample_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
        "               for k, v in sample_batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(sample_batch['encoder_input'], sample_batch['decoder_input'])\n",
        "    print(f\"\\nTest forward pass output shape: {output.shape}\")\n",
        "    print(f\"Expected shape: (batch_size={batch_size}, max_output_length=15, vocab_size={tokenizer.vocab_size})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the sequence-to-sequence model with proper logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, tokenizer, device, epochs=10):\n",
        "    \"\"\"Train the sequence-to-sequence model.\"\"\"\n",
        "    \n",
        "    # Setup training\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.char_to_idx[tokenizer.PAD_token])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "    \n",
        "    # Training logs\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "    print(f\"Training on {device}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")\n",
        "        for batch in train_pbar:\n",
        "            # Move batch to device\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            decoder_target = batch['decoder_target'].to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(encoder_input, decoder_input)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.reshape(-1, tokenizer.vocab_size), \n",
        "                           decoder_target.reshape(-1))\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "            train_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "        \n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        \n",
        "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch in val_pbar:\n",
        "                # Move batch to device\n",
        "                encoder_input = batch['encoder_input'].to(device)\n",
        "                decoder_input = batch['decoder_input'].to(device)\n",
        "                decoder_target = batch['decoder_target'].to(device)\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(encoder_input, decoder_input)\n",
        "                \n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs.reshape(-1, tokenizer.vocab_size), \n",
        "                               decoder_target.reshape(-1))\n",
        "                \n",
        "                total_val_loss += loss.item()\n",
        "                val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "        \n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        # Test on a few samples\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(\"\\n  Sample predictions:\")\n",
        "            test_samples = val_loader.dataset.data.sample(3)\n",
        "            for idx, row in test_samples.iterrows():\n",
        "                input_text = row['input']\n",
        "                target_text = row['output']\n",
        "                \n",
        "                # Encode input\n",
        "                input_encoded = tokenizer.encode(input_text, max_length=50)\n",
        "                input_tensor = torch.tensor([input_encoded], dtype=torch.long, device=device)\n",
        "                \n",
        "                # Predict\n",
        "                predictions = model.predict(input_tensor, tokenizer, device=device)\n",
        "                predicted_text = tokenizer.decode(predictions)\n",
        "                \n",
        "                print(f\"    Input: '{input_text}'\")\n",
        "                print(f\"    Target: '{target_text}'\")\n",
        "                print(f\"    Predicted: '{predicted_text}'\")\n",
        "                print()\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model, train_loader, val_loader, tokenizer, device, epochs=5\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 6. Model Evaluation and Visualization\n",
        "\n",
        "Evaluate the trained model and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_code"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.diff(train_losses), label='Training Loss Change', color='blue', alpha=0.7)\n",
        "plt.plot(np.diff(val_losses), label='Validation Loss Change', color='red', alpha=0.7)\n",
        "plt.title('Loss Change per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Change')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comprehensive evaluation\n",
        "def evaluate_model(model, test_loader, tokenizer, device):\n",
        "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    exact_matches = 0\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"Evaluating model on test set...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluation\"):\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            input_texts = batch['input_text']\n",
        "            target_texts = batch['output_text']\n",
        "            \n",
        "            batch_size = encoder_input.size(0)\n",
        "            \n",
        "            for i in range(batch_size):\n",
        "                # Get single sample\n",
        "                single_input = encoder_input[i:i+1]\n",
        "                input_text = input_texts[i]\n",
        "                target_text = target_texts[i]\n",
        "                \n",
        "                # Predict\n",
        "                predictions = model.predict(single_input, tokenizer, device=device)\n",
        "                predicted_text = tokenizer.decode(predictions)\n",
        "                \n",
        "                # Check exact match\n",
        "                if predicted_text.strip() == target_text.strip():\n",
        "                    exact_matches += 1\n",
        "                \n",
        "                total_predictions += 1\n",
        "                \n",
        "                results.append({\n",
        "                    'input': input_text,\n",
        "                    'target': target_text,\n",
        "                    'predicted': predicted_text,\n",
        "                    'correct': predicted_text.strip() == target_text.strip()\n",
        "                })\n",
        "    \n",
        "    accuracy = exact_matches / total_predictions\n",
        "    \n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"  Total samples: {total_predictions}\")\n",
        "    print(f\"  Exact matches: {exact_matches}\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    \n",
        "    return results, accuracy\n",
        "\n",
        "# Evaluate the model\n",
        "results, accuracy = evaluate_model(model, test_loader, tokenizer, device)\n",
        "\n",
        "# Show some example predictions\n",
        "print(\"\\nüéØ Sample Predictions:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show correct predictions\n",
        "correct_results = [r for r in results if r['correct']]\n",
        "if correct_results:\n",
        "    print(\"\\n‚úÖ Correct Predictions:\")\n",
        "    for i, result in enumerate(correct_results[:5]):\n",
        "        print(f\"{i+1}. Input: '{result['input']}'\")\n",
        "        print(f\"   Target: '{result['target']}'\")\n",
        "        print(f\"   Predicted: '{result['predicted']}'\")\n",
        "        print()\n",
        "\n",
        "# Show incorrect predictions\n",
        "incorrect_results = [r for r in results if not r['correct']]\n",
        "if incorrect_results:\n",
        "    print(\"\\n‚ùå Incorrect Predictions:\")\n",
        "    for i, result in enumerate(incorrect_results[:5]):\n",
        "        print(f\"{i+1}. Input: '{result['input']}'\")\n",
        "        print(f\"   Target: '{result['target']}'\")\n",
        "        print(f\"   Predicted: '{result['predicted']}'\")\n",
        "        print()\n",
        "\n",
        "# Show Vietnamese/English examples if any\n",
        "vietnamese_results = [r for r in results if 'ng√†y' in r['input'] or 'th√°ng' in r['input']]\n",
        "if vietnamese_results:\n",
        "    print(\"\\nüáªüá≥ Vietnamese Examples:\")\n",
        "    for i, result in enumerate(vietnamese_results[:3]):\n",
        "        print(f\"{i+1}. Input: '{result['input']}'\")\n",
        "        print(f\"   Target: '{result['target']}'\")\n",
        "        print(f\"   Predicted: '{result['predicted']}'\")\n",
        "        print(f\"   Correct: {'‚úÖ' if result['correct'] else '‚ùå'}\")\n",
        "        print()\n",
        "\n",
        "print(f\"\\nüéØ Final Model Performance: {accuracy*100:.2f}% accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## 7. Interactive Inference\n",
        "\n",
        "Test the model with custom date strings including Vietnamese/English examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_inference"
      },
      "outputs": [],
      "source": [
        "def predict_date_conversion(model, tokenizer, input_text, device):\n",
        "    \"\"\"Predict date conversion for a given input text.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Encode input\n",
        "    input_encoded = tokenizer.encode(input_text, max_length=50)\n",
        "    input_tensor = torch.tensor([input_encoded], dtype=torch.long, device=device)\n",
        "    \n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        predictions = model.predict(input_tensor, tokenizer, device=device)\n",
        "        predicted_text = tokenizer.decode(predictions)\n",
        "    \n",
        "    return predicted_text\n",
        "\n",
        "# Test with various date formats\n",
        "test_inputs = [\n",
        "    # English formats\n",
        "    \"January 15, 2023\",\n",
        "    \"15 January 2023\",\n",
        "    \"Jan 15, 2023\",\n",
        "    \"15 Jan 2023\",\n",
        "    \"01/15/2023\",\n",
        "    \"15/01/2023\",\n",
        "    \"the first of January 2023\",\n",
        "    \"March 22nd, 2024\",\n",
        "    \"December 31st, 1999\",\n",
        "    \n",
        "    # Vietnamese formats (if in training data)\n",
        "    \"ng√†y 15 Th√°ng m·ªôt nƒÉm 2023\",\n",
        "    \"ng√†y 22 Th√°ng ba nƒÉm 2024\",\n",
        "    \"ng√†y 31 Th√°ng m∆∞·ªùi hai nƒÉm 1999\",\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Date Conversion Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, input_text in enumerate(test_inputs, 1):\n",
        "    predicted = predict_date_conversion(model, tokenizer, input_text, device)\n",
        "    \n",
        "    # Determine if it's Vietnamese or English\n",
        "    lang = \"üáªüá≥ Vietnamese\" if any(word in input_text.lower() for word in ['ng√†y', 'th√°ng', 'nƒÉm']) else \"üá∫üá∏ English\"\n",
        "    \n",
        "    print(f\"{i:2d}. {lang}\")\n",
        "    print(f\"    Input: '{input_text}'\")\n",
        "    print(f\"    Predicted: '{predicted}'\")\n",
        "    print()\n",
        "\n",
        "# Interactive function for custom inputs\n",
        "def interactive_date_conversion():\n",
        "    \"\"\"Interactive date conversion function.\"\"\"\n",
        "    print(\"\\nüîÑ Interactive Date Conversion\")\n",
        "    print(\"Enter date strings to convert to ISO format (YYYY-MM-DD)\")\n",
        "    print(\"Examples:\")\n",
        "    print(\"  - 'April 22, 2019'\")\n",
        "    print(\"  - '15 March 2023'\")\n",
        "    print(\"  - 'ng√†y 10 Th√°ng hai nƒÉm 2024'\")\n",
        "    print(\"Type 'quit' to exit\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"Enter date string: \").strip()\n",
        "        \n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        if not user_input:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            predicted = predict_date_conversion(model, tokenizer, user_input, device)\n",
        "            print(f\"Converted: '{predicted}'\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n\")\n",
        "\n",
        "# Uncomment the line below to run interactive mode\n",
        "# interactive_date_conversion()\n",
        "\n",
        "print(\"\\n‚úÖ Date Conversion Model Demo Complete!\")\n",
        "print(\"\\nüìù Key Achievements:\")\n",
        "print(\"1. Implemented character-level sequence-to-sequence model in PyTorch\")\n",
        "print(\"2. Trained encoder-decoder LSTM for date format conversion\")\n",
        "print(\"3. Achieved reasonable accuracy on date conversion task\")\n",
        "print(\"4. Demonstrated multilingual support (English/Vietnamese)\")\n",
        "print(\"5. Created interactive inference capability\")\n",
        "\n",
        "print(\"\\nüáªüá≥üá∫üá∏ Multilingual Examples:\")\n",
        "print(\"English: 'My name is John' ‚Üí Vietnamese: 'T√™n t√¥i l√† John'\")\n",
        "print(\"English: 'April 22, 2019' ‚Üí ISO: '2019-04-22'\")\n",
        "print(\"Vietnamese: 'ng√†y 22 Th√°ng t∆∞ nƒÉm 2019' ‚Üí ISO: '2019-04-22'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}