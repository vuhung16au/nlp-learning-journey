{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "date_converter_title"
   },
   "source": [
    "# Date String Converter using Encoder-Decoder Architecture\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/date-string-converter.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build an Encoder-Decoder model that converts date strings from natural language format (e.g., \"April 22, 2019\") to ISO format (e.g., \"2019-04-22\"). We'll use a character-level sequence-to-sequence approach with LSTM networks in TensorFlow/Keras.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Sequence-to-sequence modeling for string transformation\n",
    "- Character-level tokenization for date processing\n",
    "- LSTM Encoder-Decoder architecture\n",
    "- Data generation for training date conversion models\n",
    "- Model training and evaluation techniques\n",
    "- Inference and prediction on new date strings\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, neural networks, and sequence-to-sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"tensorflow\",\n",
    "    \"numpy\",\n",
    "    \"pandas\", \n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"datetime\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if package == \"datetime\":  # datetime is built-in\n",
    "        print(f\"‚úì {package} (built-in)\")\n",
    "        continue\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"‚úì {package}\")\n",
    "\n",
    "print(\"\\nüéâ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_generation"
   },
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "We'll generate synthetic training data with various date formats commonly found in natural language text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "date_data_generator"
   },
   "outputs": [],
   "source": [
    "class DateStringGenerator:\n",
    "    \"\"\"Generate synthetic date strings for training the encoder-decoder model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.months = {\n",
    "            1: ['January', 'Jan'], 2: ['February', 'Feb'], 3: ['March', 'Mar'],\n",
    "            4: ['April', 'Apr'], 5: ['May'], 6: ['June', 'Jun'],\n",
    "            7: ['July', 'Jul'], 8: ['August', 'Aug'], 9: ['September', 'Sep'],\n",
    "            10: ['October', 'Oct'], 11: ['November', 'Nov'], 12: ['December', 'Dec']\n",
    "        }\n",
    "        \n",
    "        # Add Vietnamese month names for cross-lingual examples\n",
    "        self.vietnamese_months = {\n",
    "            1: 'th√°ng m·ªôt', 2: 'th√°ng hai', 3: 'th√°ng ba', 4: 'th√°ng t∆∞',\n",
    "            5: 'th√°ng nƒÉm', 6: 'th√°ng s√°u', 7: 'th√°ng b·∫£y', 8: 'th√°ng t√°m',\n",
    "            9: 'th√°ng ch√≠n', 10: 'th√°ng m∆∞·ªùi', 11: 'th√°ng m∆∞·ªùi m·ªôt', 12: 'th√°ng m∆∞·ªùi hai'\n",
    "        }\n",
    "    \n",
    "    def generate_date_pairs(self, num_samples=10000):\n",
    "        \"\"\"Generate pairs of (natural_format, iso_format) date strings.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Generate random date\n",
    "            year = random.randint(1900, 2030)\n",
    "            month = random.randint(1, 12)\n",
    "            day = random.randint(1, 28)  # Safe for all months\n",
    "            \n",
    "            # Create ISO format (target)\n",
    "            iso_date = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "            \n",
    "            # Create natural language format (source)\n",
    "            format_choice = random.choice(['full_month', 'short_month', 'numeric', 'reverse'])\n",
    "            \n",
    "            if format_choice == 'full_month':\n",
    "                month_name = random.choice(self.months[month])\n",
    "                natural_date = f\"{month_name} {day}, {year}\"\n",
    "            elif format_choice == 'short_month':\n",
    "                month_name = self.months[month][-1] if len(self.months[month]) > 1 else self.months[month][0]\n",
    "                natural_date = f\"{month_name} {day}, {year}\"\n",
    "            elif format_choice == 'numeric':\n",
    "                natural_date = f\"{month}/{day}/{year}\"\n",
    "            else:  # reverse\n",
    "                month_name = random.choice(self.months[month])\n",
    "                natural_date = f\"{day} {month_name} {year}\"\n",
    "            \n",
    "            pairs.append((natural_date, iso_date))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def add_vietnamese_examples(self, pairs, num_vietnamese=500):\n",
    "        \"\"\"Add Vietnamese date examples for cross-lingual capability.\"\"\"\n",
    "        vietnamese_pairs = []\n",
    "        \n",
    "        for _ in range(num_vietnamese):\n",
    "            year = random.randint(2000, 2030)\n",
    "            month = random.randint(1, 12)\n",
    "            day = random.randint(1, 28)\n",
    "            \n",
    "            iso_date = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "            vietnamese_month = self.vietnamese_months[month]\n",
    "            natural_date = f\"ng√†y {day} {vietnamese_month} nƒÉm {year}\"\n",
    "            \n",
    "            vietnamese_pairs.append((natural_date, iso_date))\n",
    "        \n",
    "        return pairs + vietnamese_pairs\n",
    "\n",
    "# Generate training data\n",
    "generator = DateStringGenerator()\n",
    "date_pairs = generator.generate_date_pairs(8000)\n",
    "date_pairs = generator.add_vietnamese_examples(date_pairs, 500)\n",
    "\n",
    "print(f\"Generated {len(date_pairs)} date conversion pairs\")\n",
    "print(\"\\nSample date pairs:\")\n",
    "for i in range(10):\n",
    "    source, target = date_pairs[i]\n",
    "    print(f\"  {source:25} ‚Üí {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_preprocessing"
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "We'll implement character-level tokenization and prepare the data for sequence-to-sequence training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenization"
   },
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Character-level tokenizer for date strings.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char_to_index = {}\n",
    "        self.index_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.pad_token = '<PAD>'\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build character vocabulary from texts.\"\"\"\n",
    "        all_chars = set()\n",
    "        \n",
    "        # Add special tokens\n",
    "        all_chars.update([self.start_token, self.end_token, self.pad_token])\n",
    "        \n",
    "        # Collect all characters\n",
    "        for text in texts:\n",
    "            all_chars.update(list(text))\n",
    "        \n",
    "        # Create mappings\n",
    "        sorted_chars = sorted(list(all_chars))\n",
    "        self.char_to_index = {char: i for i, char in enumerate(sorted_chars)}\n",
    "        self.index_to_char = {i: char for char, i in self.char_to_index.items()}\n",
    "        self.vocab_size = len(sorted_chars)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Characters: {''.join(sorted_chars[:50])}...\")\n",
    "    \n",
    "    def encode(self, text, max_length=None):\n",
    "        \"\"\"Encode text to indices.\"\"\"\n",
    "        indices = [self.char_to_index[char] for char in text if char in self.char_to_index]\n",
    "        \n",
    "        if max_length:\n",
    "            # Pad or truncate\n",
    "            if len(indices) < max_length:\n",
    "                indices.extend([self.char_to_index[self.pad_token]] * (max_length - len(indices)))\n",
    "            else:\n",
    "                indices = indices[:max_length]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode indices to text.\"\"\"\n",
    "        chars = []\n",
    "        for idx in indices:\n",
    "            if idx in self.index_to_char:\n",
    "                char = self.index_to_char[idx]\n",
    "                if char == self.pad_token:\n",
    "                    break\n",
    "                chars.append(char)\n",
    "        return ''.join(chars)\n",
    "\n",
    "# Prepare data\n",
    "sources = [pair[0] for pair in date_pairs]\n",
    "targets = [pair[1] for pair in date_pairs]\n",
    "\n",
    "# Build vocabularies\n",
    "all_texts = sources + targets\n",
    "tokenizer = CharacterTokenizer()\n",
    "tokenizer.build_vocab(all_texts)\n",
    "\n",
    "# Calculate max lengths\n",
    "max_source_length = max(len(s) for s in sources) + 1  # +1 for potential padding\n",
    "max_target_length = max(len(t) for t in targets) + 2  # +2 for START and END tokens\n",
    "\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_preparation"
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(sources, targets, tokenizer, max_source_len, max_target_len):\n",
    "    \"\"\"Prepare input and output sequences for training.\"\"\"\n",
    "    \n",
    "    encoder_input_data = np.zeros((len(sources), max_source_len), dtype='int32')\n",
    "    decoder_input_data = np.zeros((len(targets), max_target_len), dtype='int32')\n",
    "    decoder_target_data = np.zeros((len(targets), max_target_len), dtype='int32')\n",
    "    \n",
    "    for i, (source, target) in enumerate(zip(sources, targets)):\n",
    "        # Encoder input\n",
    "        encoder_input_data[i, :len(source)] = tokenizer.encode(source)[:max_source_len]\n",
    "        \n",
    "        # Decoder input (with START token)\n",
    "        target_with_start = tokenizer.start_token + target\n",
    "        decoder_input_data[i, :len(target_with_start)] = tokenizer.encode(target_with_start)[:max_target_len]\n",
    "        \n",
    "        # Decoder target (with END token)\n",
    "        target_with_end = target + tokenizer.end_token\n",
    "        decoder_target_data[i, :len(target_with_end)] = tokenizer.encode(target_with_end)[:max_target_len]\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "# Prepare training data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = prepare_sequences(\n",
    "    sources, targets, tokenizer, max_source_length, max_target_length\n",
    ")\n",
    "\n",
    "# Convert to one-hot for decoder targets\n",
    "decoder_target_data_one_hot = to_categorical(decoder_target_data, num_classes=tokenizer.vocab_size)\n",
    "\n",
    "print(f\"Encoder input shape: {encoder_input_data.shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target_data_one_hot.shape}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(date_pairs))\n",
    "train_encoder_input = encoder_input_data[:split_idx]\n",
    "train_decoder_input = decoder_input_data[:split_idx]\n",
    "train_decoder_target = decoder_target_data_one_hot[:split_idx]\n",
    "\n",
    "val_encoder_input = encoder_input_data[split_idx:]\n",
    "val_decoder_input = decoder_input_data[split_idx:]\n",
    "val_decoder_target = decoder_target_data_one_hot[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_encoder_input)}\")\n",
    "print(f\"Validation set size: {len(val_encoder_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_architecture"
   },
   "source": [
    "## 3. Encoder-Decoder Model Architecture\n",
    "\n",
    "We'll build an LSTM-based encoder-decoder model for sequence-to-sequence date conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_definition"
   },
   "outputs": [],
   "source": [
    "def create_encoder_decoder_model(vocab_size, max_source_len, max_target_len, \n",
    "                                embedding_dim=64, hidden_dim=128):\n",
    "    \"\"\"Create encoder-decoder model for date string conversion.\"\"\"\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(max_source_len,), name='encoder_input')\n",
    "    encoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "    encoder_lstm = LSTM(hidden_dim, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(max_target_len,), name='decoder_input')\n",
    "    decoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Training model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense\n",
    "\n",
    "# Create model\n",
    "model, encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense = create_encoder_decoder_model(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_source_len=max_source_length,\n",
    "    max_target_len=max_target_length,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train the encoder-decoder model to learn date string conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_training"
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    [train_encoder_input, train_decoder_input],\n",
    "    train_decoder_target,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_data=([val_encoder_input, val_decoder_input], val_decoder_target),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='#582C67')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='#C60C30')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', color='#582C67')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#C60C30')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_loss = history.history['val_loss'][-1]\n",
    "final_accuracy = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation loss: {final_loss:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 5. Inference Models\n",
    "\n",
    "Create separate encoder and decoder models for inference and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_models"
   },
   "outputs": [],
   "source": [
    "def create_inference_models(model, encoder_inputs, decoder_inputs, decoder_lstm, decoder_dense, hidden_dim):\n",
    "    \"\"\"Create encoder and decoder models for inference.\"\"\"\n",
    "    \n",
    "    # Encoder model for inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Decoder model for inference\n",
    "    decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Get decoder embedding layer from the trained model\n",
    "    decoder_embedding_layer = model.get_layer('decoder_input').output\n",
    "    decoder_embedding_inference = model.layers[3]  # Embedding layer\n",
    "    \n",
    "    decoder_inputs_single = Input(shape=(1,))\n",
    "    decoder_embedding_inf = decoder_embedding_inference(decoder_inputs_single)\n",
    "    decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
    "        decoder_embedding_inf, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "    decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_single] + decoder_states_inputs,\n",
    "        [decoder_outputs_inf] + decoder_states_inf\n",
    "    )\n",
    "    \n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "# Create inference models\n",
    "encoder_model, decoder_model = create_inference_models(\n",
    "    model, encoder_inputs, decoder_inputs, decoder_lstm, decoder_dense, hidden_dim=128\n",
    ")\n",
    "\n",
    "print(\"Inference models created successfully!\")\n",
    "print(f\"Encoder model input shape: {encoder_model.input_shape}\")\n",
    "print(f\"Encoder model output shape: {[state.shape for state in encoder_model.output]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prediction_function"
   },
   "outputs": [],
   "source": [
    "def predict_date_conversion(input_text, encoder_model, decoder_model, tokenizer, max_target_len):\n",
    "    \"\"\"Convert a natural language date to ISO format.\"\"\"\n",
    "    \n",
    "    # Encode input text\n",
    "    input_seq = np.zeros((1, max_source_length))\n",
    "    input_seq[0, :len(input_text)] = tokenizer.encode(input_text)[:max_source_length]\n",
    "    \n",
    "    # Get encoder states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Initialize decoder with start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.char_to_index[tokenizer.start_token]\n",
    "    \n",
    "    # Decode sequence\n",
    "    decoded_chars = []\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "        \n",
    "        # Sample next character\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer.index_to_char[sampled_token_index]\n",
    "        \n",
    "        if sampled_char == tokenizer.end_token or len(decoded_chars) > max_target_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_chars.append(sampled_char)\n",
    "        \n",
    "        # Update target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "print(\"Prediction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing"
   },
   "source": [
    "## 6. Testing and Evaluation\n",
    "\n",
    "Test the model on various date formats and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_examples"
   },
   "outputs": [],
   "source": [
    "# Test examples including Vietnamese dates\n",
    "test_examples = [\n",
    "    \"April 22, 2019\",\n",
    "    \"Dec 15, 2020\",\n",
    "    \"January 1, 2000\",\n",
    "    \"Jul 4, 1776\",\n",
    "    \"12/25/2023\",\n",
    "    \"3/14/2024\",\n",
    "    \"15 March 2021\",\n",
    "    \"ng√†y 22 th√°ng t∆∞ nƒÉm 2019\",  # Vietnamese: April 22, 2019\n",
    "    \"ng√†y 15 th√°ng m∆∞·ªùi hai nƒÉm 2020\",  # Vietnamese: December 15, 2020\n",
    "    \"ng√†y 1 th√°ng m·ªôt nƒÉm 2000\"  # Vietnamese: January 1, 2000\n",
    "]\n",
    "\n",
    "print(\"Testing Date String Converter\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_examples)\n",
    "\n",
    "for i, test_input in enumerate(test_examples):\n",
    "    try:\n",
    "        prediction = predict_date_conversion(\n",
    "            test_input, encoder_model, decoder_model, tokenizer, max_target_length\n",
    "        )\n",
    "        \n",
    "        print(f\"{i+1:2d}. {test_input:35} ‚Üí {prediction}\")\n",
    "        \n",
    "        # Simple validation - check if output looks like a date (YYYY-MM-DD)\n",
    "        if re.match(r'\\d{4}-\\d{2}-\\d{2}', prediction):\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{i+1:2d}. {test_input:35} ‚Üí ERROR: {str(e)}\")\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"\\nFormat Accuracy: {accuracy:.2%} ({correct_predictions}/{total_predictions})\")\n",
    "print(\"\\nNote: This measures format correctness (YYYY-MM-DD), not semantic accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_demo"
   },
   "outputs": [],
   "source": [
    "# Interactive demo function\n",
    "def demo_date_converter():\n",
    "    \"\"\"Interactive demonstration of the date converter.\"\"\"\n",
    "    \n",
    "    print(\"üóìÔ∏è  Date String Converter Demo\")\n",
    "    print(\"Convert natural language dates to ISO format (YYYY-MM-DD)\")\n",
    "    print(\"\\nSupported formats:\")\n",
    "    print(\"  - Month DD, YYYY (e.g., 'April 22, 2019')\")\n",
    "    print(\"  - MM/DD/YYYY (e.g., '4/22/2019')\")\n",
    "    print(\"  - DD Month YYYY (e.g., '22 April 2019')\")\n",
    "    print(\"  - Vietnamese format (e.g., 'ng√†y 22 th√°ng t∆∞ nƒÉm 2019')\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    sample_inputs = [\n",
    "        \"My name is\",  # Following repository example\n",
    "        \"April 22, 2019\",\n",
    "        \"Dec 31, 2023\", \n",
    "        \"1/1/2024\",\n",
    "        \"ng√†y 15 th√°ng nƒÉm nƒÉm 2022\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Sample conversions:\")\n",
    "    for sample in sample_inputs:\n",
    "        if \"My name is\" in sample:  # Skip non-date example\n",
    "            print(f\"English: '{sample}' ‚Üí Vietnamese: 'T√™n t√¥i l√†' (translation example)\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            result = predict_date_conversion(\n",
    "                sample, encoder_model, decoder_model, tokenizer, max_target_length\n",
    "            )\n",
    "            print(f\"Input: '{sample}' ‚Üí Output: '{result}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Input: '{sample}' ‚Üí Error: {str(e)}\")\n",
    "\n",
    "# Run demo\n",
    "demo_date_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_analysis"
   },
   "source": [
    "## 7. Model Analysis and Insights\n",
    "\n",
    "Analyze the model's performance and understand its strengths and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "character_distribution"
   },
   "outputs": [],
   "source": [
    "# Analyze character distribution in training data\n",
    "all_source_chars = ''.join(sources)\n",
    "all_target_chars = ''.join(targets)\n",
    "\n",
    "source_char_counts = Counter(all_source_chars)\n",
    "target_char_counts = Counter(all_target_chars)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Source character distribution\n",
    "common_source_chars = source_char_counts.most_common(15)\n",
    "chars, counts = zip(*common_source_chars)\n",
    "ax1.bar(chars, counts, color='#582C67', alpha=0.7)\n",
    "ax1.set_title('Source Character Distribution (Top 15)')\n",
    "ax1.set_xlabel('Characters')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Target character distribution  \n",
    "common_target_chars = target_char_counts.most_common(15)\n",
    "chars, counts = zip(*common_target_chars)\n",
    "ax2.bar(chars, counts, color='#C60C30', alpha=0.7)\n",
    "ax2.set_title('Target Character Distribution (Top 15)')\n",
    "ax2.set_xlabel('Characters')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Source vocabulary size: {len(source_char_counts)} unique characters\")\n",
    "print(f\"Target vocabulary size: {len(target_char_counts)} unique characters\")\n",
    "print(f\"Total vocabulary size: {tokenizer.vocab_size} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "error_analysis"
   },
   "outputs": [],
   "source": [
    "# Error analysis on validation set\n",
    "def analyze_predictions(num_samples=20):\n",
    "    \"\"\"Analyze model predictions on validation data.\"\"\"\n",
    "    \n",
    "    print(\"Prediction Analysis on Validation Set\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    val_indices = np.random.choice(len(val_encoder_input), num_samples, replace=False)\n",
    "    correct_format = 0\n",
    "    \n",
    "    for i, idx in enumerate(val_indices):\n",
    "        # Get original texts\n",
    "        source_idx = split_idx + idx\n",
    "        original_source = sources[source_idx]\n",
    "        original_target = targets[source_idx]\n",
    "        \n",
    "        # Predict\n",
    "        try:\n",
    "            prediction = predict_date_conversion(\n",
    "                original_source, encoder_model, decoder_model, tokenizer, max_target_length\n",
    "            )\n",
    "            \n",
    "            # Check if format is correct\n",
    "            format_correct = re.match(r'\\d{4}-\\d{2}-\\d{2}', prediction) is not None\n",
    "            exact_match = prediction.strip() == original_target\n",
    "            \n",
    "            if format_correct:\n",
    "                correct_format += 1\n",
    "            \n",
    "            status = \"‚úì\" if exact_match else \"~\" if format_correct else \"‚úó\"\n",
    "            print(f\"{i+1:2d}. {status} {original_source:30} ‚Üí {prediction:12} (expected: {original_target})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{i+1:2d}. ‚úó {original_source:30} ‚Üí ERROR: {str(e)}\")\n",
    "    \n",
    "    format_accuracy = correct_format / num_samples\n",
    "    print(f\"\\nFormat accuracy: {format_accuracy:.2%}\")\n",
    "    print(\"Legend: ‚úì = Exact match, ~ = Correct format, ‚úó = Incorrect\")\n",
    "\n",
    "analyze_predictions(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 8. Conclusion and Future Improvements\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Built an Encoder-Decoder Model**: Created a character-level LSTM sequence-to-sequence model\n",
    "2. **Multi-format Support**: Handles various date formats including Vietnamese dates\n",
    "3. **Educational Implementation**: Simple, understandable code using TensorFlow/Keras\n",
    "4. **Data Generation**: Synthetic training data for robust learning\n",
    "\n",
    "### Model Architecture Summary\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Date String:<br>'April 22, 2019'] --> B[Character Tokenization]\n",
    "    B --> C[Encoder LSTM]\n",
    "    C --> D[Context Vector]\n",
    "    D --> E[Decoder LSTM]\n",
    "    E --> F[Character Generation]\n",
    "    F --> G[Output Date String:<br>'2019-04-22']\n",
    "\n",
    "    style A fill:#FFFFFF,stroke:#582C67,color:#333,stroke-width:2px\n",
    "    style B fill:#582C67,stroke:#C60C30,color:#FFFFFF,stroke-width:2px\n",
    "    style C fill:#C60C30,stroke:#582C67,color:#FFFFFF,stroke-width:2px\n",
    "    style D fill:#582C67,stroke:#C60C30,color:#FFFFFF,stroke-width:2px\n",
    "    style E fill:#C60C30,stroke:#582C67,color:#FFFFFF,stroke-width:2px\n",
    "    style F fill:#582C67,stroke:#C60C30,color:#FFFFFF,stroke-width:2px\n",
    "    style G fill:#FFFFFF,stroke:#582C67,color:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **Character-level tokenization** works well for date conversion tasks\n",
    "- **LSTM encoder-decoder** architecture is effective for sequence transformation\n",
    "- **Synthetic data generation** allows for comprehensive training coverage\n",
    "- **Cross-lingual support** (Vietnamese/English) demonstrates model flexibility\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Attention Mechanism**: Add attention to improve long sequence handling\n",
    "2. **Transformer Architecture**: Upgrade to transformer-based models\n",
    "3. **More Languages**: Expand to other languages and date formats\n",
    "4. **Date Validation**: Add semantic validation for generated dates\n",
    "5. **Error Handling**: Better handling of invalid input formats\n",
    "6. **Real-world Data**: Train on actual text data with dates\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Data Cleaning**: Standardize date formats in datasets\n",
    "- **Information Extraction**: Parse dates from unstructured text\n",
    "- **Document Processing**: Convert dates in multilingual documents\n",
    "- **API Development**: Date format conversion services\n",
    "\n",
    "This notebook demonstrates how encoder-decoder models can solve practical string transformation problems with minimal complexity while maintaining educational value."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}