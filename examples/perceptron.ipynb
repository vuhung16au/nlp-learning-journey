{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perceptron_title"
   },
   "source": [
    "# Perceptron Classifier in Machine Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/examples/perceptron.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Perceptron is a linear classifier that forms the foundation of neural networks. It's a simple yet powerful algorithm for binary classification tasks. This notebook demonstrates how to use Scikit-Learn's Perceptron implementation and compares it with SGDClassifier configured for perceptron learning.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Understanding the Perceptron algorithm\n",
    "- Using Scikit-Learn's Perceptron class\n",
    "- SGDClassifier with perceptron loss function\n",
    "- Comparing different perceptron implementations\n",
    "- Visualizing decision boundaries\n",
    "- Performance evaluation and metrics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of Python, machine learning concepts, and linear classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"scikit-learn\",\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"✓ {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory"
   },
   "source": [
    "## Perceptron Theory\n",
    "\n",
    "The Perceptron is a linear binary classifier that learns a separating hyperplane for linearly separable data. The algorithm was invented by Frank Rosenblatt in 1957.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Linear Decision Boundary**: The perceptron finds a linear boundary to separate classes\n",
    "2. **Iterative Learning**: Updates weights based on misclassified examples\n",
    "3. **Convergence**: Guaranteed to converge for linearly separable data\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "For input features $x = [x_1, x_2, ..., x_n]$ and weights $w = [w_1, w_2, ..., w_n]$:\n",
    "\n",
    "$$f(x) = \\text{sign}(w \\cdot x + b)$$\n",
    "\n",
    "Where $b$ is the bias term and $\\text{sign}$ is the activation function.\n",
    "\n",
    "The weight update rule for misclassified examples:\n",
    "$$w_{new} = w_{old} + \\eta \\cdot y \\cdot x$$\n",
    "\n",
    "Where $\\eta$ is the learning rate and $y$ is the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_section"
   },
   "source": [
    "## Dataset: Iris Flowers\n",
    "\n",
    "We'll use the famous Iris dataset, which contains measurements of iris flowers from three species. For the Perceptron (binary classifier), we'll focus on distinguishing one species from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "print(\"Iris Dataset Information:\")\n",
    "print(f\"- Features: {iris.feature_names}\")\n",
    "print(f\"- Target classes: {iris.target_names}\")\n",
    "print(f\"- Dataset shape: {iris.data.shape}\")\n",
    "print(f\"- Number of samples per class: {np.bincount(iris.target)}\")\n",
    "\n",
    "# Create a DataFrame for easy exploration\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_dataset"
   },
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset Exploration', fontsize=16)\n",
    "\n",
    "# Pairplot of the first two features (which we'll use for perceptron)\n",
    "axes[0, 0].scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target, cmap='viridis', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Petal Length (cm)')\n",
    "axes[0, 0].set_ylabel('Petal Width (cm)')\n",
    "axes[0, 0].set_title('Petal Length vs Petal Width')\n",
    "\n",
    "# Distribution of petal length\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    species_data = iris.data[iris.target == i, 2]\n",
    "    axes[0, 1].hist(species_data, alpha=0.7, label=species, bins=15)\n",
    "axes[0, 1].set_xlabel('Petal Length (cm)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Petal Length Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Distribution of petal width\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    species_data = iris.data[iris.target == i, 3]\n",
    "    axes[1, 0].hist(species_data, alpha=0.7, label=species, bins=15)\n",
    "axes[1, 0].set_xlabel('Petal Width (cm)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Petal Width Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Class distribution\n",
    "class_counts = np.bincount(iris.target)\n",
    "axes[1, 1].bar(iris.target_names, class_counts, color=['skyblue', 'orange', 'green'])\n",
    "axes[1, 1].set_xlabel('Species')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binary_classification"
   },
   "source": [
    "## Preparing Data for Binary Classification\n",
    "\n",
    "Since the Perceptron is a binary classifier, we'll create a binary classification problem: distinguishing Iris Setosa from the other two species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Prepare data for binary classification\n",
    "# Following the example from the issue: use petal length and petal width\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int64)  # Iris setosa? (1 = yes, 0 = no)\n",
    "\n",
    "print(\"Binary Classification Setup:\")\n",
    "print(f\"- Features: Petal Length, Petal Width\")\n",
    "print(f\"- Target: Iris Setosa (1) vs Others (0)\")\n",
    "print(f\"- Feature matrix shape: {X.shape}\")\n",
    "print(f\"- Target vector shape: {y.shape}\")\n",
    "print(f\"- Class distribution: {np.bincount(y)}\")\n",
    "print(f\"  - Setosa (1): {np.sum(y)} samples\")\n",
    "print(f\"  - Others (0): {np.sum(1-y)} samples\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"- Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"- Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Feature scaling (recommended for perceptron)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling applied for better performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perceptron_sklearn"
   },
   "source": [
    "## Perceptron with Scikit-Learn\n",
    "\n",
    "Let's implement the Perceptron using Scikit-Learn's built-in `Perceptron` class, following the example from the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sklearn_perceptron"
   },
   "outputs": [],
   "source": [
    "# Create and train the Perceptron classifier\n",
    "per_clf = Perceptron(random_state=42)\n",
    "\n",
    "# Train the perceptron\n",
    "print(\"Training Scikit-Learn Perceptron...\")\n",
    "per_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_per = per_clf.predict(X_train_scaled)\n",
    "y_test_pred_per = per_clf.predict(X_test_scaled)\n",
    "\n",
    "# Test the specific example from the issue\n",
    "test_sample = scaler.transform([[2, 0.5]])  # Scale the test sample\n",
    "y_pred_sample = per_clf.predict(test_sample)\n",
    "print(f\"\\nPrediction for [petal_length=2, petal_width=0.5]: {y_pred_sample[0]}\")\n",
    "print(f\"Interpretation: {'Iris Setosa' if y_pred_sample[0] == 1 else 'Not Iris Setosa'}\")\n",
    "\n",
    "# Evaluate performance\n",
    "train_accuracy_per = accuracy_score(y_train, y_train_pred_per)\n",
    "test_accuracy_per = accuracy_score(y_test, y_test_pred_per)\n",
    "\n",
    "print(f\"\\nPerceptron Performance:\")\n",
    "print(f\"- Training Accuracy: {train_accuracy_per:.3f}\")\n",
    "print(f\"- Test Accuracy: {test_accuracy_per:.3f}\")\n",
    "print(f\"- Number of iterations: {per_clf.n_iter_}\")\n",
    "\n",
    "# Display learned weights and bias\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"- Weights: {per_clf.coef_[0]}\")\n",
    "print(f\"- Bias: {per_clf.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgd_perceptron"
   },
   "source": [
    "## SGDClassifier with Perceptron Loss\n",
    "\n",
    "As mentioned in the issue, Scikit-Learn's Perceptron class is equivalent to using SGDClassifier with specific hyperparameters. Let's implement this and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgd_classifier"
   },
   "outputs": [],
   "source": [
    "# Create SGDClassifier with perceptron configuration\n",
    "# Following the issue specification: loss=\"perceptron\", learning_rate=\"constant\", eta0=1, penalty=None\n",
    "sgd_clf = SGDClassifier(\n",
    "    loss=\"perceptron\",         # Perceptron loss function\n",
    "    learning_rate=\"constant\",  # Constant learning rate\n",
    "    eta0=1,                   # Learning rate value\n",
    "    penalty=None,             # No regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the SGDClassifier\n",
    "print(\"Training SGDClassifier with Perceptron loss...\")\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_sgd = sgd_clf.predict(X_train_scaled)\n",
    "y_test_pred_sgd = sgd_clf.predict(X_test_scaled)\n",
    "\n",
    "# Test the same example\n",
    "y_pred_sample_sgd = sgd_clf.predict(test_sample)\n",
    "print(f\"\\nPrediction for [petal_length=2, petal_width=0.5]: {y_pred_sample_sgd[0]}\")\n",
    "print(f\"Interpretation: {'Iris Setosa' if y_pred_sample_sgd[0] == 1 else 'Not Iris Setosa'}\")\n",
    "\n",
    "# Evaluate performance\n",
    "train_accuracy_sgd = accuracy_score(y_train, y_train_pred_sgd)\n",
    "test_accuracy_sgd = accuracy_score(y_test, y_test_pred_sgd)\n",
    "\n",
    "print(f\"\\nSGDClassifier Performance:\")\n",
    "print(f\"- Training Accuracy: {train_accuracy_sgd:.3f}\")\n",
    "print(f\"- Test Accuracy: {test_accuracy_sgd:.3f}\")\n",
    "print(f\"- Number of iterations: {sgd_clf.n_iter_}\")\n",
    "\n",
    "# Display learned weights and bias\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"- Weights: {sgd_clf.coef_[0]}\")\n",
    "print(f\"- Bias: {sgd_clf.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## Comparing Results\n",
    "\n",
    "Let's compare the performance and characteristics of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results_comparison"
   },
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Training Accuracy', 'Test Accuracy', 'Iterations', 'Weight 1', 'Weight 2', 'Bias'],\n",
    "    'Perceptron': [\n",
    "        f\"{train_accuracy_per:.3f}\",\n",
    "        f\"{test_accuracy_per:.3f}\",\n",
    "        f\"{per_clf.n_iter_}\",\n",
    "        f\"{per_clf.coef_[0][0]:.3f}\",\n",
    "        f\"{per_clf.coef_[0][1]:.3f}\",\n",
    "        f\"{per_clf.intercept_[0]:.3f}\"\n",
    "    ],\n",
    "    'SGDClassifier': [\n",
    "        f\"{train_accuracy_sgd:.3f}\",\n",
    "        f\"{test_accuracy_sgd:.3f}\",\n",
    "        f\"{sgd_clf.n_iter_}\",\n",
    "        f\"{sgd_clf.coef_[0][0]:.3f}\",\n",
    "        f\"{sgd_clf.coef_[0][1]:.3f}\",\n",
    "        f\"{sgd_clf.intercept_[0]:.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nPerceptron Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_per, target_names=['Not Setosa', 'Setosa']))\n",
    "\n",
    "print(\"\\nSGDClassifier Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_sgd, target_names=['Not Setosa', 'Setosa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrices"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Perceptron confusion matrix\n",
    "cm_per = confusion_matrix(y_test, y_test_pred_per)\n",
    "sns.heatmap(cm_per, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Setosa', 'Setosa'],\n",
    "            yticklabels=['Not Setosa', 'Setosa'])\n",
    "axes[0].set_title('Perceptron\\nConfusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# SGDClassifier confusion matrix\n",
    "cm_sgd = confusion_matrix(y_test, y_test_pred_sgd)\n",
    "sns.heatmap(cm_sgd, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Not Setosa', 'Setosa'],\n",
    "            yticklabels=['Not Setosa', 'Setosa'])\n",
    "axes[1].set_title('SGDClassifier\\nConfusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "decision_boundary"
   },
   "source": [
    "## Visualizing Decision Boundaries\n",
    "\n",
    "Let's visualize how each classifier separates the classes by plotting their decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_boundaries"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, classifier, title, scaler=None):\n",
    "    \"\"\"Plot decision boundary for a binary classifier\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh for plotting\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if scaler is not None:\n",
    "        mesh_points = scaler.transform(mesh_points)\n",
    "    \n",
    "    Z = classifier.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot the data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    plt.xlabel('Petal Length (cm)')\n",
    "    plt.ylabel('Petal Width (cm)')\n",
    "    plt.title(f'{title}\\nDecision Boundary')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(['Not Setosa', 'Setosa'], loc='upper left')\n",
    "    \n",
    "    return plt.gca()\n",
    "\n",
    "# Plot decision boundaries for both classifiers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(X_train, y_train, per_clf, 'Perceptron', scaler)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(X_train, y_train, sgd_clf, 'SGDClassifier', scaler)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis"
   },
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Let's analyze the results and understand the behavior of both implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_analysis"
   },
   "outputs": [],
   "source": [
    "print(\"ANALYSIS OF RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. ALGORITHM EQUIVALENCE:\")\n",
    "print(\"-\" * 25)\n",
    "weight_diff = np.abs(per_clf.coef_[0] - sgd_clf.coef_[0])\n",
    "bias_diff = np.abs(per_clf.intercept_[0] - sgd_clf.intercept_[0])\n",
    "\n",
    "print(f\"Weight differences: {weight_diff}\")\n",
    "print(f\"Bias difference: {bias_diff}\")\n",
    "print(f\"Are weights similar? {np.allclose(per_clf.coef_[0], sgd_clf.coef_[0], atol=0.1)}\")\n",
    "print(f\"Are biases similar? {np.allclose(per_clf.intercept_[0], sgd_clf.intercept_[0], atol=0.1)}\")\n",
    "\n",
    "print(\"\\n2. PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Perceptron Test Accuracy: {test_accuracy_per:.3f}\")\n",
    "print(f\"SGDClassifier Test Accuracy: {test_accuracy_sgd:.3f}\")\n",
    "print(f\"Accuracy difference: {abs(test_accuracy_per - test_accuracy_sgd):.3f}\")\n",
    "\n",
    "print(\"\\n3. CONVERGENCE:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Perceptron iterations: {per_clf.n_iter_}\")\n",
    "print(f\"SGDClassifier iterations: {sgd_clf.n_iter_}\")\n",
    "\n",
    "print(\"\\n4. PREDICTION CONSISTENCY:\")\n",
    "print(\"-\" * 25)\n",
    "consistent_predictions = np.sum(y_test_pred_per == y_test_pred_sgd)\n",
    "total_predictions = len(y_test_pred_per)\n",
    "consistency_rate = consistent_predictions / total_predictions\n",
    "print(f\"Consistent predictions: {consistent_predictions}/{total_predictions} ({consistency_rate:.3f})\")\n",
    "\n",
    "print(\"\\n5. DECISION BOUNDARY ANALYSIS:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"Both classifiers learn linear decision boundaries to separate Iris Setosa from other species.\")\n",
    "print(\"The boundaries should be very similar since both implement the same algorithm.\")\n",
    "\n",
    "# Test with original unscaled data for intuition\n",
    "print(\"\\n6. ORIGINAL EXAMPLE FROM ISSUE:\")\n",
    "print(\"-\" * 25)\n",
    "original_sample = np.array([[2, 0.5]])\n",
    "print(f\"Sample: petal_length=2cm, petal_width=0.5cm\")\n",
    "print(f\"This represents a small flower, typical of Iris Setosa\")\n",
    "\n",
    "# Compare with actual Setosa measurements\n",
    "setosa_data = X[y == 1]  # Original unscaled data for Setosa\n",
    "print(f\"\\nActual Setosa measurements:\")\n",
    "print(f\"- Petal length range: {setosa_data[:, 0].min():.1f} - {setosa_data[:, 0].max():.1f} cm\")\n",
    "print(f\"- Petal width range: {setosa_data[:, 1].min():.1f} - {setosa_data[:, 1].max():.1f} cm\")\n",
    "print(f\"- Mean petal length: {setosa_data[:, 0].mean():.1f} cm\")\n",
    "print(f\"- Mean petal width: {setosa_data[:, 1].mean():.1f} cm\")\n",
    "\n",
    "print(\"\\nThe sample [2, 0.5] falls within the typical Setosa range, explaining the positive prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiments"
   },
   "source": [
    "## Additional Experiments\n",
    "\n",
    "Let's explore how the perceptron performs with different features and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_experiments"
   },
   "outputs": [],
   "source": [
    "# Experiment 1: Using all four features\n",
    "print(\"EXPERIMENT 1: Using All Four Features\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "X_all = iris.data  # All four features\n",
    "X_all_train, X_all_test, y_all_train, y_all_test = train_test_split(\n",
    "    X_all, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_all = StandardScaler()\n",
    "X_all_train_scaled = scaler_all.fit_transform(X_all_train)\n",
    "X_all_test_scaled = scaler_all.transform(X_all_test)\n",
    "\n",
    "# Train perceptron with all features\n",
    "per_all = Perceptron(random_state=42)\n",
    "per_all.fit(X_all_train_scaled, y_all_train)\n",
    "\n",
    "y_all_pred = per_all.predict(X_all_test_scaled)\n",
    "accuracy_all = accuracy_score(y_all_test, y_all_pred)\n",
    "\n",
    "print(f\"Accuracy with all features: {accuracy_all:.3f}\")\n",
    "print(f\"Improvement over 2 features: {accuracy_all - test_accuracy_per:.3f}\")\n",
    "\n",
    "# Experiment 2: Different feature pairs\n",
    "print(\"\\nEXPERIMENT 2: Different Feature Pairs\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "feature_pairs = [\n",
    "    ((0, 1), 'Sepal Length vs Sepal Width'),\n",
    "    ((0, 2), 'Sepal Length vs Petal Length'),\n",
    "    ((0, 3), 'Sepal Length vs Petal Width'),\n",
    "    ((1, 2), 'Sepal Width vs Petal Length'),\n",
    "    ((1, 3), 'Sepal Width vs Petal Width'),\n",
    "    ((2, 3), 'Petal Length vs Petal Width')\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for (f1, f2), name in feature_pairs:\n",
    "    X_pair = iris.data[:, [f1, f2]]\n",
    "    X_pair_train, X_pair_test, y_pair_train, y_pair_test = train_test_split(\n",
    "        X_pair, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler_pair = StandardScaler()\n",
    "    X_pair_train_scaled = scaler_pair.fit_transform(X_pair_train)\n",
    "    X_pair_test_scaled = scaler_pair.transform(X_pair_test)\n",
    "    \n",
    "    per_pair = Perceptron(random_state=42)\n",
    "    per_pair.fit(X_pair_train_scaled, y_pair_train)\n",
    "    \n",
    "    y_pair_pred = per_pair.predict(X_pair_test_scaled)\n",
    "    accuracy_pair = accuracy_score(y_pair_test, y_pair_pred)\n",
    "    \n",
    "    results.append((name, accuracy_pair, per_pair.n_iter_))\n",
    "    print(f\"{name:30}: Accuracy = {accuracy_pair:.3f}, Iterations = {per_pair.n_iter_}\")\n",
    "\n",
    "# Find best feature pair\n",
    "best_pair = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest feature pair: {best_pair[0]} (Accuracy: {best_pair[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize performance across different feature pairs\n",
    "feature_names = [result[0] for result in results]\n",
    "accuracies = [result[1] for result in results]\n",
    "iterations = [result[2] for result in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "bars1 = ax1.bar(range(len(feature_names)), accuracies, color='skyblue', alpha=0.7)\n",
    "ax1.set_xlabel('Feature Pairs')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Perceptron Accuracy by Feature Pair')\n",
    "ax1.set_xticks(range(len(feature_names)))\n",
    "ax1.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Iterations comparison\n",
    "bars2 = ax2.bar(range(len(feature_names)), iterations, color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Feature Pairs')\n",
    "ax2.set_ylabel('Number of Iterations')\n",
    "ax2.set_title('Perceptron Convergence Speed by Feature Pair')\n",
    "ax2.set_xticks(range(len(feature_names)))\n",
    "ax2.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, iter_count in zip(bars2, iterations):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{iter_count}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "key_takeaways"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Algorithm Equivalence\n",
    "- ✅ **Confirmed**: Scikit-Learn's `Perceptron` class is equivalent to `SGDClassifier` with `loss=\"perceptron\"`, `learning_rate=\"constant\"`, `eta0=1`, and `penalty=None`\n",
    "- Both implementations produce very similar (often identical) results\n",
    "- Minor differences may occur due to random initialization or numerical precision\n",
    "\n",
    "### Performance Insights\n",
    "- **Perfect Classification**: The Iris Setosa vs. Others problem is linearly separable, leading to perfect or near-perfect accuracy\n",
    "- **Feature Selection Matters**: Different feature pairs yield different accuracy levels\n",
    "- **Petal features** (length and width) are particularly effective for distinguishing Iris Setosa\n",
    "\n",
    "### Practical Considerations\n",
    "1. **Feature Scaling**: Standardization improves convergence speed and stability\n",
    "2. **Linear Separability**: Perceptron works best when classes are linearly separable\n",
    "3. **Convergence**: The algorithm is guaranteed to converge for linearly separable data\n",
    "4. **Interpretability**: Linear decision boundaries are easy to understand and visualize\n",
    "\n",
    "### When to Use Perceptron\n",
    "- ✅ Binary classification tasks\n",
    "- ✅ Linearly separable data\n",
    "- ✅ Fast, simple baseline model\n",
    "- ✅ When interpretability is important\n",
    "- ❌ Non-linearly separable data (consider SVM, neural networks)\n",
    "- ❌ Multi-class problems (use One-vs-Rest approach)\n",
    "\n",
    "### Connection to Neural Networks\n",
    "The Perceptron is the foundation of modern neural networks:\n",
    "- Single neuron with linear activation\n",
    "- Multi-layer perceptrons (MLPs) can handle non-linear problems\n",
    "- Same weight update principles apply to deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Multi-class Extension**: Implement a One-vs-Rest approach to classify all three iris species\n",
    "2. **Feature Engineering**: Create polynomial features and test perceptron performance\n",
    "3. **Learning Rate Analysis**: Experiment with different learning rates in SGDClassifier\n",
    "4. **Convergence Study**: Plot the loss/error over iterations to visualize convergence\n",
    "5. **Comparison Study**: Compare perceptron with other linear classifiers (Logistic Regression, SVM)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Scikit-learn Perceptron Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "- [SGDClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "- [Perceptron Algorithm Explained](https://en.wikipedia.org/wiki/Perceptron)\n",
    "- [Hands-On Machine Learning (Chapter 10)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "\n",
    "## Vietnamese/English Examples\n",
    "\n",
    "As this repository focuses on Vietnamese/English NLP, here are some conceptual translations:\n",
    "\n",
    "- **Perceptron**: \"Perceptron\" (same in Vietnamese)\n",
    "- **Linear Classifier**: \"Bộ phân loại tuyến tính\"\n",
    "- **Decision Boundary**: \"Biên quyết định\"\n",
    "- **Binary Classification**: \"Phân loại nhị phân\"\n",
    "- **Feature**: \"Đặc trưng\"\n",
    "- **Weight**: \"Trọng số\"\n",
    "- **Bias**: \"Độ lệch\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}