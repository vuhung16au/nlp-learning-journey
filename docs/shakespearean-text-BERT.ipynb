{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespearean Text Generation using BERT with PyTorch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/nlp-learning-journey/blob/main/docs/shakespearean-text-BERT.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use BERT (Bidirectional Encoder Representations from Transformers) for generating Shakespearean-style text using PyTorch. While BERT is primarily designed for understanding tasks rather than generation, we can leverage its masked language modeling capabilities to create text in the style of Shakespeare.\n",
    "\n",
    "**Note**: This repository prioritizes PyTorch over TensorFlow. This notebook has been updated to use PyTorch implementations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to adapt BERT for text generation using masked language modeling\n",
    "- Fine-tuning BERT on Shakespeare's works using PyTorch\n",
    "- Implementing iterative text generation with BERT\n",
    "- Comparing BERT-based generation with traditional autoregressive models\n",
    "- Educational insights about BERT's bidirectional nature\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of transformers, BERT architecture, and Python programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup (Required for all notebooks in this repository)\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq libpq-dev\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")\n",
    "\n",
    "# PyTorch logging setup\n",
    "def setup_pytorch_logging():\n",
    "    \"\"\"Setup platform-specific PyTorch logging directories.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        root_logdir = \"/content/pytorch_logs\"\n",
    "    elif IS_KAGGLE:\n",
    "        root_logdir = \"./pytorch_logs\"\n",
    "    else:\n",
    "        root_logdir = os.path.join(os.getcwd(), \"pytorch_logs\")\n",
    "    \n",
    "    os.makedirs(root_logdir, exist_ok=True)\n",
    "    return root_logdir\n",
    "\n",
    "def get_run_logdir(experiment_name=\"run\"):\n",
    "    \"\"\"Generate unique run directory for training logs.\"\"\"\n",
    "    root_logdir = setup_pytorch_logging()\n",
    "    run_id = time.strftime(f\"{experiment_name}_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# Install required packages for this notebook\n",
    "required_packages = [\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"requests\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"✓ {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForMaskedLM, BertConfig,\n",
    "    pipeline\n",
    ")\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Text Generator Class\n",
    "\n",
    "We'll create a PyTorch-based text generator using BERT's masked language modeling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTextGenerator:\n",
    "    \"\"\"PyTorch-based BERT text generator using masked language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"Initialize the BERT text generator.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load BERT model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            # Use PyTorch version\n",
    "            self.model = BertForMaskedLM.from_pretrained(self.model_name)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"✓ Loaded BERT model: {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Could not load BERT model: {e}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def predict_masked_words(self, text, num_predictions=5):\n",
    "        \"\"\"Predict words for [MASK] tokens in the text.\"\"\"\n",
    "        if self.model is None:\n",
    "            return []\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "        \n",
    "        # Find mask positions\n",
    "        mask_token_id = self.tokenizer.mask_token_id\n",
    "        mask_positions = (inputs['input_ids'] == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        results = []\n",
    "        for pos in mask_positions:\n",
    "            # Get top predictions for this position\n",
    "            masked_predictions = predictions[0, pos]\n",
    "            top_k = torch.topk(masked_predictions, num_predictions)\n",
    "            \n",
    "            predicted_tokens = []\n",
    "            for i, (score, token_id) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "                token = self.tokenizer.decode([token_id])\n",
    "                predicted_tokens.append({\n",
    "                    'token': token,\n",
    "                    'score': score.item(),\n",
    "                    'probability': torch.softmax(masked_predictions, dim=-1)[token_id].item()\n",
    "                })\n",
    "            \n",
    "            results.append(predicted_tokens)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_shakespearean_text(self, prompt, max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate Shakespearean-style text using iterative masking.\"\"\"\n",
    "        if self.model is None:\n",
    "            return \"Model not loaded\"\n",
    "        \n",
    "        # Start with the prompt\n",
    "        generated_text = prompt.strip()\n",
    "        \n",
    "        # Generate additional words iteratively\n",
    "        for _ in range(max_length):\n",
    "            # Add a mask token\n",
    "            masked_text = generated_text + \" [MASK]\"\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = self.predict_masked_words(masked_text, num_predictions=10)\n",
    "            \n",
    "            if not predictions:\n",
    "                break\n",
    "            \n",
    "            # Sample from top predictions with temperature\n",
    "            top_predictions = predictions[0]\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            scores = torch.tensor([p['score'] for p in top_predictions])\n",
    "            probabilities = torch.softmax(scores / temperature, dim=-1)\n",
    "            \n",
    "            # Sample a token\n",
    "            selected_idx = torch.multinomial(probabilities, 1).item()\n",
    "            selected_token = top_predictions[selected_idx]['token']\n",
    "            \n",
    "            # Add the selected token\n",
    "            generated_text += \" \" + selected_token.strip()\n",
    "            \n",
    "            # Stop if we hit punctuation that ends a sentence\n",
    "            if selected_token.strip() in ['.', '!', '?']:\n",
    "                break\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "# Initialize the generator\n",
    "bert_generator = BERTTextGenerator()\n",
    "print(\"BERT text generator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BERT's Masked Language Modeling\n",
    "\n",
    "Let's test BERT's ability to fill in masked words in Shakespearean contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test masked language modeling with Shakespearean examples\n",
    "test_sentences = [\n",
    "    \"To be or not to [MASK], that is the question.\",\n",
    "    \"Romeo, Romeo, wherefore art thou [MASK]?\",\n",
    "    \"All the world's a [MASK], and all the men and women merely players.\",\n",
    "    \"What light through yonder [MASK] breaks?\",\n",
    "    \"Fair is [MASK] and foul is fair.\"\n",
    "]\n",
    "\n",
    "print(\"🎭 Testing BERT's Shakespearean Knowledge:\\n\")\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    predictions = bert_generator.predict_masked_words(sentence, num_predictions=3)\n",
    "    \n",
    "    if predictions:\n",
    "        print(\"Top predictions:\")\n",
    "        for i, pred in enumerate(predictions[0][:3]):\n",
    "            print(f\"  {i+1}. {pred['token']} (confidence: {pred['probability']:.3f})\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean-Style Text\n",
    "\n",
    "Now let's use our generator to create new Shakespearean-style text starting from various prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Shakespearean-style text with different prompts\n",
    "prompts = [\n",
    "    \"Shall I compare thee\",\n",
    "    \"Once upon a midnight\",\n",
    "    \"Love is\",\n",
    "    \"The fair maiden\",\n",
    "    \"In the forest deep\"\n",
    "]\n",
    "\n",
    "print(\"🎭 Generating Shakespearean-Style Text:\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    \n",
    "    # Generate with different temperatures\n",
    "    for temp in [0.8, 1.2]:\n",
    "        generated = bert_generator.generate_shakespearean_text(\n",
    "            prompt, max_length=15, temperature=temp\n",
    "        )\n",
    "        print(f\"  Temperature {temp}: {generated}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Text Generation\n",
    "\n",
    "Let's create an interactive function where you can input your own prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_generation(prompt=\"To be or not\", max_length=20, temperature=1.0):\n",
    "    \"\"\"Interactive Shakespearean text generation.\"\"\"\n",
    "    print(f\"\\n🎭 Generating from prompt: '{prompt}'\")\n",
    "    print(f\"Parameters: max_length={max_length}, temperature={temperature}\\n\")\n",
    "    \n",
    "    # Generate multiple variations\n",
    "    for i in range(3):\n",
    "        generated = bert_generator.generate_shakespearean_text(\n",
    "            prompt, max_length=max_length, temperature=temperature\n",
    "        )\n",
    "        print(f\"Variation {i+1}: {generated}\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Example usage\n",
    "result = interactive_generation(\n",
    "    prompt=\"The course of true love\", \n",
    "    max_length=15, \n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# You can modify these parameters to experiment:\n",
    "# interactive_generation(\"My love is\", max_length=10, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: BERT vs Autoregressive Models\n",
    "\n",
    "Let's analyze the differences between BERT's bidirectional generation and traditional autoregressive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison analysis\n",
    "def analyze_generation_quality(prompts, num_samples=5):\n",
    "    \"\"\"Analyze the quality and characteristics of BERT-generated text.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nAnalyzing prompt: '{prompt}'\")\n",
    "        \n",
    "        generations = []\n",
    "        for i in range(num_samples):\n",
    "            generated = bert_generator.generate_shakespearean_text(\n",
    "                prompt, max_length=15, temperature=1.0\n",
    "            )\n",
    "            generations.append(generated)\n",
    "            print(f\"  {i+1}. {generated}\")\n",
    "        \n",
    "        # Simple analysis\n",
    "        avg_length = sum(len(g.split()) for g in generations) / len(generations)\n",
    "        unique_words = set()\n",
    "        for g in generations:\n",
    "            unique_words.update(g.lower().split())\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'avg_length': avg_length,\n",
    "            'unique_words': len(unique_words),\n",
    "            'generations': generations\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze different types of prompts\n",
    "analysis_prompts = [\n",
    "    \"Love is\",\n",
    "    \"The king\",\n",
    "    \"In fair Verona\"\n",
    "]\n",
    "\n",
    "analysis_results = analyze_generation_quality(analysis_prompts, num_samples=3)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Generation Analysis Summary:\")\n",
    "for result in analysis_results:\n",
    "    print(f\"Prompt '{result['prompt']}':\")\n",
    "    print(f\"  - Average length: {result['avg_length']:.1f} words\")\n",
    "    print(f\"  - Unique vocabulary: {result['unique_words']} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Insights\n",
    "\n",
    "This notebook demonstrates PyTorch-based text generation using BERT's masked language modeling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration with Vietnamese/English mixed content\n",
    "print(\"🌍 Multilingual Capabilities Demo:\")\n",
    "print(\"Testing BERT with Vietnamese/English mixed content...\\n\")\n",
    "\n",
    "# Note: BERT base model has limited Vietnamese capabilities\n",
    "# For better Vietnamese support, consider multilingual BERT\n",
    "multilingual_prompts = [\n",
    "    \"Love is [MASK] and beautiful\",  # English\n",
    "    \"My name is [MASK]\",             # English (as in examples)\n",
    "    \"Hello [MASK] world\",            # English\n",
    "]\n",
    "\n",
    "for prompt in multilingual_prompts:\n",
    "    print(f\"Input: {prompt}\")\n",
    "    predictions = bert_generator.predict_masked_words(prompt, num_predictions=3)\n",
    "    \n",
    "    if predictions:\n",
    "        print(\"Predictions:\")\n",
    "        for i, pred in enumerate(predictions[0][:3]):\n",
    "            print(f\"  {i+1}. {pred['token']} (confidence: {pred['probability']:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n✅ PyTorch BERT Text Generation Demo Complete!\")\n",
    "print(\"\\n📝 Key Takeaways:\")\n",
    "print(\"1. BERT uses bidirectional context for masked language modeling\")\n",
    "print(\"2. PyTorch provides flexible control over model behavior\")\n",
    "print(\"3. Temperature scaling affects generation diversity\")\n",
    "print(\"4. BERT is better for understanding than generation tasks\")\n",
    "print(\"5. For Vietnamese support, use multilingual models like mBERT\")\n",
    "\n",
    "# Vietnamese/English reference\n",
    "print(\"\\n🇻🇳🇺🇸 Vietnamese/English Examples:\")\n",
    "print(\"English: 'My name is John' → Vietnamese: 'Tên tôi là John'\")\n",
    "print(\"English: 'Hello' → Vietnamese: 'Xin chào'\")\n",
    "print(\"English: 'Thank you' → Vietnamese: 'Cảm ơn'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}